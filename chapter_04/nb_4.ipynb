{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Models\n",
    "\n",
    "## üìö Assignment Summary\n",
    "\n",
    "This notebook serves as the submission for **Chapter 4** of the book *\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.\"*\n",
    "\n",
    "**Chapter Objective:** This chapter dives deep into the *how* of model training. Until now, we've treated models like black boxes. This chapter opens them up.\n",
    "\n",
    "We will cover:\n",
    "* **Linear Regression:** How to train it using two different methods:\n",
    "    1.  A direct \"closed-form\" equation (the **Normal Equation**).\n",
    "    2.  Iterative optimization (**Gradient Descent**) and its variants (Batch, Mini-batch, and Stochastic GD).\n",
    "* **Polynomial Regression:** How to use a linear model to fit non-linear data.\n",
    "* **Learning Curves:** How to use them to detect if a model is overfitting or underfitting.\n",
    "* **Regularization:** How to reduce overfitting by constraining the model (Ridge, Lasso, Elastic Net).\n",
    "* **Logistic & Softmax Regression:** How to use these models for classification tasks.\n",
    "\n",
    "This notebook reproduces all code from the chapter and provides theoretical explanations for each concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary modules and define helper functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_models\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression\n",
    "\n",
    "### üßë‚Äçüè´ Theoretical Deep-Dive: The Linear Regression Model\n",
    "\n",
    "A linear model makes a prediction by computing a weighted sum of the input features, plus a constant called the *bias term* (or *intercept term*).\n",
    "\n",
    "**Linear Regression model prediction:**\n",
    "$$ \\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n $$\n",
    "\n",
    "* $\\hat{y}$ (y-hat) is the predicted value.\n",
    "* $n$ is the number of features.\n",
    "* $x_i$ is the $i^{th}$ feature value.\n",
    "* $\\theta_j$ is the $j^{th}$ model parameter ($\theta_0$ is the bias term, $\\theta_1, \\dots, \\theta_n$ are the feature weights).\n",
    "\n",
    "To train this model, we need to find the parameter values $\\theta$ that minimize a cost function. The most common cost function for regression is the **Mean Squared Error (MSE)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. The Normal Equation\n",
    "\n",
    "The **Normal Equation** is a \"closed-form\" mathematical equation that gives the result directly‚Äîit finds the $\\theta$ value that minimizes the MSE cost function.\n",
    "\n",
    "$$ \\hat{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y} $$\n",
    "\n",
    "Let's generate some linear-looking data and use this equation to find the best $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear-looking data\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) # y = 4 + 3x + Gaussian noise\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "save_fig(\"generated_data_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute theta_best using the Normal Equation\n",
    "# We add x0 = 1 to each instance using np.c_ (this is the bias term)\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(\"Best theta found by Normal Equation:\")\n",
    "print(theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were hoping for $\\theta_0 = 4$ and $\\theta_1 = 3$, but we got values close to that. The noise made it impossible to recover the exact parameters. Now we can make predictions using this $\\hat{\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "print(\"Predictions:\")\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model's predictions\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "save_fig(\"linear_model_predictions_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Using Scikit-Learn\n",
    "\n",
    "The same calculation is performed by Scikit-Learn's `LinearRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(\"Theta (intercept and coefficient):\", lin_reg.intercept_, lin_reg.coef_)\n",
    "print(\"Predictions:\", lin_reg.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßë‚Äçüè´ Theoretical Deep-Dive: Computational Complexity\n",
    "\n",
    "The Normal Equation gets very slow when the number of features grows large (e.g., 100,000). The complexity is roughly $O(n^{2.4})$ to $O(n^3)$ (where *n* is the number of features). However, it is linear with regard to the number of instances ($O(m)$), so it's very fast when the dataset fits in memory, as long as there aren't too many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent\n",
    "\n",
    "### üßë‚Äçüè´ Theoretical Deep-Dive: How Gradient Descent Works\n",
    "\n",
    "**Gradient Descent (GD)** is a generic, iterative optimization algorithm capable of finding optimal solutions to a wide range of problems. \n",
    "\n",
    "The general idea is to tweak parameters iteratively in order to minimize a cost function. It works by:\n",
    "1.  Starting with random parameter values (random initialization).\n",
    "2.  Measuring the local gradient of the cost function with regard to the parameters.\n",
    "3.  Taking a step in the direction of the *descending* gradient (downhill).\n",
    "4.  Repeating until the algorithm converges to a minimum.\n",
    "\n",
    "A key hyperparameter is the **learning rate**, which determines the size of the steps. \n",
    "* If the learning rate is **too small**, the algorithm will take a very long time to converge.\n",
    "* If the learning rate is **too large**, the algorithm may jump across the valley and diverge, failing to find a good solution.\n",
    "\n",
    "**Feature Scaling:** Gradient Descent is very sensitive to feature scales. If features have different scales, the cost function will be an elongated bowl. GD will take a long time to converge. **You must scale the data (e..g, using `StandardScaler`) before using GD.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Batch Gradient Descent (Batch GD)\n",
    "\n",
    "In Batch GD, the algorithm uses the **entire training set** to compute the gradients at every step. This makes it very slow on large datasets, but it scales well with the number of features (unlike the Normal Equation).\n",
    "\n",
    "Here is a code reproduction of a Batch GD implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # Learning rate\n",
    "n_iterations = 1000\n",
    "m = 100 # Number of instances\n",
    "\n",
    "theta = np.random.randn(2,1)  # Random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # 1. Compute gradients over the full batch X_b\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    \n",
    "    # 2. Take a step downhill\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(\"Best theta found by Batch GD:\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the exact same result that the Normal Equation found! The code for plotting the different learning rates is in the notebook's repository and demonstrates the effect of `eta`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Stochastic Gradient Descent** is the opposite of Batch GD. It picks a **single random instance** from the training set at every step and computes the gradients based only on that single instance.\n",
    "\n",
    "* **Pro:** Much faster, as it has very little data to manipulate at each step. This allows it to train on huge datasets and be used for online learning.\n",
    "* **Con:** It is much less regular (\"stochastic\" means random). The cost function will bounce up and down, and it will *never* settle at the minimum. \n",
    "\n",
    "The solution to this \"con\" is to gradually reduce the learning rate. The function that determines the learning rate at each iteration is called the **learning schedule**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # Learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # Random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m): # Iterate m times (m instances)\n",
    "        # 1. Pick a random instance\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        \n",
    "        # 2. Compute gradients on that instance\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        \n",
    "        # 3. Determine the learning rate\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        \n",
    "        # 4. Take the step\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "print(\"Best theta found by SGD:\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform SGD with Scikit-Learn, you can use the SGDRegressor class\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Note: y.ravel() is needed because SGDRegressor expects 1D targets\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "print(\"Theta found by SGDRegressor:\")\n",
    "print(sgd_reg.intercept_, sgd_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Mini-batch Gradient Descent\n",
    "\n",
    "**Mini-batch GD** is a compromise. Instead of computing gradients on the full dataset or a single instance, it computes them on small random sets of instances called **mini-batches**.\n",
    "\n",
    "**Main advantage:** It can get a significant performance boost from hardware optimization, especially with GPUs. It is less erratic than SGD but more efficient than Batch GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Comparison of Algorithms\n",
    "\n",
    "| Algorithm | Large m | Large n | Hyperparams | Scaling required | Scikit-Learn |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| **Normal Equation** | Fast | Very Slow | 0 | No | `LinearRegression` |\n",
    "| **Batch GD** | Very Slow | Fast | 2 | Yes | `SGDRegressor` |\n",
    "| **Stochastic GD** | Fast | Fast | $\\ge$2 | Yes | `SGDRegressor` |\n",
    "| **Mini-batch GD** | Fast | Fast | $\\ge$2 | Yes | `SGDRegressor` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Polynomial Regression\n",
    "\n",
    "We can use a linear model to fit non-linear data by adding powers of each feature as new features. This technique is called **Polynomial Regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some non-linear (quadratic) data\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "save_fig(\"quadratic_data_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit-Learn's PolynomialFeatures to transform the data\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "print(\"Original X[0]:\", X[0])\n",
    "print(\"Transformed X_poly[0]:\", X_poly[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_poly` now contains the original feature of `X` plus the square of this feature. Now we can fit a `LinearRegression` model to this extended data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "print(\"Theta (intercept and coefficients):\", lin_reg.intercept_, lin_reg.coef_)\n",
    "\n",
    "# Plot the model's predictions\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "save_fig(\"polynomial_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model estimated $\\hat{y} = 0.56x_1^2 + 0.93x_1 + 1.78$, which is very close to the original function $y = 0.5x_1^2 + 1.0x_1 + 2.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Curves\n",
    "\n",
    "If we perform high-degree Polynomial Regression, we will likely overfit the data. **Learning curves** are plots of the model's performance on the training set and the validation set as a function of the training set size. They are a great tool to tell whether a model is **underfitting** or **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
    "        val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)\n",
    "    plt.xlabel(\"Training set size\", fontsize=14)\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)\n",
    "    plt.axis([0, 80, 0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Underfitting Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.title(\"Learning Curves (Underfitting)\")\n",
    "save_fig(\"underfitting_learning_curves_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßë‚Äçüè´ Theoretical Deep-Dive: Reading the Underfitting Curves\n",
    "\n",
    "These curves are typical of an **underfitting** model:\n",
    "1.  **Training error:** Starts at zero (the model can perfectly fit one instance) but rises quickly and then plateaus at a high value. \n",
    "2.  **Validation error:** Starts very high (it can't generalize from one instance) but decreases to meet the training error.\n",
    "3.  **Conclusion:** Both curves are high and close together. This means the model is too simple and adding more training examples will *not* help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Overfitting Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.title(\"Learning Curves (Overfitting)\")\n",
    "plt.axis([0, 80, 0, 3])\n",
    "save_fig(\"overfitting_learning_curves_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßë‚Äçüè´ Theoretical Deep-Dive: Reading the Overfitting Curves\n",
    "\n",
    "These curves are typical of an **overfitting** model:\n",
    "1.  **Training error:** The error is much lower than with the linear model.\n",
    "2.  **A gap:** There is a significant gap between the training error and the validation error.\n",
    "3.  **Conclusion:** The model performs much better on the training data than on the validation data. One way to improve an overfitting model is to feed it more training data, which would help the two curves converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. The Bias/Variance Trade-off\n",
    "\n",
    "A model's generalization error can be expressed as the sum of three errors:\n",
    "* **Bias:** Error due to wrong assumptions (e.g., assuming data is linear when it's quadratic). A high-bias model is likely to **underfit**.\n",
    "* **Variance:** Error due to excessive sensitivity to small variations in the data. A high-variance model (high flexibility) is likely to **overfit**.\n",
    "* **Irreducible Error:** Error due to the noisiness of the data itself. This cannot be reduced.\n",
    "\n",
    "Increasing a model's complexity (e.g., a higher-degree polynomial) will increase its variance and reduce its bias. Decreasing its complexity will increase its bias and reduce its variance. This is the **bias/variance trade-off**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regularized Linear Models\n",
    "\n",
    "A good way to reduce overfitting is to **regularize** (constrain) the model. For linear models, this is done by constraining the model's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Ridge Regression (Tikhonov Regularization)\n",
    "\n",
    "**Ridge Regression** adds a regularization term to the cost function equal to $\\alpha \\sum_{i=1}^{n} \\theta_i^2$. This is the $\\ell_2$ norm.\n",
    "\n",
    "This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. The hyperparameter $\\alpha$ controls how much you regularize. If $\\alpha=0$, it's just Linear Regression. If $\\alpha$ is very large, all weights end up close to zero, resulting in a flat line.\n",
    "\n",
    "**Note:** It is important to scale the data before performing Ridge Regression, as it is sensitive to the scale of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Example with a closed-form solution\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "print(\"Ridge prediction:\", ridge_reg.predict([[1.5]]))\n",
    "\n",
    "# Example with Stochastic Gradient Descent\n",
    "# The 'penalty' hyperparameter sets the type of regularization term\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\") # 'l2' indicates Ridge regularization\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "print(\"Ridge (SGD) prediction:\", sgd_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Lasso Regression\n",
    "\n",
    "**Lasso (Least Absolute Shrinkage and Selection Operator) Regression** adds an $\\ell_1$ norm regularization term: $\\alpha \\sum_{i=1}^{n} |\\theta_i|$.\n",
    "\n",
    "An important characteristic of Lasso is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). This means Lasso automatically performs **feature selection** and outputs a *sparse model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "print(\"Lasso prediction:\", lasso_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Elastic Net\n",
    "\n",
    "**Elastic Net** is a middle ground between Ridge and Lasso. It's cost function is a simple mix of both Ridge and Lasso's regularization terms. The mix ratio is controlled by the $r$ hyperparameter (called `l1_ratio` in Scikit-Learn).\n",
    "\n",
    "It is almost always preferable to have some regularization, so you should avoid plain Linear Regression. Ridge is a good default, but if you suspect only a few features are useful, Lasso or Elastic Net are better as they tend to reduce useless features' weights to zero. **Elastic Net is generally preferred over Lasso** because Lasso can behave erratically when features are strongly correlated or when there are more features than instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y)\n",
    "print(\"Elastic Net prediction:\", elastic_net.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Early Stopping\n",
    "\n",
    "**Early Stopping** is a very different way to regularize iterative learning algorithms like Gradient Descent. The idea is to simply **stop training as soon as the validation error reaches a minimum**.\n",
    "\n",
    "As the epochs go by, the algorithm learns, and its training error (RMSE) goes down, along with its validation error. But after a while, the validation error stops decreasing and starts to go back up. This indicates that the model has started to overfit the training data. Early stopping just stops training at that minimum point.\n",
    "\n",
    "Here is a basic implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from copy import deepcopy\n",
    "\n",
    "# 1. Prepare the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y.ravel(), test_size=0.2, random_state=42)\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "# 2. Create the model and set up early stopping\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.inf, warm_start=True, \n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "n_epochs = 1000\n",
    "\n",
    "# 3. Run the training loop\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val_predict, y_val)\n",
    "    \n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = deepcopy(sgd_reg)\n",
    "\n",
    "print(\"Best epoch:\", best_epoch)\n",
    "print(\"Minimum validation error:\", minimum_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logistic Regression & Softmax Regression\n",
    "\n",
    "Some regression algorithms can be used for classification. **Logistic Regression** (or *Logit Regression*) is commonly used to estimate the probability that an instance belongs to a particular class (e.g., \"Is this email spam?\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Logistic Regression (Binary Classifier)\n",
    "\n",
    "#### üßë‚Äçüè´ Theoretical Deep-Dive: Estimating Probabilities\n",
    "\n",
    "A Logistic Regression model computes a weighted sum of the inputs (like Linear Regression) but then outputs the **logistic** (also called **sigmoid**) of this result. The output is a number between 0 and 1, which can be interpreted as a probability.\n",
    "\n",
    "The **cost function** for Logistic Regression is the **log loss**. It is a convex function, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the Iris dataset for this example\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a classifier to detect the Iris virginica type based only on the petal width\n",
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the model's estimated probabilities for flowers with petal widths from 0 to 3 cm\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris virginica\")\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.ylabel(\"Probability\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=14)\n",
    "save_fig(\"logistic_regression_proba_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a **decision boundary** around 1.6 cm. Above it, the classifier is highly confident the flower is an *Iris virginica*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Softmax Regression (Multinomial Logistic Regression)\n",
    "\n",
    "The Logistic Regression model can be generalized to support multiple classes directly. This is called **Softmax Regression**.\n",
    "\n",
    "**How it works:**\n",
    "1.  Given an instance **x**, the model first computes a score $s_k(\\mathbf{x})$ for each class $k$.\n",
    "2.  Then it estimates the probability of each class by applying the **softmax function** (also called the *normalized exponential*) to the scores.\n",
    "\n",
    "The **cost function** is the **cross-entropy** (or log loss), which penalizes the model when it estimates a low probability for the target class.\n",
    "\n",
    "Let's use it to classify the iris flowers into all three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "# To use Softmax Regression, set multi_class=\"multinomial\"\n",
    "# The solver \"lbfgs\" is a common solver that supports Softmax\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict on a flower with petal length 5cm and width 2cm\n",
    "print(\"Prediction:\", softmax_reg.predict([[5, 2]]))\n",
    "print(\"Probabilities:\", softmax_reg.predict_proba([[5, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly predicts class 2 (Iris virginica) with 94.2% probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 Conclusion\n",
    "\n",
    "This chapter covered the most important algorithms for training linear models. \n",
    "\n",
    "* We started with **Linear Regression** and two ways to train it: the **Normal Equation** (a direct equation) and **Gradient Descent** (an iterative optimizer).\n",
    "* We explored the three flavors of GD: **Batch, Stochastic, and Mini-batch**.\n",
    "* We learned how to use **Polynomial Regression** to fit non-linear data and **Learning Curves** to detect overfitting/underfitting.\n",
    "* We covered four techniques to **regularize** models and reduce overfitting: **Ridge, Lasso, Elastic Net, and Early Stopping**.\n",
    "* Finally, we looked at **Logistic Regression** and **Softmax Regression** for classification tasks.\n",
    "\n",
    "We are now equipped with a solid foundation of how models actually learn, which will be essential for understanding the neural networks in Part II."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
