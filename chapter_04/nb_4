{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Classification\n",
    "\n",
    "## ðŸ“š Assignment Summary\n",
    "\n",
    "This notebook serves as the submission for **Chapter 3** of the book *\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.\"*\n",
    "\n",
    "**Chapter Objective:** This chapter introduces classification, one of the most common supervised learning tasks. We will use the MNIST dataset to explore key concepts:\n",
    "\n",
    "* Training a binary classifier (e.g., 5 vs. not-5).\n",
    "* Evaluating a classifier using various performance measures (accuracy, confusion matrix, precision, recall, F1-score, ROC curve).\n",
    "* Understanding the precision/recall trade-off.\n",
    "* Working with multiclass, multilabel, and multioutput classification.\n",
    "* Performing error analysis to improve models.\n",
    "\n",
    "This notebook reproduces all code from the chapter and provides theoretical explanations for each concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary modules and define helper functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Helper function to plot digits (from the book's GitHub repository)\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MNIST Dataset\n",
    "\n",
    "This chapter uses the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and US Census Bureau employees. It's often called the \"hello world\" of Machine Learning. Scikit-Learn provides a helper function to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# This may take a couple of minutes\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "print(mnist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets loaded by Scikit-Learn have a similar dictionary structure\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70,000 images, and each image has 784 features. This is because each image is 28Ã—28 pixels, and each feature simply represents one pixelâ€™s intensity, from 0 (white) to 255 (black).\n",
    "\n",
    "Let's plot one of the digits to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[0]\n",
    "plot_digit(some_digit)\n",
    "plt.show()\n",
    "\n",
    "# The label for this digit is '5'\n",
    "print(\"Label:\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels are strings, so let's cast them to integers\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "# Let's also plot a few more digits\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Test Set\n",
    "\n",
    "The MNIST dataset is already split into a training set (the first 60,000 images) and a test set (the last 10,000 images). The training set is also already shuffled, which is good as it ensures all cross-validation folds will be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Binary Classifier\n",
    "\n",
    "Let's simplify the problem and try to identify just one digit: the number 5. This \"5-detector\" will be a **binary classifier**, capable of distinguishing between just two classes: 5 and not-5.\n",
    "\n",
    "Let's create the target vectors for this classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)  # True for all 5s, False for all other digits.\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pick a classifier and train it. A good place to start is with a **Stochastic Gradient Descent (SGD)** classifier, using Scikit-Learnâ€™s `SGDClassifier` class. This classifier is fast and capable of handling very large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42) # Set random_state for reproducible results\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "\n",
    "# Now, let's detect images of the number 5\n",
    "print(\"Prediction for 'some_digit' (which is a 5):\")\n",
    "print(sgd_clf.predict([some_digit]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier correctly guesses that the image is a 5. Now, let's evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Measures\n",
    "\n",
    "Evaluating a classifier is often trickier than evaluating a regressor. We will explore several methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Measuring Accuracy Using Cross-Validation\n",
    "\n",
    "We can use `cross_val_score` to evaluate our `SGDClassifier` model using K-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# This may take a few minutes\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Over 95% accuracy on all folds? This looks amazing, but accuracy is generally *not* the preferred performance measure for classifiers, especially when you are dealing with **skewed datasets** (i.e., when some classes are much more frequent than others).\n",
    "\n",
    "Let's see why. Let's create a classifier that just classifies every single image in the \"not-5\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dumb classifier has over 90% accuracy! This is simply because only about 10% of the images are 5s. If you always guess \"not-5\", you will be right about 90% of the time. This demonstrates why accuracy is a poor measure for skewed datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Confusion Matrix\n",
    "\n",
    "A much better way to evaluate a classifier is to look at the **confusion matrix**. The general idea is to count the number of times instances of class A are classified as class B.\n",
    "\n",
    "To compute it, we first need a set of predictions to compare to the actual targets. We can use `cross_val_predict()` which, instead of returning scores, returns the predictions made on each test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ§‘â€ðŸ« Theoretical Deep-Dive: Understanding the Confusion Matrix\n",
    "\n",
    "Each row in a confusion matrix represents an **actual class**, while each column represents a **predicted class**.\n",
    "\n",
    "The matrix for our 5-detector looks like this:\n",
    "\n",
    "| | Predicted: Not-5 | Predicted: 5 |\n",
    "|---|---|---|\n",
    "| **Actual: Not-5** | 53,892 (True Negatives) | 687 (False Positives) |\n",
    "| **Actual: 5** | 1,891 (False Negatives) | 3,530 (True Positives) |\n",
    "\n",
    "* **True Negatives (TN):** Non-5s that were correctly classified as non-5s.\n",
    "* **False Positives (FP):** Non-5s that were incorrectly classified as 5s.\n",
    "* **False Negatives (FN):** 5s that were incorrectly classified as non-5s.\n",
    "* **True Positives (TP):** 5s that were correctly classified as 5s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Precision, Recall, and F1-Score\n",
    "\n",
    "The confusion matrix gives us a lot of information, but sometimes we may prefer more concise metrics.\n",
    "\n",
    "**Precision:** The accuracy of the *positive* predictions. \"When the model predicts a 5, how often is it correct?\"\n",
    "$$ \\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "**Recall (Sensitivity or True Positive Rate):** The ratio of positive instances that are correctly detected by the classifier. \"What percentage of 5s did the model correctly identify?\"\n",
    "$$ \\text{Recall} = \\frac{TP}{TP + FN} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Precision:\", precision_score(y_train_5, y_train_pred))\n",
    "print(\"Recall:\", recall_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 5-detector doesn't look as shiny now. It is correct only 83.7% of the time it predicts a 5, and it only detects 65.1% of all 5s.\n",
    "\n",
    "It is often convenient to combine precision and recall into a single metric called the **F1-score**. The F1-score is the **harmonic mean** of precision and recall. It gives more weight to low values, so a high F1-score is only possible if *both* precision and recall are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 Score:\", f1_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Precision/Recall Trade-off\n",
    "\n",
    "Unfortunately, you canâ€™t have it both ways: increasing precision reduces recall, and vice versa. This is called the **precision/recall trade-off**.\n",
    "\n",
    "The `SGDClassifier` makes its decisions based on a score from a **decision function**. If that score is greater than a **threshold**, it assigns the instance to the positive class. Raising this threshold increases precision (the model is pickier) but decreases recall (it misses more positive instances).\n",
    "\n",
    "We can get the decision scores instead of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.grid(True)\n",
    "    plt.axis([-50000, 50000, 0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to select a good precision/recall trade-off is to plot precision directly against recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we decide to aim for 90% precision. We can find the threshold that gives us this precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lowest threshold that gives at least 90% precision\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "print(\"Threshold for 90% precision:\", threshold_90_precision)\n",
    "\n",
    "# Now we can make predictions based on this threshold\n",
    "y_train_pred_90 = (y_scores >= threshold_90_precision)\n",
    "\n",
    "print(\"Precision at this threshold:\", precision_score(y_train_5, y_train_pred_90))\n",
    "print(\"Recall at this threshold:\", recall_score(y_train_5, y_train_pred_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. The ROC Curve\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC)** curve is another common tool. It plots the **True Positive Rate (TPR)** (another name for recall) against the **False Positive Rate (FPR)**.\n",
    "\n",
    "* **FPR** is the ratio of negative instances that are incorrectly classified as positive (`FP / (FP + TN)`).\n",
    "* A perfect classifier will have a ROC curve that goes straight up to the top-left corner (TPR = 1, FPR = 0).\n",
    "\n",
    "One way to compare classifiers is to measure the **Area Under the Curve (AUC)**. A perfect classifier will have a ROC AUC of 1, while a purely random classifier will have a ROC AUC of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal for random classifier\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_train_5, y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule of thumb:** Use the **Precision/Recall (PR) curve** whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the **ROC curve**.\n",
    "\n",
    "Let's train a `RandomForestClassifier` and compare its ROC curve to the `SGDClassifier`'s. A Random Forest has a `predict_proba()` method, which we can use to get scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "                                    method=\"predict_proba\")\n",
    "\n",
    "# The ROC curve needs scores, not probabilities. We use the positive class's probability as the score.\n",
    "y_scores_forest = y_probas_forest[:, 1]\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)\n",
    "\n",
    "# Plot both ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Random Forest ROC AUC Score:\", roc_auc_score(y_train_5, y_scores_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest classifier is much better. Its ROC curve is closer to the top-left corner, and its ROC AUC score is significantly higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiclass Classification\n",
    "\n",
    "**Multiclass classifiers** (or *multinomial classifiers*) can distinguish between more than two classes.\n",
    "\n",
    "Some algorithms (like SGD, Random Forests, Naive Bayes) can handle multiple classes natively. Others (like SVM, Logistic Regression) are strictly binary classifiers. However, you can use binary classifiers for multiclass tasks using two main strategies:\n",
    "\n",
    "* **One-versus-the-Rest (OvR) / One-versus-All (OvA):** Train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, etc.). When you want to classify an image, you get the decision score from each classifier and select the class whose classifier outputs the highest score.\n",
    "* **One-versus-One (OvO):** Train a binary classifier for every pair of digits (a 0-vs-1, a 0-vs-2, a 1-vs-2, etc.). For N classes, this requires training N Ã— (N â€“ 1) / 2 classifiers. For MNIST, this means 45 classifiers. When you classify an image, you run it through all 45 classifiers and see which class wins the most duels.\n",
    "\n",
    "Scikit-Learn detects when you try to use a binary classifier for a multiclass task, and it automatically runs OvR or OvO depending on the algorithm (e.g., SVMs use OvO, most others use OvR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train an SVM. This uses OvO under the hood.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train[:2000], y_train[:2000]) # Train on a small subset for speed\n",
    "\n",
    "print(\"Prediction for 'some_digit':\", svm_clf.predict([some_digit]))\n",
    "\n",
    "# Get the scores for each of the 10 classes\n",
    "some_digit_scores = svm_clf.decision_function([some_digit])\n",
    "print(\"\\nScores per class:\", some_digit_scores)\n",
    "print(\"Highest score index:\", np.argmax(some_digit_scores))\n",
    "print(\"Class:\", svm_clf.classes_[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDClassifier can handle multiclass directly. Let's train it on all digits.\n",
    "sgd_clf.fit(X_train, y_train) # y_train, not y_train_5\n",
    "print(\"SGD prediction for 'some_digit':\", sgd_clf.predict([some_digit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate this multiclass SGD classifier\n",
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets over 84% on all folds. This is not bad, but we can improve it by simply scaling the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "\n",
    "print(\"--- Evaluating SGD with scaled data ---\")\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "\n",
    "Now that we have a promising model, we can analyze the types of errors it makes. A good way to do this is to use the confusion matrix again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "print(conf_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is often more convenient to view as an image\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix looks good, as most images are on the main diagonal (meaning they were classified correctly). The 5s look slightly darker, which could mean there are fewer images of 5s or that the classifier does not perform as well on 5s.\n",
    "\n",
    "Let's focus the plot on the errors. We'll divide each value by the number of images in its actual class (to compare error *rates*) and fill the diagonal with zeros to see only the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.title(\"Error Matrix (Normalized)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix clearly shows the kinds of errors the classifier makes. Remember that rows are actual classes, while columns are predicted classes.\n",
    "\n",
    "* The column for class 8 is bright, which means many digits get misclassified *as* 8s.\n",
    "* The row for class 8 is not that bad, meaning most actual 8s get correctly classified.\n",
    "* The 3s and 5s often get confused in both directions.\n",
    "\n",
    "Analyzing this plot suggests we should focus on reducing the false 8s. We can also plot examples of the 3s and 5s that get confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting examples of 3s and 5s\n",
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.title(\"Correctly classified 3s\")\n",
    "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "plt.title(\"3s classified as 5s\")\n",
    "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "plt.title(\"5s classified as 3s\")\n",
    "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
    "plt.title(\"Correctly classified 5s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis shows that the errors are often on ambiguous, poorly written digits. The `SGDClassifier` is a linear model, so it makes errors when the classes are not linearly separable. It is sensitive to image shifting and rotation. Preprocessing the images to ensure they are well-centered and not too rotated could help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multilabel Classification\n",
    "\n",
    "Until now, each instance has been assigned to only one class. In **multilabel classification**, the system outputs multiple binary tags. For example, a face-recognition system might identify several people in one picture, outputting `[1, 0, 1]` for \"Alice yes, Bob no, Charlie yes.\"\n",
    "\n",
    "Let's create a multilabel target for our MNIST data: one label to identify large digits (7, 8, or 9) and one to identify odd digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)\n",
    "\n",
    "print(\"Multilabel prediction for 'some_digit' (a 5):\")\n",
    "print(knn_clf.predict([some_digit]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier correctly predicts that the digit 5 is *not large* (False) and *is odd* (True).\n",
    "\n",
    "There are many ways to evaluate a multilabel classifier. One option is to measure the F1 score for each individual label and then compute the average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell may take a very long time to run\n",
    "\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
    "\n",
    "# 'average=\"macro\"' computes the mean F1 score, assuming all labels are equally important\n",
    "# 'average=\"weighted\"' would give each label a weight equal to its support (number of instances)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multioutput Classification\n",
    "\n",
    "**Multioutputâ€“multiclass classification** (or simply *multioutput classification*) is a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).\n",
    "\n",
    "To illustrate this, let's build a system that removes noise from images. It will take a noisy digit image as input and will output a clean digit image. The classifier's output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity from 0 to 255).\n",
    "\n",
    "First, let's create the noisy dataset by adding noise to the pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the MNIST images\n",
    "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
    "X_train_mod = X_train + noise\n",
    "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
    "X_test_mod = X_test + noise\n",
    "\n",
    "# The target images are the original clean images\n",
    "y_train_mod = X_train\n",
    "y_test_mod = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a noisy image and its clean target\n",
    "some_index = 0\n",
    "plt.subplot(121); plot_digit(X_test_mod[some_index])\n",
    "plt.title(\"Noisy\")\n",
    "plt.subplot(122); plot_digit(y_test_mod[some_index])\n",
    "plt.title(\"Clean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier to clean the image\n",
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
    "plot_digit(clean_digit)\n",
    "plt.title(\"Cleaned by KNN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 Conclusion\n",
    "\n",
    "In this chapter, we covered a lot of ground in classification. We learned:\n",
    "\n",
    "* The importance of choosing the right **performance metrics**, especially for skewed datasets, moving beyond simple accuracy.\n",
    "* How to use the **confusion matrix**, **precision**, **recall**, **F1-score**, and **ROC curves** to get a much deeper understanding of our model's performance.\n",
    "* The concept of the **precision/recall trade-off** and how to adjust the decision threshold to select the trade-off that best fits our task.\n",
    "* How to build models for **multiclass**, **multilabel**, and **multioutput** tasks.\n",
    "* The importance of **error analysis** in guiding our efforts to improve a model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
