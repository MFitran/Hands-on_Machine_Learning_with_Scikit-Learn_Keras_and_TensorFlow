{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n",
    "\n",
    "This notebook contains the code reproductions and theoretical explanations for Chapter 17 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter explores two powerful unsupervised learning techniques: **Autoencoders** and **Generative Adversarial Networks (GANs)**. Both are capable of learning dense representations (codings) of data, but they work very differently.\n",
    "\n",
    "**Autoencoders** learn to reconstruct their inputs. They are composed of an **encoder** that compresses the input into a low-dimensional **latent representation** (or coding) and a **decoder** that reconstructs the input from this coding. By constraining the autoencoder (e.g., limiting the size of the coding layer or adding noise to the inputs), we force it to learn important features.\n",
    "\n",
    "Key autoencoder topics covered include:\n",
    "* **Stacked Autoencoders:** Building deep autoencoders for dimensionality reduction or unsupervised pretraining.\n",
    "* **Convolutional and Recurrent Autoencoders:** Applying the autoencoder concept to image and sequence data.\n",
    "* **Denoising Autoencoders:** Training an autoencoder to recover a clean input from a corrupted (noisy) one.\n",
    "* **Sparse Autoencoders:** Using regularization (like L1 or KL Divergence) to force the autoencoder to learn a sparse set of features.\n",
    "* **Variational Autoencoders (VAEs):** A generative autoencoder that learns a probability distribution of the data. It can be used to generate new instances that look similar to the training data.\n",
    "\n",
    "**GANs** are generative models composed of two competing networks: a **generator** that creates fake data and a **discriminator** that tries to distinguish the fake data from real data. This adversarial training pushes the generator to create increasingly realistic data.\n",
    "\n",
    "Key GAN topics covered include:\n",
    "* **Training Difficulties:** The challenges of training GANs, such as **mode collapse** (where the generator produces limited variety) and training instability.\n",
    "* **Deep Convolutional GANs (DCGANs):** A set of architectural guidelines for building stable GANs using convolutional layers.\n",
    "* **Progressive Growing of GANs:** A technique to generate very high-resolution images by starting with small images and progressively adding layers to both networks.\n",
    "* **StyleGANs:** A state-of-the-art architecture that uses *style transfer* to control different levels of the generated image (e.g., fine-grained texture vs. high-level features) and produces incredibly realistic images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Common setup for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Function to plot images (from chapter 10)\n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Data Representations\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "An **autoencoder** is an unsupervised neural network that learns to copy its inputs to its outputs. It has two parts:\n",
    "1.  **Encoder:** Compresses the input data into a lower-dimensional latent representation (the *coding*).\n",
    "2.  **Decoder:** Reconstructs the original data from the coding.\n",
    "\n",
    "This task sounds trivial, but by placing constraints on the network, we force it to learn efficient patterns in the data. The most common constraint is to make the coding layer smaller than the input layer. This is called an **undercomplete autoencoder**. It's forced to learn the most important features (representation learning) to be able to reconstruct the input, effectively performing dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing PCA with an Undercomplete Linear Autoencoder\n",
    "\n",
    "If the autoencoder uses only linear activations (i.e., no activation function) and the loss function is the Mean Squared Error (MSE), it will end up performing Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 3D dataset (as in Chapter 8)\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "data = np.empty((m, 3))\n",
    "data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(data[:100])\n",
    "X_test = scaler.transform(data[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the linear autoencoder to perform PCA\n",
    "encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\n",
    "decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "autoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(X_train, X_train, epochs=20)\n",
    "\n",
    "# Use the encoder to project the data to 2D\n",
    "codings = encoder.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoders\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "Autoencoders can have multiple hidden layers, just like a regular MLP. When they do, they are called **stacked autoencoders** (or deep autoencoders). Adding more layers helps the network learn more complex codings. The architecture is typically *symmetrical* around the central coding layer (e.g., 784 -> 100 -> 30 -> 100 -> 784)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Stacked Autoencoder Using Keras\n",
    "\n",
    "We'll build a stacked autoencoder for Fashion MNIST. We use binary cross-entropy as the loss because it treats the reconstruction task as a multilabel binary classification problem (is this pixel black or white?), which often helps the model converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion MNIST data (as in Chapter 10)\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\"), # This is the coding layer\n",
    "])\n",
    "\n",
    "stacked_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "stacked_ae.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=keras.optimizers.SGD(lr=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = stacked_ae.fit(X_train, X_train, epochs=10,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Reconstructions\n",
    "\n",
    "Let's check how well the autoencoder reconstructs the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructions(model, n_images=5):\n",
    "    reconstructions = model.predict(X_valid[:n_images])\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plot_image(X_valid[image_index])\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plot_image(reconstructions[image_index])\n",
    "\n",
    "show_reconstructions(stacked_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Fashion MNIST Dataset\n",
    "\n",
    "Autoencoders are a powerful tool for dimensionality reduction. We can use the encoder to reduce the 784-dimensional input to 30-dimensional codings, then use another technique like t-SNE to reduce it further to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_valid_compressed = stacked_encoder.predict(X_valid)\n",
    "tsne = TSNE()\n",
    "X_valid_2D = tsne.fit_transform(X_valid_compressed)\n",
    "\n",
    "plt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining Using Stacked Autoencoders\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "A key application of autoencoders is unsupervised pretraining. If you have a complex task (like classification) but very little *labeled* data, you can first train a stacked autoencoder on all your data (labeled + unlabeled). \n",
    "\n",
    "This autoencoder will learn to detect useful features and patterns in the data. You can then take the encoder part of the autoencoder, add a new output layer (e.g., a Dense classification layer) on top, and fine-tune this new model on your small *labeled* dataset. This reuse of layers is a form of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tying Weights\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "When an autoencoder is symmetrical, a common technique is to **tie the weights** of the decoder layers to the weights of the encoder layers. Specifically, the decoder's weight matrix is set to be the *transpose* of the corresponding encoder's weight matrix (**W**_decoder_ = **W**_encoder_ᵀ). \n",
    "\n",
    "This halves the number of parameters in the model, which speeds up training and reduces the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a custom layer to implement tied weights\n",
    "class DenseTranspose(keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense = dense\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\", initializer=\"zeros\",\n",
    "                                      shape=[self.dense.input_shape[-1]])\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        # Use the transposed kernel of the encoder layer\n",
    "        Z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(Z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_1 = keras.layers.Dense(100, activation=\"selu\")\n",
    "dense_2 = keras.layers.Dense(30, activation=\"selu\")\n",
    "\n",
    "tied_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    dense_1,\n",
    "    dense_2\n",
    "])\n",
    "\n",
    "tied_decoder = keras.models.Sequential([\n",
    "    DenseTranspose(dense_2, activation=\"selu\"),\n",
    "    DenseTranspose(dense_1, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "tied_ae = keras.models.Sequential([tied_encoder, tied_decoder])\n",
    "\n",
    "tied_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.5))\n",
    "history_tied = tied_ae.fit(X_train, X_train, epochs=10,\n",
    "                           validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoders\n",
    "\n",
    "For images, autoencoders made of dense layers won't work well. We should use **convolutional autoencoders**.\n",
    "\n",
    "* The **encoder** is a standard CNN (Conv2D and MaxPool2D layers) that downsamples the image into a latent representation.\n",
    "* The **decoder** must do the reverse: it must *upsample* the image. This is typically done using `Conv2DTranspose` layers (also called deconvolutional layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_encoder = keras.models.Sequential([\n",
    "    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n",
    "    keras.layers.Conv2D(16, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
    "    keras.layers.MaxPool2D(pool_size=2) # Output shape: [3, 3, 64]\n",
    "])\n",
    "\n",
    "conv_decoder = keras.models.Sequential([\n",
    "    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"valid\",\n",
    "                                  activation=\"selu\", input_shape=[3, 3, 64]),\n",
    "    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"same\",\n",
    "                                  activation=\"selu\"),\n",
    "    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"same\",\n",
    "                                  activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n",
    "\n",
    "conv_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.0))\n",
    "history_conv = conv_ae.fit(X_train, X_train, epochs=5,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "Another way to force the autoencoder to learn useful features is to add noise to its inputs (e.g., using Gaussian noise or dropout) and train it to recover the *original, noise-free* inputs. \n",
    "\n",
    "The autoencoder is forced to learn patterns in the data so it can distinguish the underlying signal from the noise. This makes it a good feature extractor and can also be used to clean up noisy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(0.5), # Add noise to the inputs\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(30, activation=\"selu\")\n",
    "])\n",
    "\n",
    "dropout_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])\n",
    "\n",
    "dropout_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.0))\n",
    "\n",
    "# Note: We feed the noisy inputs (X_train_noisy) but the target is the clean inputs (X_train)\n",
    "# For this example, we'll use Dropout, so the inputs are X_train and targets are X_train.\n",
    "# Keras's Dropout layer is only active during training.\n",
    "history_denoising = dropout_ae.fit(X_train, X_train, epochs=10,\n",
    "                                 validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Autoencoders\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "A **sparse autoencoder** is an autoencoder (typically overcomplete) that is constrained to produce a *sparse* latent representation. This means most of the neurons in the coding layer are inactive (output zero).\n",
    "\n",
    "This forces the model to represent each input as a combination of a small number of \"active\" features, pushing each coding neuron to learn a useful and specific feature.\n",
    "\n",
    "Two ways to achieve sparsity:\n",
    "1.  **L1 Regularization:** Add L1 regularization to the *activations* of the coding layer. This pushes the model to output zeros.\n",
    "2.  **KL Divergence:** A more complex method. We measure the average activation of each coding neuron over a batch and penalize it if it deviates from a target sparsity (e.g., 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sparse Autoencoder with L1 Regularization\n",
    "sparse_l1_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(300, activation=\"sigmoid\"), # Overcomplete coding layer\n",
    "    # Add L1 regularization to the layer's *activity*\n",
    "    keras.layers.ActivityRegularization(l1=1e-3)\n",
    "])\n",
    "\n",
    "sparse_l1_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[300]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])\n",
    "sparse_l1_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sparse Autoencoder with KL Divergence\n",
    "K = keras.backend\n",
    "kl_divergence = keras.losses.kullback_leibler_divergence\n",
    "\n",
    "class KLDivergenceRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, weight, target=0.1):\n",
    "        self.weight = weight\n",
    "        self.target = target\n",
    "    def __call__(self, inputs):\n",
    "        mean_activities = K.mean(inputs, axis=0)\n",
    "        return self.weight * (\n",
    "            kl_divergence(self.target, mean_activities) +\n",
    "            kl_divergence(1. - self.target, 1. - mean_activities))\n",
    "    def get_config(self):\n",
    "        return {\"weight\": self.weight, \"target\": self.target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)\n",
    "\n",
    "sparse_kl_encoder = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(300, activation=\"sigmoid\", activity_regularizer=kld_reg)\n",
    "])\n",
    "\n",
    "sparse_kl_decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[300]),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])\n",
    "sparse_kl_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "**Variational Autoencoders (VAEs)** are different from other autoencoders. \n",
    "1.  They are **probabilistic**: their outputs are partly determined by chance, even after training.\n",
    "2.  They are **generative**: they can generate new instances that look like they were sampled from the training set.\n",
    "\n",
    "How they work:\n",
    "* Instead of mapping an input to a single point (coding), the **encoder** maps the input to a probability distribution. Specifically, it outputs a **mean (μ)** and a **standard deviation (σ)**.\n",
    "* A **coding** is then randomly *sampled* from a Gaussian distribution with this mean μ and standard deviation σ.\n",
    "* The **decoder** then takes this sampled coding and reconstructs the input.\n",
    "\n",
    "**The VAE Loss Function:**\n",
    "The model is trained with a composite loss function:\n",
    "1.  **Reconstruction Loss:** This is the standard loss (e.g., binary cross-entropy) that pushes the model to reconstruct its input. It's the `decoder_loss`.\n",
    "2.  **Latent Loss (KL Divergence):** This loss pushes the model to make the codings look as if they were sampled from a simple standard Gaussian distribution (mean=0, std=1). This regularization ensures that the *latent space* (the space of all possible codings) is well-organized and dense, which allows us to generate new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a custom sampling layer\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VAE (using the Functional API)\n",
    "codings_size = 10\n",
    "\n",
    "# --- Encoder ---\n",
    "inputs = keras.layers.Input(shape=[28, 28])\n",
    "z = keras.layers.Flatten()(inputs)\n",
    "z = keras.layers.Dense(150, activation=\"selu\")(z)\n",
    "z = keras.layers.Dense(100, activation=\"selu\")(z)\n",
    "codings_mean = keras.layers.Dense(codings_size)(z)    # μ\n",
    "codings_log_var = keras.layers.Dense(codings_size)(z) # γ = log(σ²)\n",
    "codings = Sampling()([codings_mean, codings_log_var])\n",
    "variational_encoder = keras.Model(\n",
    "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "\n",
    "# --- Decoder ---\n",
    "decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
    "x = keras.layers.Dense(100, activation=\"selu\")(decoder_inputs)\n",
    "x = keras.layers.Dense(150, activation=\"selu\")(x)\n",
    "x = keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n",
    "outputs = keras.layers.Reshape([28, 28])(x)\n",
    "variational_decoder = keras.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "\n",
    "# --- VAE Model ---\n",
    "_, _, codings = variational_encoder(inputs)\n",
    "reconstructions = variational_decoder(codings)\n",
    "variational_ae = keras.Model(inputs=[inputs], outputs=[reconstructions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the custom latent (KL divergence) loss\n",
    "latent_loss = -0.5 * K.sum(\n",
    "    1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n",
    "    axis=-1)\n",
    "\n",
    "# Note: we divide by 784 to scale the latent loss to be on the same scale \n",
    "# as the reconstruction loss (which is the mean pixel error, not the sum)\n",
    "variational_ae.add_loss(K.mean(latent_loss) / 784.)\n",
    "\n",
    "variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = variational_ae.fit(X_train, X_train, epochs=25, batch_size=128,\n",
    "                             validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Fashion MNIST Images\n",
    "\n",
    "Because the VAE learned a probability distribution in its latent space, we can sample a random point from that space (a simple Gaussian distribution) and pass it to the decoder to generate a brand new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings = tf.random.normal(shape=[12, codings_size])\n",
    "images = variational_decoder(codings).numpy()\n",
    "\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(8, 4))\n",
    "for index, image in enumerate(images):\n",
    "    plt.subplot(3, 4, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "**GANs** are a completely different approach to generative modeling. They consist of two networks in competition:\n",
    "\n",
    "1.  **The Generator:** Its job is to create fake data (e.g., images). It takes random noise (latent representations) as input and tries to output data that looks like it came from the training set.\n",
    "2.  **The Discriminator:** Its job is to be a classifier. It takes an image as input (either a *real* one from the training set or a *fake* one from the generator) and tries to predict if it's real or fake.\n",
    "\n",
    "**The Training Process:**\n",
    "This is a *zero-sum game* between the two networks.\n",
    "* **Phase 1: Train the Discriminator.** We create a batch of fake images from the generator and a batch of real images. We label the fake images '0' and the real images '1'. We then train the discriminator on this batch for one step (with the generator's weights frozen).\n",
    "* **Phase 2: Train the Generator.** We create a new batch of fake images. We label them all '1' (as in \"real\"). We then train the *full GAN model* (with the discriminator's weights frozen). The gradients will flow back from the discriminator to the generator, teaching the generator how to produce images that the discriminator will classify as '1'.\n",
    "\n",
    "Over time, the generator gets better at making fakes, and the discriminator gets better at spotting them, until (ideally) the generator produces perfectly realistic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the GAN components for Fashion MNIST\n",
    "codings_size = 30\n",
    "\n",
    "# The Generator (a decoder)\n",
    "generator = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"selu\", input_shape=[codings_size]),\n",
    "    keras.layers.Dense(150, activation=\"selu\"),\n",
    "    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    keras.layers.Reshape([28, 28])\n",
    "])\n",
    "\n",
    "# The Discriminator (a classifier)\n",
    "discriminator = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(150, activation=\"selu\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# The full GAN model\n",
    "gan = keras.models.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False # This is key for phase 2\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "batch_size = 32\n",
    "# We reshape and rescale the data to -1 to 1 for the sigmoid output (tanh is better)\n",
    "# For this simple GAN, sigmoid (0 to 1) is fine. We will not rescale.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom training loop\n",
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs=50):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        for X_batch in dataset:\n",
    "            # --- Phase 1: Train the Discriminator ---\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            \n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            \n",
    "            # --- Phase 2: Train the Generator ---\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size) # We want the discriminator to be Fooled\n",
    "            \n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(noise, y2)\n",
    "        # You would typically add code to generate and save images here\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 1 epoch to demonstrate (real training takes hours)\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Difficulties of Training GANs\n",
    "\n",
    "Training GANs is notoriously hard. The two networks are in a zero-sum game that needs to reach a **Nash equilibrium** (where the generator produces perfectly realistic images and the discriminator is forced to guess 50/50).\n",
    "\n",
    "However, this equilibrium is very unstable. The main problems are:\n",
    "\n",
    "* **Mode Collapse:** The generator gets good at producing one type of image (e.g., a good-looking shoe) and only produces that. It forgets how to make anything else. The discriminator then learns to spot this one type of fake, and the generator moves on to another single class, and so on.\n",
    "* **Instability:** The parameters may oscillate or diverge as the two networks constantly try to outsmart each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Convolutional GANs (DCGAN)\n",
    "\n",
    "The DCGAN architecture proposed a set of guidelines to make GANs more stable:\n",
    "\n",
    "* Replace pooling layers with **strided convolutions** (in the discriminator) and **transposed convolutions** (in the generator).\n",
    "* Use **Batch Normalization** in both networks (except the generator's output and discriminator's input).\n",
    "* Remove fully connected hidden layers for deeper architectures.\n",
    "* Use `ReLU` in the generator (except for the output, which uses `tanh`).\n",
    "* Use `LeakyReLU` in the discriminator.\n",
    "\n",
    "The code below implements a DCGAN, but uses `SELU` in the generator and `Dropout` in the discriminator (as the book notes, BN was unstable in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the training set to -1 to 1 (for the tanh output)\n",
    "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1.\n",
    "\n",
    "codings_size = 100\n",
    "\n",
    "generator = keras.models.Sequential([\n",
    "    keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]),\n",
    "    keras.layers.Reshape([7, 7, 128]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"same\",\n",
    "                                  activation=\"selu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"same\",\n",
    "                                  activation=\"tanh\")\n",
    "])\n",
    "\n",
    "discriminator = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
    "                        activation=keras.layers.LeakyReLU(0.2),\n",
    "                        input_shape=[28, 28, 1]),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
    "                        activation=keras.layers.LeakyReLU(0.2)),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "gan = keras.models.Sequential([generator, discriminator])\n",
    "\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan).shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 1 epoch to demonstrate\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressive Growing of GANs & StyleGAN\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "More recent architectures have produced stunningly realistic images. \n",
    "\n",
    "* **Progressive Growing of GANs (ProGAN):** Instead of generating high-res images from the start, the model is trained to generate very small images (e.g., 4x4), and then new layers are progressively added to both the generator and discriminator to double the resolution (8x8, 16x16, ... up to 1024x1024). This is much more stable.\n",
    "\n",
    "* **StyleGAN:** The current state-of-the-art. Its generator uses two networks:\n",
    "    1.  A **Mapping Network** (an MLP) that converts the input coding (z) into a *style vector* (w).\n",
    "    2.  A **Synthesis Network** that generates the image. The style vector (w) is used to control the "style" of the image at each convolutional layer (via a technique called *Adaptive Instance Normalization*). This allows for incredible control over the generated image. It also injects noise at each level to generate realistic stochastic details (like freckles or hair placement)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "See Appendix A in the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
