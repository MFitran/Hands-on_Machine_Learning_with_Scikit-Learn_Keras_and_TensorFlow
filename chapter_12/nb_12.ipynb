{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Training with TensorFlow\n",
    "\n",
    "This notebook contains the code reproductions and theoretical explanations for Chapter 12 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter transitions from the high-level `tf.keras` API to TensorFlow's lower-level Python API. The main objective is to provide the tools and knowledge necessary for when you need extra control over your models and training loops.\n",
    "\n",
    "Key topics covered include:\n",
    "\n",
    "* **TensorFlow Basics:** Using TensorFlow like NumPy for tensor manipulation, understanding the difference between `tf.Tensor` (immutable) and `tf.Variable` (mutable), and exploring other data structures like ragged and sparse tensors.\n",
    "* **Customizing Keras Components:** We learn how to create:\n",
    "    * Custom Loss Functions\n",
    "    * Custom Activation Functions, Initializers, Regularizers, and Constraints\n",
    "    * Custom Metrics (including streaming/stateful metrics)\n",
    "    * Custom Layers (stateless and stateful)\n",
    "    * Custom Models (by subclassing `keras.Model`)\n",
    "    * How to save and load models containing these custom components.\n",
    "* **Autodiff and Custom Training Loops:** The chapter explains how to compute gradients automatically using `tf.GradientTape`. This is the foundation for building custom training loops, which gives you full control over the training process (e.g., using multiple optimizers for different parts of a model).\n",
    "* **TF Functions and Graphs:** Finally, it covers `tf.function`, a powerful tool that converts Python functions into high-performance TensorFlow computation graphs. It explains how AutoGraph and Tracing work to make this conversion, and the rules you must follow to ensure your functions are convertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Tour of TensorFlow\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "TensorFlow is an open-source library for numerical computation, fine-tuned for large-scale Machine Learning. \n",
    "\n",
    "Its main features include:\n",
    "* **NumPy-like Core:** Its core API is similar to NumPy's but with GPU support.\n",
    "* **Distributed Computing:** It can run on multiple devices and servers.\n",
    "* **JIT Compiler:** It includes a just-in-time (JIT) compiler that optimizes computation graphs for speed and memory.\n",
    "* **Portability:** You can train a model in one environment (like Python on Linux) and run it in another (like Java on an Android device).\n",
    "* **Autodiff:** It implements automatic differentiation (which we'll explore) and provides excellent optimizers.\n",
    "\n",
    "While `tf.keras` is the high-level API, TensorFlow also provides a powerful lower-level API. We'll start by exploring how to use TF like NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow like NumPy\n",
    "\n",
    "### Tensors and Operations\n",
    "\n",
    "A `tf.Tensor` is the primary data structure in TensorFlow. It's very similar to a NumPy `ndarray`: it's a multidimensional array that can hold a scalar. Tensors are **immutable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Create tensors\n",
    "tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(42) # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape and dtype\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing (works like NumPy)\n",
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations\n",
    "t + 10 # equivalent to tf.add(t, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "t @ tf.transpose(t) # equivalent to tf.matmul(t, tf.transpose(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras's Low-Level API\n",
    "\n",
    "`tf.keras` also has its own low-level API in `keras.backend`. These functions generally just call the corresponding TensorFlow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "K.square(K.transpose(t)) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and NumPy\n",
    "\n",
    "Tensors and NumPy arrays work together nicely. You can create a tensor from a NumPy array and vice-versa. You can even apply TF ops to NumPy arrays and NumPy ops to tensors.\n",
    "\n",
    "**Note:** NumPy uses 64-bit precision by default, while TensorFlow uses 32-bit precision. This is because 32-bit is generally enough for neural networks and is faster, using less RAM. When converting, you should generally set `dtype=tf.float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.numpy() # Convert a tensor to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.square(a) # Apply TF op to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(t) # Apply NumPy op to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Conversions\n",
    "\n",
    "TensorFlow is very strict about types. It **does not perform automatic type conversions** as this can hurt performance. Operations between tensors of incompatible types will raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40)\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tf.constant(2.) + tf.constant(40., dtype=tf.float64)\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must cast explicitly\n",
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "`tf.Tensor` values are immutable. If you need a value that can be modified (like the weights of a neural network), you must use a `tf.Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify a variable in-place\n",
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will fail, direct item assignment is not supported\n",
    "# try:\n",
    "#     v[1, 2] = 100\n",
    "# except TypeError as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Data Structures\n",
    "\n",
    "TensorFlow supports several other data structures:\n",
    "* **Sparse Tensors (`tf.SparseTensor`):** Efficiently represent tensors with mostly zeros.\n",
    "* **Tensor Arrays (`tf.TensorArray`):** Lists of tensors. Fixed size by default, but can be dynamic.\n",
    "* **Ragged Tensors (`tf.RaggedTensor`):** Represent lists of lists of tensors (static lists, but tensors can have varying lengths).\n",
    "* **String Tensors (`tf.string`):** Tensors of byte strings. `tf.strings` provides ops for them.\n",
    "* **Sets (`tf.sets`):** Represented as regular tensors (or sparse tensors).\n",
    "* **Queues (`tf.queue`):** e.g., `FIFOQueue`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Models and Training Algorithms\n",
    "\n",
    "This section covers how to create custom losses, metrics, layers, and models. \n",
    "\n",
    "### Custom Loss Functions\n",
    "\n",
    "Let's say we want to use the Huber loss. We can just define it as a Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Load and prepare the California Housing dataset (from Ch 10)\n",
    "# This is needed to have data to compile and fit a model.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a simple model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code reproduction from Chapter 12 starts here\n",
    "\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "# Now we can compile the model with this custom loss\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2, \n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models That Contain Custom Components\n",
    "\n",
    "When you save a model, Keras saves the *name* of the loss function. When you load it, you must provide a dictionary mapping the name to the actual function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_custom_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"my_model_with_custom_loss.h5\",\n",
    "    custom_objects={\"huber_fn\": huber_fn}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if your custom function has hyperparameters (like the `threshold` in Huber loss)? The threshold will not be saved. \n",
    "\n",
    "One way is to create a function that returns a configured loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_custom_loss_threshold_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must specify the threshold value when loading\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_custom_loss_threshold_2.h5\",\n",
    "    custom_objects={\"huber_fn\": create_huber(2.0)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way is to subclass `keras.losses.Loss` and implement the `get_config()` method. This allows Keras to save and load the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_custom_loss_class.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When loading, Keras will call the from_config() method, \n",
    "# which creates an instance and passes the config to the constructor.\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_custom_loss_class.h5\",\n",
    "    custom_objects={\"HuberLoss\": HuberLoss}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Activation Functions, Initializers, Regularizers, and Constraints\n",
    "\n",
    "Most of these can be implemented as simple functions. Keras will handle them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # equivalent to tf.nn.softplus(z) or keras.activations.softplus\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32): # equivalent to keras.initializers.glorot_normal\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights): # equivalent to keras.regularizers.l1(0.01)\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # equivalent to keras.constraints.nonneg() or tf.nn.relu\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These custom functions can be used normally:\n",
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your custom component needs to save its state (like hyperparameters), you should subclass the appropriate Keras class, such as `keras.regularizers.Regularizer`, `keras.constraints.Constraint`, `keras.initializers.Initializer`, or `keras.layers.Layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "Losses and metrics are not the same:\n",
    "* **Losses:** Used by Gradient Descent to train the model. Must be differentiable (mostly) and should not be 0 everywhere. Being easily interpretable is not required.\n",
    "* **Metrics:** Used to evaluate a model. Must be interpretable. Can be non-differentiable or have 0 gradients.\n",
    "\n",
    "In simple cases, you can define a metric just like a loss function. For example, our `huber_fn` could be used as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming Metrics (Stateful Metrics)\n",
    "\n",
    "Some metrics, like precision, cannot be simply averaged across batches. For example, if Batch 1 has 4/5 precision (80%) and Batch 2 has 0/3 precision (0%), the average is 40%. But the *overall* precision is (4+0)/(5+3) = 4/8 (50%).\n",
    "\n",
    "For this, we need a **streaming metric** (or stateful metric) that can keep track of its state over multiple batches. Keras's built-in metrics, like `keras.metrics.Precision`, do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
    "# Note: The output here is the mean precision *so far*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.result() # This is the 50% we calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.variables # Shows the tracked true_positives and false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your own streaming metric, you subclass `keras.metrics.Metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold) # use the function we defined earlier\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layers\n",
    "\n",
    "If your model needs an exotic layer, you can create a custom one.\n",
    "\n",
    "**Stateless Layers:** For simple, stateless custom layers, you can use `keras.layers.Lambda`. For example, this layer applies the `tf.exp()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stateful Layers:** For layers with weights (state), you must subclass `keras.layers.Layer`. \n",
    "You need to implement:\n",
    "* `__init__()`: Define hyperparameters.\n",
    "* `build()`: Create the layer's variables (weights). This is called the first time the layer is used, when Keras knows the input shape.\n",
    "* `call()`: Perform the layer's operations.\n",
    "* `get_config()`: To allow the layer to be saved and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a layer needs to have a different behavior during training and testing (e.g., `Dropout`, `BatchNormalization`), it must have a `training` argument in its `call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Models\n",
    "\n",
    "To create a custom model, you subclass `keras.Model`. This is very similar to creating a custom layer. This is useful for building complex architectures, such as models with skip connections (like a residual block).\n",
    "\n",
    "First, let's define a residual block layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "                      for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this custom layer to build a model with the Subclassing API. The `Model` class is a subclass of `Layer`, but it provides extra features like `compile()`, `fit()`, `evaluate()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3): # Book example shows a loop\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Metrics Based on Model Internals\n",
    "\n",
    "Sometimes you want a loss or metric to be based on the internal parts of a model (e.g., hidden layer activations or weights), not just on the predictions and labels. This is often used for regularization.\n",
    "\n",
    "You can do this by computing the loss in the `call()` method of a custom model and adding it to the model's main loss by calling the `add_loss()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                      for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        # The reconstruction layer is created in build()\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss) # This is the custom internal loss\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients Using Autodiff\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "To write a custom training loop, you need to be able to compute gradients manually. TensorFlow's autodiff feature, `tf.GradientTape`, makes this easy. \n",
    "\n",
    "You create a `tf.GradientTape()` context, and it will automatically record every operation that involves a variable. Then, you can call its `gradient()` method to compute the gradients of a result (like the loss) with regard to the variables (like the model weights).\n",
    "\n",
    "This is far more efficient and accurate than manual differentiation or finite difference approximation (numerical differentiation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the tape is automatically erased immediately after `gradient()` is called. If you need to call `gradient()` more than once, you must make the tape **persistent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2) # This works now\n",
    "del tape\n",
    "print(dz_dw1, dz_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tape only tracks operations involving variables. If you want to compute gradients with respect to non-variable tensors (like constants), you must call `tape.watch()` on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop gradients from backpropagating through some part of your network by using `tf.stop_gradient()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(gradients) # The gradient for w2 will be None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into numerical issues (like `NaN` gradients), you can provide a custom, numerically stable gradient function using the `@tf.custom_gradient` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loops\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "Sometimes, the `fit()` method is not flexible enough. For example, the Wide & Deep paper used two different optimizers, but `fit()` only uses one. In such rare cases, you can write your own training loop.\n",
    "\n",
    "This process is as follows:\n",
    "1.  Create two nested loops: one for epochs, one for batches.\n",
    "2.  Get the batch of inputs and labels.\n",
    "3.  Open a `tf.GradientTape()` context.\n",
    "4.  Inside the context, make a prediction (the forward pass) and compute the loss.\n",
    "5.  Outside the context, use the tape to compute the gradients of the loss with regard to the model's trainable variables.\n",
    "6.  Apply these gradients to the optimizer to perform a Gradient Descent step.\n",
    "7.  Update your metrics and display the status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a simple model (same as before)\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get a random batch of data\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# Helper function to print the status\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and components\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train_scaled) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The custom training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        # 1. Get the batch\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        \n",
    "        # 2. Open the tape\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 3. Make prediction and compute loss\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        \n",
    "        # 4. Compute gradients\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # 5. Apply gradients\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # Handle weight constraints (if any)\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "\n",
    "        # 6. Update metrics and display status\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    \n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Functions and Graphs\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "While TensorFlow 2.x runs in *eager mode* by default (like regular Python), its real power comes from its graph features. When you decorate a Python function with `@tf.function`, TensorFlow \"converts\" it into a **TF Function**.\n",
    "\n",
    "1.  **Tracing:** The first time you call the TF Function, TF *traces* it. It runs the function in *graph mode*, meaning each TF operation adds a node to a computation graph. The arguments are treated as symbolic tensors (placeholders with a shape and dtype, but no value).\n",
    "2.  **AutoGraph:** During tracing, TF's AutoGraph feature analyzes the Python source code to capture control flow statements (`if`, `for`, `while`) and converts them into TensorFlow graph operations (e.g., `tf.cond()`, `tf.while_loop()`).\n",
    "3.  **Optimization:** TensorFlow then optimizes this graph (e.g., pruning unused nodes).\n",
    "4.  **Execution:** On subsequent calls *with the same input signature* (i.e., same argument types and shapes), TensorFlow uses the optimized graph to run the computations, which is much faster than running the Python code.\n",
    "\n",
    "Keras automatically converts your model's `call()` method and all custom components (losses, metrics, etc.) into TF Functions for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "# Calling it with a Python number\n",
    "print(cube(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling it with a tensor\n",
    "print(cube(tf.constant(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TF Function\n",
    "tf_cube = tf.function(cube)\n",
    "print(tf_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_cube(2)) # Returns a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use it as a decorator\n",
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "# Check the original Python function\n",
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGraph and Tracing\n",
    "\n",
    "If you add a `print()` statement to a TF function, it will only run during tracing, not during execution. This is because `print()` is a Python side effect, not a TF operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    print(\"x =\", x)\n",
    "    return x ** 3\n",
    "\n",
    "result = tf_cube(tf.constant(2.0))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tf_cube(tf.constant(3.0)) # No print, reuses the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tf_cube(tf.constant(4.0)) # No print, reuses the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Python value creates a new signature, so it traces again\n",
    "result = tf_cube(2)\n",
    "result = tf_cube(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new shape also creates a new signature\n",
    "result = tf_cube(tf.constant([[1., 2.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more general shape (None) is created\n",
    "result = tf_cube(tf.constant([[3., 4.], [5., 6.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Function Rules\n",
    "\n",
    "To ensure your function can be converted, you must follow some rules:\n",
    "\n",
    "1.  **Use TF ops:** Only TensorFlow constructs (tensors, ops, variables) can be part of the graph. If you use external libraries (like NumPy) or Python functions (like `print()`), they will **only run during tracing**, not during graph execution.\n",
    "2.  **`tf.Variable` creation:** Create variables *outside* the TF function (e.g., in the `__init__` or `build` method), ideally on the first call only. To modify a variable, use its `.assign()` method, not the `=` operator.\n",
    "3.  **Source Code:** TensorFlow needs access to the function's source code to use AutoGraph.\n",
    "4.  **Loops:** Use `tf.range()` instead of Python's `range()` if you want the loop to be *dynamic* (part of the graph). If you use `range()`, the loop will be *static* (unrolled during tracing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
