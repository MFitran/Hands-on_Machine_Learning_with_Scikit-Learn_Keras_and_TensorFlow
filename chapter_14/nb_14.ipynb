{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14 – Deep Computer Vision Using Convolutional Neural Networks\n",
    "\n",
    "This notebook contains all the code samples and solutions to the exercises in chapter 14 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition* (O'Reilly). *Note: all code examples are based on the author's original GitHub repository.* \n",
    "\n",
    "**Assignment Instructions:**\n",
    "Per the assignment guidelines, this notebook reproduces the code from Chapter 14. It also includes theoretical explanations and summaries for each concept, as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter dives into **Convolutional Neural Networks (CNNs)**, the architecture that powers the vast majority of modern computer vision systems. CNNs are inspired by the architecture of the animal visual cortex.\n",
    "\n",
    "Key concepts covered include:\n",
    "\n",
    "* **Convolutional Layers:** These are the core building blocks of CNNs. Instead of being fully connected, neurons in a convolutional layer are only connected to a small region of the layer before it, known as their **receptive field**. All neurons in a layer share the same weights (called a **filter** or kernel), allowing the layer to detect the same feature (e.g., a vertical edge) anywhere in the image.\n",
    "* **Pooling Layers:** These layers are used to subsample (shrink) the image, which reduces the computational load, memory usage, and the number of parameters, thereby limiting overfitting. **Max pooling** is the most common type.\n",
    "* **CNN Architectures:** We explore the evolution of influential CNN architectures:\n",
    "    * **LeNet-5:** The pioneering 1998 architecture used for handwritten digit recognition.\n",
    "    * **AlexNet:** The 2012 ILSVRC challenge winner, which was much larger and deeper and was the first to stack convolutional layers directly. It also popularized the use of ReLU, dropout, and **data augmentation**.\n",
    "    * **GoogLeNet:** The 2014 winner, which introduced **inception modules**. These modules use 1x1 convolutions as **bottleneck layers** to reduce computation and allow for a much deeper network.\n",
    "    * **ResNet (Residual Network):** The 2015 winner, which introduced **skip connections** (or shortcut connections). These connections allow the network to learn *residual* functions, enabling the training of extremely deep networks (e.g., 152 layers) by mitigating the vanishing/exploding gradients problem.\n",
    "    * **Xception:** A variant of GoogLeNet that uses **depthwise separable convolution layers**, which are more efficient and often perform better.\n",
    "    * **SENet (Squeeze-and-Excitation Network):** The 2017 winner, which boosts performance by adding **SE blocks** that recalibrate feature maps, learning to emphasize important features and suppress irrelevant ones.\n",
    "\n",
    "* **Keras Implementation:** We learn how to implement these layers and models using Keras, including how to use **pretrained models** from `keras.applications` for **Transfer Learning**.\n",
    "\n",
    "* **Advanced Computer Vision Tasks:**\n",
    "    * **Classification and Localization:** Using a single network to output both a class label (classification) and a bounding box (regression) for a single object.\n",
    "    * **Object Detection:** Detecting and localizing *multiple* objects. We discuss techniques like **Fully Convolutional Networks (FCNs)** and the **YOLO (You Only Look Once)** architecture.\n",
    "    * **Semantic Segmentation:** Classifying *every pixel* in an image. This is often done using an FCN with **transposed convolutional layers** for upsampling and skip connections to recover spatial resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 3.7 or later is required for the latest versions of Scikit-Learn), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "The **convolutional layer** is the most important building block of a CNN. \n",
    "\n",
    "* **Local Receptive Fields:** Neurons in the first convolutional layer are not connected to every single pixel in the input image, but only to pixels in their **receptive fields**. This allows the network to focus on small, low-level features first.\n",
    "\n",
    "* **Filters (or Kernels):** A neuron's weights can be represented as a small image the size of its receptive field. This set of weights is called a **filter** (or convolution kernel). For example, a filter might be a 7x7 matrix that detects vertical edges. \n",
    "\n",
    "* **Feature Maps:** A layer full of neurons using the *same filter* outputs a **feature map**, which highlights the areas in an image that activate the filter the most. The fact that all neurons in a feature map share the same parameters (weights and bias) dramatically reduces the number of parameters in the model. Once the CNN learns to recognize a pattern in one location, it can recognize it in any other location.\n",
    "\n",
    "* **Stacking Feature Maps:** A single convolutional layer applies multiple filters and outputs one feature map per filter. It is represented in 3D (height, width, feature maps/channels).\n",
    "\n",
    "* **Strides:** The shift from one receptive field to the next is called the **stride**. Using a larger stride (e.g., 2) will reduce the spatial dimensions of the output layer, which reduces computational load.\n",
    "\n",
    "* **Padding:** To ensure a layer has the same height and width as the previous layer, it is common to add zeros around the inputs. This is called **zero padding**. In Keras, this is set with `padding=\"same\"`. The default, `padding=\"valid\"`, means no padding is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Convolutional Layer in TensorFlow\n",
    "\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "# Load sample images\n",
    "# We need to pip install Pillow for this\n",
    "try:\n",
    "    china = load_sample_image(\"china.jpg\") / 255\n",
    "    flower = load_sample_image(\"flower.jpg\") / 255\n",
    "except ImportError:\n",
    "    print(\"Please install the Pillow library: pip install -U Pillow\")\n",
    "    # As a fallback for the notebook to run, create dummy images\n",
    "    china = np.random.rand(427, 640, 3)\n",
    "    flower = np.random.rand(427, 640, 3)\n",
    "    \n",
    "images = np.array([china, flower])\n",
    "batch_size, height, width, channels = images.shape\n",
    "\n",
    "# Create 2 filters (7x7)\n",
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1  # vertical line\n",
    "filters[3, :, :, 1] = 1  # horizontal line\n",
    "\n",
    "# Apply the filters using TensorFlow's low-level API\n",
    "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
    "\n",
    "# Plot the first image's second feature map (which used the horizontal filter)\n",
    "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") \n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Horizontal Line Feature Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Using keras.layers.Conv2D\n",
    "\n",
    "# In a real CNN, we let the network learn the best filters.\n",
    "# We just need to define the layer with its hyperparameters.\n",
    "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\n",
    "                             padding=\"same\", activation=\"relu\")\n",
    "\n",
    "# The weights are initialized randomly and learned during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Memory Requirements\n",
    "\n",
    "Convolutional layers require a huge amount of RAM, especially during training, because the reverse pass of backpropagation needs all the intermediate values computed during the forward pass. \n",
    "\n",
    "For example, a convolutional layer with 200 feature maps of size 150x100 (using 32-bit floats) will require 200 * 150 * 100 * 32 = 96 million bits (12 MB) of RAM for just *one instance*. If a training batch contains 100 instances, this single layer will use 1.2 GB of RAM.\n",
    "\n",
    "If you get an out-of-memory error, you can try:\n",
    "* Reducing the mini-batch size.\n",
    "* Reducing dimensionality using a larger stride.\n",
    "* Removing a few layers.\n",
    "* Using 16-bit floats instead of 32-bit floats.\n",
    "* Distributing the CNN across multiple devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "**Pooling layers** are used to **subsample (shrink)** the input image to reduce the computational load, memory usage, and number of parameters (limiting overfitting).\n",
    "\n",
    "A pooling neuron has no weights; it simply aggregates the inputs using a function like `max` or `mean`.\n",
    "\n",
    "* **Max Pooling (`MaxPool2D`):** This is the most common type. It takes the maximum value from its receptive field. This is a destructive operation (it drops 75% of the input values with a 2x2 kernel and stride of 2), but it works well because it preserves only the strongest features. It also provides some **translation invariance** (a small shift in the input will not change the output).\n",
    "* **Average Pooling (`AvgPool2D`):** Computes the mean of its inputs. It was popular in the past but is less used now, as max pooling generally performs better.\n",
    "* **Global Average Pooling (`GlobalAvgPool2D`):** This layer computes the mean of each *entire* feature map. It's extremely destructive (it outputs a single number per feature map), but it's very useful as an output layer because it dramatically reduces the number of parameters needed."
   ]
  },
nbsp; {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Max Pooling and Global Average Pooling\n",
    "\n",
    "# A simple 2x2 max pooling layer\n",
    "max_pool = keras.layers.MaxPool2D(pool_size=2)\n",
    "\n",
    "# A global average pooling layer\n",
    "global_avg_pool = keras.layers.GlobalAvgPool2D()\n",
    "\n",
    "# This is equivalent to:\n",
    "global_avg_pool_lambda = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architectures\n",
    "\n",
    "### Theoretical Explanation: Typical Architecture\n",
    "\n",
    "A typical CNN architecture stacks a few convolutional layers (each followed by a ReLU), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on.\n",
    "\n",
    "The image gets smaller as it progresses through the network, but it also typically gets *deeper* (i.e., has more feature maps). It's common to double the number of filters after each pooling layer.\n",
    "\n",
    "At the top of the stack, a regular feedforward network (a few dense layers) is added to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Simple CNN for Fashion MNIST\n",
    "\n",
    "# Load and prepare data (as in Chapter 10)\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_valid = X_train_full[:-5000] / 255.0, X_train_full[-5000:] / 255.0\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Add a channel dimension (for grayscale)\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # 1. Convolutional base\n",
    "    keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\", \n",
    "                          input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    \n",
    "    # 2. Fully connected network (classifier head)\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
  _count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, \n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "print(\"Test set evaluation:\")\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Famous CNN Architectures\n",
    "\n",
    "* **LeNet-5 (1998):** The classic. Stacked convolution and average pooling layers, ending with dense layers.\n",
    "* **AlexNet (2012):** Similar to LeNet-5 but much larger and deeper. It was the first to stack convolutional layers directly on top of one another. It popularized ReLU, dropout, and data augmentation.\n",
    "* **GoogLeNet (2014):** Introduced **inception modules**, which are blocks of layers that act like a powerful convolutional layer. It used 1x1 convolutions as **bottleneck layers** to reduce computation, allowing the network to be much deeper with 10x fewer parameters than AlexNet.\n",
    "* **ResNet (2015):** Introduced **residual units (RUs)** with **skip connections** (or shortcut connections). A skip connection adds the input of a layer to its output. This allows the network to learn $f(x) = h(x) - x$ (the *residual*) instead of $h(x)$. This *residual learning* makes it possible to train extremely deep networks (e.g., 152 layers) by solving the vanishing/exploding gradients problem.\n",
    "* **Xception (2016):** Uses **depthwise separable convolution layers**, which separate spatial and cross-channel pattern learning. They are much more parameter-efficient and often perform better.\n",
    "* **SENet (2017):** Boosts existing architectures (like ResNet) by adding **SE blocks**. An SE block is a small neural network that analyzes the output of a unit and **recalibrates** the feature maps—learning to boost the most relevant features and suppress the irrelevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Reproduction: Implementing a ResNet-34 CNN\n",
    "\n",
    "We can build ResNet-34 by first creating a custom `ResidualUnit` layer and then stacking them in a `Sequential` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            keras.layers.Conv2D(filters, 3, strides=strides,\n",
    "                              padding=\"same\", use_bias=False),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            keras.layers.Conv2D(filters, 3, strides=1,\n",
    "                              padding=\"same\", use_bias=False),\n",
    "            keras.layers.BatchNormalization()]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                keras.layers.Conv2D(filters, 1, strides=strides,\n",
    "                                  padding=\"same\", use_bias=False),\n",
    "               keras.layers.BatchNormalization()]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, build the full ResNet-34 model\n",
    "# Note: This model is built for 224x224 images, not 28x28 MNIST images\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3],\n",
    "                              padding=\"same\", use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
    "\n",
    "# Stack of Residual Units\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "\n",
    "# Classifier head\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\")) # 10 classes, e.g. CIFAR10\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained Models from Keras\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "It is rarely necessary to implement a standard model like ResNet manually. Keras provides many pretrained networks in the `keras.applications` package.\n",
    "\n",
    "We can load a ResNet-50 model, pretrained on the ImageNet dataset, with a single line of code.\n",
    "\n",
    "1.  **Load the model:** `keras.applications.resnet50.ResNet50(weights=\"imagenet\")`.\n",
    "2.  **Resize images:** The model expects a specific input size (e.g., 224x224). We must resize our images to match.\n",
    "3.  **Preprocess:** Each model provides a `preprocess_input()` function that must be applied to the images.\n",
    "4.  **Predict:** Use `model.predict()` to get probabilities for all 1,000 ImageNet classes.\n",
    "5.  **Decode:** Use `decode_predictions()` to get the top K predictions with human-readable labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Using a Pretrained ResNet-50\n",
    "\n",
    "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\n",
    "\n",
    "# Resize our sample images to 224x224\n",
    "images_resized = tf.image.resize(images, [224, 224])\n",
    "\n",
    "# Preprocess the images (models expect 0-255 range, not 0-1)\n",
    "inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\n",
    "\n",
    "# Make predictions\n",
    "Y_proba = model.predict(inputs)\n",
    "\n",
    "# Decode and print the top 3 predictions\n",
    "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(\"Image #{}\".format(image_index))\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Transfer Learning\n",
    "\n",
    "If we want to build an image classifier for a task with little training data (e.g., classifying flowers), we can use **transfer learning**. We reuse the lower layers of a pretrained model, which act as powerful feature detectors.\n",
    "\n",
    "1.  Load a pretrained model (e.g., Xception) *without* its top layers (`include_top=False`).\n",
    "2.  Add our own output layer (e.g., a `GlobalAveragePooling2D` followed by a `Dense` layer with the new number of classes).\n",
    "3.  **Freeze** the weights of the base model's layers (`layer.trainable = False`) so they are not damaged during initial training.\n",
    "4.  Compile and train the model for a few epochs. Only the new output layer's weights will be trained.\n",
    "5.  **Unfreeze** all the base layers (or just the top ones) and compile the model again with a **very low learning rate**.\n",
    "6.  Continue training to **fine-tune** the pretrained weights for the new task. This will usually result in a significant performance boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Transfer Learning for the 'tf_flowers' dataset\n",
    "# This requires the tensorflow_datasets library: pip install -U tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples # 3670\n",
    "class_names = info.features[\"label\"].names # ['dandelion', 'daisy', ...]\n",
    "n_classes = info.features[\"label\"].num_classes # 5\n",
    "\n",
    "# 2. Split the dataset (75% train, 15% valid, 10% test)\n",
    "test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\n",
    "test_set = tfds.load(\"tf_flowers\", split=test_split, as_supervised=True)\n",
    "valid_set = tfds.load(\"tf_flowers\", split=valid_split, as_supervised=True)\n",
    "train_set = tfds.load(\"tf_flowers\", split=train_split, as_supervised=True)\n",
    "\n",
    "# 3. Create the preprocessing pipeline\n",
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label\n",
    "\n",
    "batch_size = 32\n",
    "train_set = train_set.shuffle(1000).map(preprocess).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create the model with transfer learning\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# 5. Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 6. Compile and train the new output layer\n",
    "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5, validation_data=valid_set)\n",
    "\n",
    "# 7. Unfreeze and fine-tune with a low learning rate\n",
    "print(\"Unfreezing base model for fine-tuning...\")\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Localization\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "What if you want to know *where* the object is in the picture? This task is called **localization**. It can be expressed as a regression task: we ask the model to predict a **bounding box** around the object. A common approach is to predict 4 numbers: the horizontal and vertical coordinates of the object's center, and its height and width.\n",
    "\n",
    "To do this, we create a model with *two* output layers:\n",
    "1.  A classification head (a `Dense` layer with `softmax`) that predicts the class.\n",
    "2.  A regression head (a `Dense` layer with 4 units and no activation) that predicts the bounding box coordinates.\n",
    "\n",
    "The model is then trained with two losses (e.g., cross-entropy for classification and MSE for regression), and their sum is used for training.\n",
    "\n",
    "The most common metric for evaluating bounding boxes is the **Intersection over Union (IoU)**: the area of overlap between the predicted and target boxes, divided by the area of their union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Model for Classification and Localization\n",
    "\n",
    "# We'll reuse the base_model from the transfer learning section\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = keras.layers.Dense(4)(avg) # 4 units for (x, y, h, w)\n",
    "\n",
    "model = keras.Model(inputs=base_model.input, \n",
    "                  outputs=[class_output, loc_output])\n",
   "\n",
    "# We give a weight to each loss\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "              loss_weights=[0.8, 0.2], # e.g., 80% for class, 20% for localization\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# To train this, you would need a dataset with both labels and bounding boxes\n",
    "# (images, (class_labels, bounding_boxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "The task of classifying and localizing *multiple* objects in an image is called **object detection**.\n",
    "\n",
    "* **Sliding Window Approach:** A classic approach is to slide a CNN across the image in small steps and run it on many small patches. This is very slow.\n",
    "* **Fully Convolutional Networks (FCNs):** A much faster way is to convert the dense layers at the top of a CNN into convolutional layers. A dense layer with $N$ neurons can be converted to a `Conv2D` layer with $N$ filters, a kernel size equal to the input feature map size, and `\"valid\"` padding. This FCN can then process a large image *only once* and output a grid of predictions, effectively doing the sliding window in one pass.\n",
    "* **Non-Max Suppression:** This FCN approach produces many bounding boxes for the same object. To clean them up, we use **non-max suppression**: first, we drop boxes with a low *objectness score* (a new output that estimates the probability that an object is present). Then, we find the box with the highest score and discard all other boxes that have a high IoU (overlap) with it. We repeat this until no more boxes can be discarded.\n",
    "\n",
    "### YOLO (You Only Look Once)\n",
    "The **YOLO** architecture is an extremely fast and accurate object detection model.\n",
    "1.  It uses an FCN to output a grid of predictions (e.g., 8x8) in one pass.\n",
    "2.  For each grid cell, it predicts 5 bounding boxes and one objectness score for each box, plus 20 class probabilities (for the 20 classes in the PASCAL VOC dataset).\n",
    "3.  Instead of predicting absolute coordinates, it predicts coordinates *relative* to the grid cell.\n",
    "4.  It predicts the size of the box relative to predefined **anchor boxes** (or bounding box priors), which are representative box shapes found by running K-Means on the training set box dimensions. This helps the network learn to predict reasonable boxes faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "In **semantic segmentation**, each *pixel* is classified according to the class of the object it belongs to (e.g., road, car, pedestrian, building). Different objects of the same class (e.g., two different bicycles) are not distinguished.\n",
    "\n",
    "The main difficulty is that as an image goes through a CNN, it gradually loses its spatial resolution (due to pooling or strided convolutions), making it hard to make precise, pixel-level predictions.\n",
    "\n",
    "A common solution is to:\n",
    "1.  Take a pretrained CNN and turn it into an FCN.\n",
    "2.  **Upsample** the output. Instead of a large upsampling factor (e.g., x32), it's better to upsample in stages. The standard way to upsample is with a **transposed convolutional layer** (`Conv2DTranspose`), which learns to perform the upsampling.\n",
    "3.  Add **skip connections** from lower, higher-resolution layers in the network. This allows the network to recover the fine-grained spatial information that was lost during downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "From Chapter 14, page 496:\n",
    "\n",
    "1.  What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "2.  Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding... What is the total number of parameters in the CNN? ...at least how much RAM will this network require...?\n",
    "3.  If your GPU runs out of memory... what are five things you could try to solve the problem?\n",
    "4.  Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "5.  When would you want to add a local response normalization layer?\n",
    "6.  Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
    "7.  What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
    "8.  What is the main technical difficulty of semantic segmentation?\n",
    "9.  Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n",
    "10. Use transfer learning for large image classification... (e.g., classify your own pictures or use an existing dataset from TensorFlow Datasets).\n",
    "11. Go through TensorFlow’s Style Transfer tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
Two-way
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
