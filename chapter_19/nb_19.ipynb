{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
    "\n",
    "This notebook contains the code reproductions and theoretical explanations for Chapter 19 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter covers the crucial final steps of a machine learning project: deploying a trained model to production and scaling up training for very large datasets and models.\n",
    "\n",
    "Key topics covered include:\n",
    "\n",
    "* **Serving TensorFlow Models:** We learn how to export a model to TensorFlow's `SavedModel` format. We then deploy this model using **TensorFlow Serving (TF Serving)**, a high-performance serving system. We learn how to install it (using Docker), run it, and query it using both its REST and gRPC APIs.\n",
    "\n",
    "* **Deploying to the Cloud:** We explore how to deploy a model to **Google Cloud AI Platform**, which provides a fully managed, scalable serving solution that handles versioning, monitoring, and more. We also create a service account and write client code to query the deployed model.\n",
    "\n",
    "* **Mobile and Web Deployment:** We briefly look at **TensorFlow Lite (TFLite)** for deploying models on mobile and embedded devices, focusing on optimization techniques like **post-training quantization** and **quantization-aware training**. We also look at **TensorFlow.js** for running models directly in a web browser.\n",
    "\n",
    "* **Speeding Up Training with GPUs:** We discuss how to use GPUs to accelerate training, either by getting a local GPU, using a GPU-equipped VM on the cloud, or using Google's **Colaboratory (Colab)**. We also cover essential techniques for managing GPU RAM.\n",
    "\n",
    "* **Distributed Training:** We learn how to train a single model across multiple devices and servers. We explore the concepts of **model parallelism** and **data parallelism** (including synchronous vs. asynchronous updates).\n",
    "\n",
    "* **Distribution Strategies API:** We use TensorFlow's `tf.distribute.Strategy` API to easily implement data parallelism. This includes `MirroredStrategy` (for multiple GPUs on one machine) and `MultiWorkerMirroredStrategy` (for multiple servers).\n",
    "\n",
    "* **Hyperparameter Tuning on AI Platform:** Finally, we see how to use Google Cloud's powerful black-box optimization service to perform large-scale hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment. We'll also train and save a basic Fashion MNIST model to use for deployment examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Common setup for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare Fashion MNIST data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_new = X_test[:3]\n",
    "\n",
    "# Train a simple model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train_full, y_train_full, epochs=10, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "print(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving a TensorFlow Model\n",
    "\n",
    "Once a model is trained, you need to deploy it. Instead of embedding it in every application, it's best to wrap it in a dedicated web service. **TensorFlow Serving** is a high-performance, battle-tested system designed for this. It can serve multiple models, handle versioning, and automatically deploy the latest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting SavedModels\n",
    "\n",
    "The first step is to export the model to TensorFlow's **`SavedModel`** format. This format is a directory containing the model's computation graph (as a protobuf) and its weights (variables). It's the universal, language-neutral format for TensorFlow models.\n",
    "\n",
    "We can use `tf.saved_model.save()` or simply the model's `save()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"0001\"\n",
    "model_name = \"my_mnist_model\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "\n",
    "# Export the model to SavedModel format\n",
    "tf.saved_model.save(model, model_path)\n",
    "\n",
    "# You could also use: \n",
    "# model.save(model_path) # This saves as SavedModel if path is not .h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the contents of the `SavedModel` using the `saved_model_cli` tool. You would run this in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command is for your shell/terminal\n",
    "!saved_model_cli show --dir {model_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `SavedModel` contains one or more *metagraphs* (a graph + function signatures), each identified by tags. When saving a Keras model, it saves a single metagraph tagged `\"serve\"`. This metagraph contains the `serving_default` signature, which corresponds to the model's `call()` function (i.e., `model.predict()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing TensorFlow Serving\n",
    "\n",
    "The easiest way to install TF Serving is using Docker. \n",
    "\n",
    "1.  **Pull the image:** `docker pull tensorflow/serving`\n",
    "2.  **Run the container:** (This command is for your terminal, not this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Get the absolute path to your model directory\n",
    "ML_PATH=$(pwd)\n",
    "\n",
    "docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
    "   -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\\n",
    "   -e MODEL_NAME=my_mnist_model \\\n",
    "   tensorflow/serving\n",
    "```\n",
    "This command:\n",
    "* Runs the `tensorflow/serving` image.\n",
    "* Forwards ports `8501` (for REST API) and `8500` (for gRPC API).\n",
    "* Mounts your local `my_mnist_model` directory into the container's `/models/` directory.\n",
    "* Sets the `MODEL_NAME` environment variable so TF Serving knows which model to serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying TF Serving through the REST API\n",
    "\n",
    "The REST API is simple and uses JSON. It's great for most use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the request in JSON format\n",
    "# The 'instances' key holds our batch of new images (as a list)\n",
    "input_data_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Send the POST request to the server's REST API endpoint\n",
    "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
    "\n",
    "try:\n",
    "    response = requests.post(SERVER_URL, data=input_data_json)\n",
    "    response.raise_for_status() # Raise an exception in case of error\n",
    "    response = response.json()\n",
    "    \n",
    "    # 3. Parse the JSON response\n",
    "    y_proba = np.array(response[\"predictions\"])\n",
    "    print(y_proba.round(2))\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Could not connect to TF Serving. Is the Docker container running?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying TF Serving through the gRPC API\n",
    "\n",
    "gRPC is a more efficient, binary protocol based on protocol buffers. It's much faster than REST and is recommended for high-performance applications, especially when transferring large amounts of data.\n",
    "\n",
    "You'll need to install the `tensorflow-serving-api` library: `pip install -U tensorflow-serving-api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import grpc\n",
    "    from tensorflow_serving.apis import predict_pb2\n",
    "    from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "    # 1. Create the request protobuf\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = \"serving_default\"\n",
    "    input_name = model.input_names[0]\n",
    "    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n",
    "\n",
    "    # 2. Open a gRPC channel and send the request\n",
    "    channel = grpc.insecure_channel('localhost:8500')\n",
    "    predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    response = predict_service.Predict(request, timeout=10.0)\n",
    "\n",
    "    # 3. Convert the response protobuf back to a tensor\n",
    "    output_name = model.output_names[0]\n",
    "    outputs_proto = response.outputs[output_name]\n",
    "    y_proba_grpc = tf.make_ndarray(outputs_proto)\n",
    "    print(y_proba_grpc.round(2))\n",
    "\n",
    "except (ImportError, grpc.framework.interfaces.face.face.AbortionError):\n",
    "    print(\"gRPC setup failed or server not found.\")\n",
    "    print(\"Install with: pip install -U tensorflow-serving-api grpcio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Prediction Service on GCP AI Platform\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "Instead of managing Docker containers yourself, you can use a fully managed service like **Google Cloud AI Platform**. \n",
    "\n",
    "The process is:\n",
    "1.  **Set up GCP:** Create a project, enable billing, and enable the AI Platform and Cloud Storage APIs.\n",
    "2.  **Create a GCS Bucket:** Create a bucket in Google Cloud Storage (GCS) to store your models.\n",
    "3.  **Upload SavedModel:** Upload your `SavedModel` directory (e.g., `my_mnist_model/0001`) to the bucket.\n",
    "4.  **Create a Model:** In the AI Platform console, create a \"model\" resource (this is just a container for versions).\n",
    "5.  **Create a Version:** Create a \"version\" of your model, pointing it to the `SavedModel` directory in your GCS bucket. AI Platform will spin up TF Serving instances to serve your model.\n",
    "6.  **Create a Service Account:** For security, create a service account with the \"ML Engine Developer\" role and download its JSON private key.\n",
    "7.  **Query the Service:** Use Google's API Client Library to authenticate with the service account key and send prediction requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assumes you have followed the steps in the book (1-6).\n",
    "# 1. Set the service account key environment variable\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"my_service_account_key.json\"\n",
    "\n",
    "# 2. Build the client resource object\n",
    "import googleapiclient.discovery\n",
    "\n",
    "project_id = \"your-gcp-project-id\" # CHANGE THIS\n",
    "model_id = \"my_mnist_model\"\n",
    "model_path = \"projects/{}/models/{}\".format(project_id, model_id)\n",
    "\n",
    "# To use a specific version:\n",
    "# model_path += \"/versions/0001\"\n",
    "\n",
    "# This line might fail if you haven't installed the client library:\n",
    "# !pip install -U google-api-python-client\n",
    "\n",
    "# ml_resource = googleapiclient.discovery.build(\"ml\", \"v1\").projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a function to query the service\n",
    "def predict_gcp(ml_resource, X):\n",
    "    input_data_json = {\"signature_name\": \"serving_default\",\n",
    "                       \"instances\": X.tolist()}\n",
    "    request = ml_resource.predict(name=model_path, body=input_data_json)\n",
    "    response = request.execute()\n",
    "    if \"error\" in response:\n",
    "        raise RuntimeError(response[\"error\"])\n",
    "    output_name = model.output_names[0]\n",
    "    return np.array([pred[output_name] for pred in response[\"predictions\"]])\n",
    "\n",
    "# 4. Query the model (this will fail unless you set up GCP)\n",
    "# try:\n",
    "#     Y_probas = predict_gcp(ml_resource, X_new)\n",
    "#     print(Y_probas.round(2))\n",
    "# except NameError:\n",
    "#     print(\"GCP client not configured. Skipping GCP prediction.\")\n",
    "# except Exception as e:\n",
    "#     print(\"GCP prediction failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Model to a Mobile or Embedded Device\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "To run models on devices with limited compute, power, and RAM, you need to use **TensorFlow Lite (TFLite)**.\n",
    "\n",
    "TFLite's main tool is a **converter** that takes a `SavedModel` and converts it to a much lighter `.tflite` file (based on FlatBuffers). This converter:\n",
    "1.  **Reduces Model Size:** It prunes unused operations (like training ops) and optimizes the graph.\n",
    "2.  **Reduces Latency & Power:** It can perform **quantization**, which converts the 32-bit float weights to 8-bit integers. \n",
    "\n",
    "**Post-training quantization** is the simplest method: it quantizes the weights after training. This gives a 4x reduction in size, but computations are still done with floats (the 8-bit integers are dequantized at runtime).\n",
    "\n",
    "**Quantization-aware training** is more complex. It adds \"fake\" quantization operations to the model *during* training. This makes the model robust to the loss of precision, resulting in higher accuracy after quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the SavedModel to a TFLite FlatBuffer\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"converted_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use post-training quantization:\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model_quantized = converter.convert()\n",
    "\n",
    "print(f\"Original model size: {os.path.getsize(model_path + '/saved_model.pb')} bytes\")\n",
    "print(f\"TFLite model size: {len(tflite_model)} bytes\")\n",
    "print(f\"Quantized TFLite model size: {len(tflite_model_quantized)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Model to a Web Browser (TensorFlow.js)\n",
    "\n",
    "You can also deploy models to a web browser using **TensorFlow.js**. This is great for privacy (data never leaves the user's machine) and low latency (no server round-trip).\n",
    "\n",
    "1.  **Convert the model:** Use the `tensorflowjs_converter` command-line tool (which comes with the `tensorflowjs` pip package) to convert a `SavedModel` to the TensorFlow.js Layers format (a `model.json` file and several binary `shard` files).\n",
    "2.  **Load in JavaScript:** Use the `tf.loadLayersModel()` function from the TensorFlow.js library to load the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install the converter (run in terminal)\n",
    "# !pip install -U tensorflowjs\n",
    "\n",
    "# 2. Run the converter (run in terminal)\n",
    "# !tensorflowjs_converter --input_format=tf_saved_model {model_path} ./my_tfjs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 3. Example JavaScript code to run the model in a browser\n",
    "// import * as tf from '@tensorflow/tfjs';\n",
    "// const model = await tf.loadLayersModel('https://example.com/tfjs/model.json');\n",
    "// const image = tf.fromPixels(webcamElement);\n",
    "// const prediction = model.predict(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPUs to Speed Up Computations\n",
    "\n",
    "Training deep nets on CPUs is very slow. You can get a massive speedup by using a **Graphics Processing Unit (GPU)**. \n",
    "\n",
    "To do this, you need:\n",
    "1.  A supported GPU (currently NVIDIA, with CUDA Compute Capability 3.5+).\n",
    "2.  The NVIDIA drivers.\n",
    "3.  NVIDIA's CUDA library and cuDNN library.\n",
    "4.  The `tensorflow-gpu` package (or just `tensorflow` as of 2.1, which bundles GPU support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TensorFlow can see the GPU\n",
    "print(\"Is GPU available:\", tf.test.is_gpu_available())\n",
    "print(\"GPU device name:\", tf.test.gpu_device_name())\n",
    "print(\"Physical GPUs:\", tf.config.experimental.list_physical_devices(device_type='GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the GPU RAM\n",
    "\n",
    "By default, TensorFlow automatically grabs *all* the RAM in all available GPUs. If you want to run multiple programs on one machine, you must manage this.\n",
    "\n",
    "**Option 1: Limit visible devices (via environment variable)**\n",
    "This is the cleanest way. Set this in your terminal *before* running your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal A:\n",
    "# $ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0 python3 program_1.py\n",
    "\n",
    "# Terminal B:\n",
    "# $ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=1 python3 program_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Tell TF to only grab memory as needed (memory growth)**\n",
    "This must be done right at the start of your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3: Create virtual devices with a fixed limit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        # Set two virtual GPUs on the first physical GPU, each with 2GiB of RAM\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            physical_gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),\n",
    "             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing Operations on Devices\n",
    "\n",
    "By default, TF places ops on the GPU (`/gpu:0`) if it has a GPU-kernel, otherwise it falls back to the CPU (`/cpu:0`). You can manually control this with a `tf.device()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"): \n",
    "    # These operations will run on the CPU\n",
    "    c = tf.Variable(42.0)\n",
    "    print(c.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models Across Multiple Devices\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "There are two main approaches to training a single model across multiple devices:\n",
    "\n",
    "1.  **Model Parallelism:** You split your model across devices. For example, the bottom layers run on GPU 0 and the top layers run on GPU 1. This is complex and rarely efficient because of the high communication cost between devices.\n",
    "2.  **Data Parallelism:** You replicate the *entire model* on every device. At each step, you give each replica a different mini-batch of data. Each replica computes the gradients for its batch. These gradients are then aggregated (e.g., averaged) and used to update the parameters on *all* replicas. This is the most common and effective strategy.\n",
    "\n",
    "This aggregation can be done:\n",
    "* **Synchronously:** (e.g., **Mirrored Strategy**) All replicas wait until every replica has computed its gradients. The gradients are averaged (using an **AllReduce** algorithm), and every replica applies the same update. This is the simplest and most common approach.\n",
    "* **Asynchronously:** (e.g., **Parameter Server Strategy**) Replicas run independently. When a replica finishes its gradients, it sends them to a \"parameter server,\" which updates the central parameters and sends the new parameters back. This avoids waiting but can lead to *stale gradients*, which can destabilize training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training at Scale Using the Distribution Strategies API\n",
    "\n",
    "TensorFlow's `tf.distribute.Strategy` API makes data parallelism incredibly simple. You just create a strategy object and define/compile your Keras model *within its scope*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MirroredStrategy: For synchronous training on all GPUs on one machine.\n",
    "\n",
    "# List available devices (to see if you have multiple GPUs)\n",
    "print(tf.config.experimental.list_physical_devices(\"GPU\"))\n",
    "\n",
    "# Create the strategy\n",
    "distribution = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# If you only want to use a subset of GPUs:\n",
    "# distribution = tf.distribute.MirroredStrategy([\"/gpu:0\", \"/gpu:1\"])\n",
    "\n",
    "with distribution.scope():\n",
    "    mirrored_model = keras.models.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    mirrored_model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "# The batch size must be divisible by the number of replicas (GPUs)\n",
    "batch_size = 64 # e.g., if 2 GPUs, each gets 32 instances\n",
    "mirrored_model.fit(X_train_scaled, y_train, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on a TensorFlow Cluster\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "A **TF Cluster** is a group of TF processes (tasks) running on different machines, talking to each other. Each task has a **type** (job) and an **index**:\n",
    "* **`\"worker\"`:** A task that performs computations (usually on a GPU).\n",
    "* **`\"chief\"`:** A special worker (usually `worker:0`) that handles extra work like saving checkpoints and writing TensorBoard logs.\n",
    "* **`\"ps\"`:** A **Parameter Server** task. It only stores and updates model parameters. Used by the `ParameterServerStrategy`.\n",
    "\n",
    "To configure a cluster, you must set the `TF_CONFIG` environment variable on *each machine* before it starts. This JSON variable defines the addresses of all tasks (`cluster` key) and the current task's role (`task` key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example TF_CONFIG for worker 0\n",
    "cluster_spec = {\n",
    "    \"worker\": [\n",
    "        \"machine-a.example.com:2222\",  # /job:worker/task:0\n",
    "        \"machine-b.example.com:2222\"   # /job:worker/task:1\n",
    "    ],\n",
    "    \"ps\": [\"machine-a.example.com:2221\"] # /job:ps/task:0\n",
    "}\n",
    "\n",
    "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "    \"cluster\": cluster_spec,\n",
    "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MultiWorkerMirroredStrategy (for synchronous multi-worker training)\n",
    "# You would run this same script on all workers.\n",
    "\n",
    "# Note: No \"ps\" jobs are needed for this strategy.\n",
    "# distribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Using ParameterServerStrategy (for asynchronous multi-worker training)\n",
    "# You would run this script on all workers and parameter servers.\n",
    "\n",
    "# distribution = tf.distribute.experimental.ParameterServerStrategy()\n",
    "\n",
    "# The rest of the code (defining, compiling, and fitting the model)\n",
    "# would be the same as in the MirroredStrategy example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black Box Hyperparameter Tuning on AI Platform\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "GCP AI Platform offers a powerful **Bayesian optimization** service (Google Vizier) for hyperparameter tuning. \n",
    "\n",
    "You provide:\n",
    "1.  A YAML configuration file (`tuning.yaml`) specifying the hyperparameter search space (e.g., `n_layers` from 10 to 100), the metric to optimize (e.g., `accuracy`), and the number of trials.\n",
    "2.  Your training code, which must accept the hyperparameters as command-line arguments.\n",
    "3.  Your training code must use a `TensorBoard` callback to log the metric you want to optimize.\n",
    "\n",
    "AI Platform will then run your training job multiple times (trials), and it will use the results from previous trials to intelligently choose the hyperparameter values for the next trial, quickly homing in on the optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 'tuning.yaml' file (this is not Python code)\n",
    "\"\"\"\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    maxTrials: 10\n",
    "    maxParallelTrials: 2\n",
    "    params:\n",
    "      - parameterName: n_layers\n",
    "        type: INTEGER\n",
    "        minValue: 10\n",
    "        maxValue: 100\n",
    "        scaleType: UNIT_LINEAR_SCALE\n",
    "      - parameterName: momentum\n",
    "        type: DOUBLE\n",
    "        minValue: 0.1\n",
    "        maxValue: 1.0\n",
    "        scaleType: UNIT_LOG_SCALE\n",
    "\"\"\"\n",
    "\n",
    "# Example gcloud command to launch the tuning job (run in terminal)\n",
    "# !gcloud ai-platform jobs submit training my_tuning_job \\\n",
    "#   --config tuning.yaml \\\n",
    "#   --package-path /my_project/src/trainer \\\n",
    "#   --module-name trainer.task \\\n",
    "#   [...other gcloud args...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "See Appendix A in the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
