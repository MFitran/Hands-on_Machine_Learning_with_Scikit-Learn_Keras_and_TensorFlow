{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19: Training and Deploying TensorFlow Models at Scale\n",
    "\n",
    "**Based on \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\" by Aurélien Géron**\n",
    "\n",
    "This notebook reproduces the code from Chapter 19 and provides theoretical explanations for each concept, as required by the individual task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter covers the crucial final steps in a machine learning project: deploying a trained model into a production environment and scaling up training for large, complex models.\n",
    "\n",
    "1.  **Serving a TensorFlow Model:** We learn how to export a model to TensorFlow's **SavedModel** format, which is a universal, language-neutral format that includes the computation graph and weights. We then deploy this model using **TensorFlow Serving (TF Serving)**, a high-performance, production-ready serving system. We learn how to:\n",
    "    * Install TF Serving using Docker.\n",
    "    * Serve a model and query it using both the **REST API** (simple, JSON-based) and the **gRPC API** (high-performance, binary-based).\n",
    "    * Deploy new model versions seamlessly for graceful transitions.\n",
    "\n",
    "2.  **Deploying on the Cloud (GCP AI Platform):** We explore how to use a managed cloud service like **Google Cloud AI Platform** to serve our models. This handles all the infrastructure, scaling, and version management for us. We learn how to set up a project, create a model, and deploy a model version from a SavedModel stored in Google Cloud Storage (GCS).\n",
    "\n",
    "3.  **Deploying to Other Platforms:** We briefly cover other deployment targets:\n",
    "    * **TensorFlow Lite (TFLite):** For deploying models on **mobile and embedded devices**. This involves converting the model to a lightweight `.tflite` (FlatBuffer) format and using optimizations like **quantization** to reduce model size and latency.\n",
    "    * **TensorFlow.js:** For running models directly in a **web browser**, enabling client-side ML, user privacy, and low latency.\n",
    "\n",
    "4.  **Using GPUs:** We cover how to accelerate training using GPUs. This includes managing GPU memory (e.g., enabling memory growth to avoid grabbing all RAM at once) and how to explicitly place operations on a CPU or GPU using `tf.device()`.\n",
    "\n",
    "5.  **Distributed Training at Scale:** For very large models or datasets, we explore how to train a model across multiple devices and servers using TensorFlow's **Distribution Strategies API**.\n",
    "    * **Data Parallelism:** The main strategy, where the model is replicated on each device, and each replica processes a different batch of data.\n",
    "    * **`MirroredStrategy`:** For synchronous training on all GPUs on a single machine.\n",
    "    * **`MultiWorkerMirroredStrategy`:** For synchronous training across multiple machines (a TF Cluster).\n",
    "    * **`ParameterServerStrategy`:** An asynchronous strategy where parameters are stored on dedicated parameter servers.\n",
    "\n",
    "6.  **Hyperparameter Tuning on GCP:** Finally, we learn how to use **GCP AI Platform's hyperparameter tuning service** (based on Google Vizier) to run a black-box Bayesian optimization search to find the best hyperparameters for our model automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We'll also train a basic MNIST model to use for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Common imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Prepare MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
    "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# A simple MNIST model to be saved and served\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model (for demonstration, we'll just train for 1 epoch)\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Set up paths\n",
    "model_name = \"my_mnist_model\"\n",
    "model_version = \"0001\"\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "\n",
    "# Get a few test instances\n",
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Serving a TensorFlow Model\n",
    "\n",
    "> **Theoretical Deep-Dive: Why Use a Model Server?**\n",
    ">\n",
    "> While you *can* just call `model.predict()` in any Python app, a dedicated model server like **TF Serving** is much better for production. \n",
    ">\n",
    "> 1.  **Decoupling:** Your main application (e.g., a web backend) is decoupled from the ML model. You can update the model without redeploying your entire application.\n",
    "> 2.  **Performance:** TF Serving is written in C++ and highly optimized for performance. It can handle many requests per second (QPS) and leverage hardware like GPUs.\n",
    "> 3.  **Scalability:** You can easily scale the number of TF Serving instances up or down to meet demand, independently of your main application.\n",
    "> 4.  **Versioning:** TF Serving can manage multiple versions of a model. You can deploy a new version, test it (a \"canary\" release), and roll back to a previous version if it doesn't perform well.\n",
    "> 5.  **Batching:** It can automatically batch requests together to get much higher throughput on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting to SavedModel Format\n",
    "\n",
    "To deploy a model, we first need to export it to TensorFlow's **SavedModel** format. This is a language-neutral, portable format that contains the full computation graph and all the learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to SavedModel format\n",
    "tf.saved_model.save(model, model_path)\n",
    "\n",
    "# You can also use the model's save() method.\n",
    "# If the path does not end in .h5, it saves to SavedModel format by default.\n",
    "# model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a directory `my_mnist_model/0001/` containing:\n",
    "* `saved_model.pb`: The computation graph (as a protocol buffer).\n",
    "* `variables/`: A subdirectory with all the learned weights (variables).\n",
    "* `assets/`: A subdirectory for any extra files (like vocabularies, not used here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the SavedModel with `saved_model_cli`\n",
    "\n",
    "We can use the `saved_model_cli` command-line tool (which comes with TensorFlow) to inspect our model's **signatures**. A signature defines the inputs and outputs of a function in the model. Keras models default to a `serving_default` signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir {model_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing TensorFlow Serving\n",
    "\n",
    "The easiest way to install TF Serving is with Docker. These commands are for your terminal (not this notebook).\n",
    "\n",
    "```bash\n",
    "# Download the official TF Serving image\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "# Start the TF Serving container\n",
    "docker run -it --rm -p 8500:8500 -p 8501:8501 \\\n",
    "   -v \"$(pwd)/my_mnist_model:/models/my_mnist_model\" \\\n",
    "   -e MODEL_NAME=my_mnist_model \\\n",
    "   tensorflow/serving\n",
    "```\n",
    "\n",
    "**Explanation of the command:**\n",
    "* `-it --rm`: Makes the container interactive and cleans it up after it stops.\n",
    "* `-p 8500:8500`: Forwards the host's port 8500 to the container's port 8500 (for gRPC).\n",
    "* `-p 8501:8501`: Forwards the host's port 8501 to the container's port 8501 (for the REST API).\n",
    "* `-v \"...\"`: Mounts our local model directory (`my_mnist_model`) into the container's `/models/` directory.\n",
    "* `-e MODEL_NAME=...`: Tells TF Serving which model to serve from the `/models/` directory.\n",
    "* `tensorflow/serving`: The name of the Docker image to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying TF Serving via the REST API\n",
    "\n",
    "The REST API is simple and uses JSON. It's great for general-purpose use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q requests\n",
    "import requests\n",
    "\n",
    "# Create the JSON payload\n",
    "input_data_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist(),\n",
    "})\n",
    "\n",
    "# Define the server URL\n",
    "SERVER_URL = 'http://localhost:8501/v1/models/my_mnist_model:predict'\n",
    "\n",
    "try:\n",
    "    response = requests.post(SERVER_URL, data=input_data_json)\n",
    "    response.raise_for_status() # Raise an exception in case of error\n",
    "    response = response.json()\n",
    "    \n",
    "    y_proba_rest = np.array(response[\"predictions\"])\n",
    "    print(y_proba_rest.round(2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Could not connect to TF Serving. Is the Docker container running?\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying TF Serving via the gRPC API\n",
    "\n",
    "The gRPC API is much more efficient. It uses Protocol Buffers (protobufs) to send binary, low-latency requests. This is preferred for high-performance, internal services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q grpcio tensorflow-serving-api\n",
    "import grpc\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "try:\n",
    "    channel = grpc.insecure_channel('localhost:8500')\n",
    "    predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "    # Create the gRPC request\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = \"serving_default\"\n",
    "    \n",
    "    # Convert the NumPy array to a TensorProto\n",
    "    input_name = model.input_names[0]\n",
    "    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n",
    "\n",
    "    # Send the request\n",
    "    response_grpc = predict_service.Predict(request, timeout=10.0)\n",
    "    \n",
    "    # Convert the response (a TensorProto) back to a NumPy array\n",
    "    output_name = model.output_names[0]\n",
    "    outputs_proto = response_grpc.outputs[output_name]\n",
    "    y_proba_grpc = tf.make_ndarray(outputs_proto)\n",
    "    \n",
    "    print(y_proba_grpc.round(2))\n",
    "    \n",
TUGAS (ENRICHMENT FOR MACHINE LEARNING AND DEEP LEARNING CLASSES) - INDIVIDUAL TASK
"except Exception as e:\n",
    "    print(\"Could not connect to TF Serving. Is the Docker container running?\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploying a Model on Google Cloud AI Platform\n",
    "\n",
    "Instead of managing our own TF Serving containers, we can use a managed service like Google Cloud AI Platform. It handles setup, versioning, and scaling for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Theoretical Deep-Dive: Cloud AI Platform Setup**\n",
    ">\n",
    "> 1.  **Create a GCP Project:** All resources (models, storage) live inside a project.\n",
    "> 2.  **Enable Billing:** You must have an active billing account.\n",
    "> 3.  **Enable APIs:** You must enable the \"AI Platform Training & Prediction API\" and \"Google Cloud Storage API\".\n",
    "> 4.  **Create a GCS Bucket:** Google Cloud Storage (GCS) is where you will store your SavedModel files. You need to create a unique \"bucket\" name.\n",
    "> 5.  **Upload Model:** You upload your `my_mnist_model/0001` directory to the GCS bucket.\n",
    "> 6.  **Create AI Platform Model:** In the AI Platform console, you create a \"model\" resource (e.g., `my_mnist_model`).\n",
    "> 7.  **Create Model Version:** Inside that model, you create a \"version\" (e.g., `v0001`) and point it to the GCS path of your SavedModel (e.g., `gs://your-bucket/my_mnist_model/0001`).\n",
    "> 8.  **Create Service Account:** For security, you create a **service account** (an account for an application, not a person) and give it the \"AI Platform Developer\" role. You download its private key (a JSON file) to authenticate your client code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the GCP AI Platform Prediction Service\n",
    "\n",
    "Once the model is deployed on GCP, we query it using Google's client libraries. This is similar to the REST API, but it handles authentication for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q google-api-python-client\n",
    "import googleapiclient.discovery\n",
    "\n",
    "# --- CONFIGURATION --- \n",
    "# 1. Set this environment variable to point to your downloaded service account JSON key\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"my_service_account_key.json\"\n",
    "\n",
    "# 2. Replace with your actual project and model IDs\n",
    "project_id = \"your-gcp-project-id\"\n",
    "model_id = \"my_mnist_model\"\n",
    "# ---------------------\n",
    "\n",
    "model_path = f\"projects/{project_id}/models/{model_id}\"\n",
    "# You can also query a specific version:\n",
    "# model_path = f\"projects/{project_id}/models/{model_id}/versions/{model_version}\"\n",
    "\n",
    "# Create a resource object to interact with the service\n",
    "try:\n",
    "    ml_resource = googleapiclient.discovery.build(\"ml\", \"v1\").projects()\n",
    "except Exception as e:\n",
    "    print(\"Error building Google API client. Is the library installed and GOOGLE_APPLICATION_CREDENTIALS set?\")\n",
    "    print(e)\n",
    "\n",
    "def predict_gcp(X):\n",
    "    # Format the request body\n",
    "    input_data_json = {\"signature_name\": \"serving_default\",\n",
    "                       \"instances\": X.tolist()}\n",
    "    \n",
    "    # Prepare and send the request\n",
    "    request = ml_resource.predict(name=model_path, body=input_data_json)\n",
    "    response = request.execute()\n",
    "    \n",
    "    if \"error\" in response:\n",
    "        raise RuntimeError(response[\"error\"])\n",
    "        \n",
    "    # Parse the response\n",
    "    output_name = model.output_names[0]\n",
    "    return np.array([pred[output_name] for pred in response[\"predictions\"]])\n",
    "\n",
    "# try:\n",
    "#     Y_probas_gcp = predict_gcp(X_new)\n",
    "#     print(Y_probas_gcp.round(2))\n",
    "# except Exception as e:\n",
    "#     print(\"Error querying GCP. Please check your project_id, model_id, and authentication.\")\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Note: The cell above is commented out to prevent errors, as it requires a live GCP project and a valid service account key.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying to Mobile, Embedded, and Web\n",
    "\n",
    "### TensorFlow Lite (TFLite)\n",
    "\n",
    "> **Theoretical Deep-Dive: TFLite and Quantization**\n",
    ">\n",
    "> **TFLite** is a framework for running TF models on devices with low compute power and memory, like mobile phones and microcontrollers.\n",
    ">\n",
    "> It uses a **TFLite Converter** to convert a SavedModel into a highly optimized `.tflite` file (a FlatBuffer). This conversion reduces the model size and latency.\n",
    ">\n",
    "> A key optimization technique is **quantization**. This converts the model's 32-bit floating-point weights (and optionally, activations) into 8-bit integers (or 16-bit floats). \n",
    "> -   **Pros:** 4x reduction in model size, 2-4x speedup, and less power consumption. \n",
    "> -   **Cons:** A small (usually acceptable) drop in accuracy.\n",
    "> \n",
    "> **Post-training quantization** is the simplest method: you quantize the model after training. For full-integer quantization (the fastest), you need to provide a small sample of data for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the SavedModel to a TFLite FlatBuffer\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"converted_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# We can also convert a Keras model directly\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# ...\n",
    "\n",
    "# To use post-training quantization:\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # or OPTIMIZE_FOR_SIZE\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "print(f\"Original model size: {os.path.getsize(model_path + '/saved_model.pb')/1024:.2f} KB (graph only)\")\n",
    "print(f\"TFLite model size: {len(tflite_model)/1024:.2f} KB\")\n",
    "print(f\"Quantized TFLite model size: {len(tflite_quantized_model)/1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow.js (TF.js)\n",
    "\n",
    "> **Theoretical Deep-Dive:**\n",
    ">\n",
    "> **TensorFlow.js** is a JavaScript library for training and deploying models in the browser and on Node.js.\n",
    ">\n",
    "> **Why use it?**\n",
    "> -   **Client-Side Inference:** The model runs on the user's machine, so no server is needed. This scales infinitely and works offline.\n",
    "> -   **Privacy:** Sensitive data (e.g., from a webcam) never leaves the user's browser.\n",
    "> -   **Interactivity:** You can build highly interactive web applications (e.g., in-browser pose estimation).\n",
    ">\n",
    "> You use the `tensorflowjs_converter` tool to convert a SavedModel or Keras model into a `model.json` file (for the architecture) and a set of binary `groupX-shardXofX.bin` files (for the weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.js Converter (Terminal Command)\n",
    "\n",
    "```bash\n",
    "# First, install the converter\n",
    "pip install tensorflowjs\n",
    "\n",
    "# Run the converter\n",
    "tensorflowjs_converter --input_format=tf_saved_model \\\n",
    "                       my_mnist_model/0001 \\\n",
    "                       my_tfjs_model\n",
    "```\n",
    "This creates a `my_tfjs_model` directory with `model.json` and the binary weight files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.js Example Usage (JavaScript)\n",
    "\n",
    "```html\n",
    "\n",
    "<script src=\"[https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest](https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest)\"></script>\n",
    "\n",
    "<script>\n",
    "    async function runModel() {\n",
    "        // Load the model\n",
    "        const model = await tf.loadLayersModel('my_tfjs_model/model.json');\n",
    "        \n",
    "        // Create a dummy input tensor (e.g., a 1x28x28x1 image)\n",
    "        const image = tf.zeros([1, 28, 28, 1]);\n",
    "        \n",
    "        // Make a prediction\n",
    "        const prediction = model.predict(image);\n",
    "        prediction.print();\n",
    "    }\n",
    "    runModel();\n",
    "</script>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using GPUs to Speed Up Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for GPU Access\n",
    "\n",
    "If you have a compatible NVIDIA GPU and the correct CUDA and cuDNN libraries installed, TensorFlow will automatically use the GPU for most operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is GPU available:\", tf.test.is_gpu_available())\n",
    "print(\"GPU device name:\", tf.test.gpu_device_name())\n",
    "print(\"List of physical GPUs:\", tf.config.experimental.list_physical_devices(device_type='GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing GPU RAM\n",
    "\n",
    "> **Theoretical Deep-Dive: GPU Memory Management**\n",
    ">\n",
    "> By default, TensorFlow tries to grab *all* available GPU memory when it starts. This is efficient, but it prevents you from running a second TF process on the same GPU.\n",
    ">\n",
    "> A better approach is to enable **memory growth**. This tells TensorFlow to only grab the memory it needs, *as* it needs it. This must be done right at the start of your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        # Set memory growth to True for all GPUs\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"{len(physical_gpus)} Physical GPUs found, Memory Growth set to True.\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing Operations on Devices\n",
    "\n",
    "TensorFlow automatically places operations on the GPU if one is available and a GPU-kernel (implementation) exists for that op. You can manually override this using a `tf.device()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Default placement:\")\n",
    "a = tf.Variable(42.0)\n",
    "print(a.device)\n",
    "\n",
    "print(\"Forced CPU placement:\")\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    b = tf.Variable(42.0)\n",
    "\n",
    "print(b.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Models Across Multiple Devices\n",
    "\n",
    "For very large models, we need to distribute training across multiple GPUs, and even multiple machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Theoretical Deep-Dive: Model vs. Data Parallelism**\n",
    ">\n",
    "> -   **Model Parallelism:** You split *one* model across multiple devices. For example, layer 1 goes on GPU 0, layer 2 goes on GPU 1. This is complex and often inefficient due to communication bottlenecks (layer 2 must wait for layer 1).\n",
    "> -   **Data Parallelism (Recommended):** You replicate the *entire* model on each device. Each replica processes a different mini-batch of data. The gradients from all replicas are then aggregated (e.g., averaged), and the model parameters on all replicas are updated. This is much easier to implement and scales well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Distribution Strategies API\n",
    "\n",
    "TensorFlow's `tf.distribute` API makes data parallelism easy. You just create a **strategy** object and build/compile your model inside its `scope()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `MirroredStrategy`: Training on one machine with multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a strategy to use all available GPUs\n",
    "distribution = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# You can also specify which GPUs to use:\n",
    "# distribution = tf.distribute.MirroredStrategy([\"/gpu:0\", \"/gpu:1\"])\n",
    "\n",
    "with distribution.scope():\n",
    "    mirrored_model = keras.models.Sequential([\n",
    "        keras.layers.Dense(100, activation=\"relu\", input_shape=[28*28]),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    mirrored_model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                             optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "                             metrics=[\"accuracy\"])\n",
    "\n",
    "# Flatten the data for this simple model\n",
    "X_train_flat = X_train.reshape(-1, 28*28)\n",
    "\n",
    "# The batch size should be divisible by the number of replicas (GPUs)\n",
    "# e.g., if you have 2 GPUs, use a batch_size of 32, 64, etc.\n",
    "batch_size = 64 \n",
    "history = mirrored_model.fit(X_train_flat, y_train, epochs=5, batch_size=batch_size)\n",
    "\n",
    "# Saving the model saves a single, non-distributed version\n",
    "# mirrored_model.save(\"my_mirrored_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `MultiWorkerMirroredStrategy`: Training on a Multi-Machine Cluster\n",
    "\n",
    "> **Theoretical Deep-Dive: TF Cluster**\n",
    ">\n",
    "> A TensorFlow Cluster is a group of TF processes (tasks) running on different machines.\n",
    "> -   `worker`: A task that performs computations (training).\n",
    "> -   `chief`: A special worker (usually `worker:0`) that also handles tasks like saving checkpoints and writing TensorBoard logs.\n",
    "> -   `ps`: A **Parameter Server** task that only stores and updates variables (used by the `ParameterServerStrategy`).\n",
    ">\n",
    "> To configure a cluster, you must set the `TF_CONFIG` environment variable on *each machine* before it starts. This JSON variable tells the task what the cluster looks like (all task addresses) and what its own role is (e.g., `{\"type\": \"worker\", \"index\": 1}`).\n",
    ">\n",
    "> `MultiWorkerMirroredStrategy` implements synchronous data parallelism across all workers. It's the multi-machine equivalent of `MirroredStrategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code would be run on every machine in the cluster.\n",
    "# TF_CONFIG would be set as an environment variable before running the script.\n",
    "\n",
    "# Example TF_CONFIG for Worker 0:\n",
    "# os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "#     \"cluster\": {\n",
    "#         \"worker\": [\"machine-a.example.com:2222\", \"machine-b.example.com:2222\"]\n",
    "#     },\n",
    "#     \"task\": {\"type\": \"worker\", \"index\": 0}\n",
    "# })\n",
    "\n",
    "# Example TF_CONFIG for Worker 1:\n",
    "# os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "#     \"cluster\": {\n",
    "#         \"worker\": [\"machine-a.example.com:2222\", \"machine-b.example.com:2222\"]\n",
    "#     },\n",
    "#     \"task\": {\"type\": \"worker\", \"index\": 1}\n",
    "# })\n",
    "\n",
    "# --- The Python script (run on all workers) --- \n",
    "\n",
    "# distribution_multi = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# with distribution_multi.scope():\n",
    "#     multi_worker_model = keras.models.Sequential([...]) # same model as before\n",
    "#     multi_worker_model.compile([...])\n",
    "\n",
    "# history = multi_worker_model.fit(X_train_flat, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "print(\"MultiWorkerMirroredStrategy setup (conceptual)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running Training Jobs on GCP AI Platform\n",
    "\n",
    "AI Platform can manage the entire cluster for you. You package your code and use the `gcloud` command-line tool to submit a training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting a Training Job (Terminal Command)\n",
    "\n",
    "```bash\n",
    "gcloud ai-platform jobs submit training my_job_20251116_150000 \\\n",
    "    --region us-central1 \\\n",
    "    --scale-tier PREMIUM_1 \\\n",
    "    --runtime-version 2.0 \\\n",
    "    --python-version 3.7 \\\n",
    "    --package-path ./my_project/src/trainer \\\n",
    "    --module-name trainer.task \\\n",
    "    --staging-bucket gs://my-staging-bucket \\\n",
    "    --job-dir gs://my-model-bucket/trained_model \\\n",
    "    -- \\\n",
    "    --my-extra-argument1 foo\n",
    "```\n",
    "\n",
    "This command packages the code from `my_project/src/trainer`, uploads it to a `staging-bucket`, and runs the `trainer.task` module on a pre-configured cluster (`PREMIUM_1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black Box Hyperparameter Tuning on AI Platform\n",
    "\n",
    "> **Theoretical Deep-Dive: Bayesian Hyperparameter Optimization**\n",
    ">\n",
    "> Instead of grid search or random search, GCP's **Hyperparameter Tuning Service (Google Vizier)** uses a more intelligent, black-box Bayesian optimization approach.\n",
    ">\n",
    "> It works like this:\n",
    "> 1.  It tries a few random combinations of hyperparameters.\n",
    "> 2.  It uses the results to build a probabilistic model of how the hyperparameters relate to the model's performance (the `hyperparameterMetricTag`).\n",
    "> 3.  It uses this model to intelligently choose the *next* set of hyperparameters that are most likely to yield an improvement.\n",
    "> 4.  It repeats this process, getting smarter with each trial.\n",
    ">\n",
    "> You define the search in a YAML config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example `tuning.yaml` file\n",
    "\n",
    "```yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    hyperparameterMetricTag: accuracy # Must match the name in your TensorBoard logs\n",
    "    maxTrials: 10\n",
    "    maxParallelTrials: 2\n",
    "    params:\n",
    "      - parameterName: n_layers\n",
    "        type: INTEGER\n",
    "        minValue: 1\n",
    "        maxValue: 5\n",
    "      - parameterName: learning_rate\n",
    "        type: DOUBLE\n",
    "        minValue: 1e-4\n",
    "        maxValue: 1e-2\n",
    "        scaleType: UNIT_LOG_SCALE\n",
    "```\n",
    "\n",
    "Your training script just needs to:\n",
    "1.  Accept arguments like `--n_layers` and `--learning_rate`.\n",
    "2.  Log the target metric (`accuracy`) using a `TensorBoard` callback. The service will automatically read these logs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
