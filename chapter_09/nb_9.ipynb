{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 – Unsupervised Learning Techniques\n",
    "\n",
    "This notebook contains all the code samples and solutions to the exercises in chapter 9 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition* (O'Reilly). *Note: all code examples are based on the author's original GitHub repository.* \n",
    "\n",
    "**Assignment Instructions:**\n",
    "Per the assignment guidelines, this notebook reproduces the code from Chapter 9. It also includes theoretical explanations and summaries for each concept, as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter explores several key **Unsupervised Learning** techniques, where the model learns from data that has no labels. The vast majority of available data is unlabeled, making these techniques incredibly important.\n",
    "\n",
    "Key techniques covered include:\n",
    "\n",
    "1.  **Clustering:** The task of grouping similar instances together into clusters. We explore this for data analysis, customer segmentation, image segmentation, preprocessing, semi-supervised learning, and anomaly detection.\n",
    "    * **K-Means:** A simple and efficient algorithm that finds a predefined number of clusters ($k$) by identifying cluster centers (centroids) and assigning instances to the nearest one. We learn how to find the optimal $k$ using the elbow method and silhouette scores.\n",
    "    * **DBSCAN:** A density-based algorithm that defines clusters as continuous regions of high density. It's powerful because it can find clusters of any shape and is robust to outliers.\n",
    "\n",
    "2.  **Gaussian Mixture Models (GMMs):** A probabilistic model that assumes the data is generated from a mixture of several Gaussian distributions with unknown parameters. GMMs are more flexible than K-Means, as they can capture clusters of various ellipsoidal shapes, sizes, and densities.\n",
    "    * **Density Estimation & Anomaly Detection:** GMMs are great for density estimation. This, in turn, allows us to perform anomaly detection by identifying instances in low-density regions.\n",
    "    * **Cluster Selection:** We use information criteria like the **Bayesian Information Criterion (BIC)** or **Akaike Information Criterion (AIC)** to find the optimal number of clusters. We also look at Bayesian GMMs, which can automatically find the right number of clusters.\n",
    "\n",
    "3.  **Other Anomaly/Novelty Detection Algorithms:** A brief overview of other methods like Isolation Forest and One-Class SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 3.7 or later is required for the latest versions of Scikit-Learn), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"unsupervised_learning\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### Theoretical Explanation: What is Clustering?\n",
    "\n",
    "**Clustering** is the unsupervised task of identifying similar instances and assigning them to groups, or *clusters*. The goal is to find a structure in the data where instances *within* a cluster are similar to each other, and *different* from instances in other clusters.\n",
    "\n",
    "It has many applications:\n",
    "* **Customer Segmentation:** Grouping customers to adapt products and marketing.\n",
    "* **Data Analysis:** Analyze each cluster separately to find patterns.\n",
    "* **Dimensionality Reduction:** Replace an instance's features with a vector of its cluster affinities.\n",
    "* **Anomaly Detection:** Instances with low affinity to all clusters are likely anomalies.\n",
    "* **Semi-Supervised Learning:** Propagate labels from a few labeled instances to all other instances in their cluster.\n",
    "* **Image Segmentation:** Grouping pixels by color to create an image with fewer colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "#### Theoretical Explanation: The K-Means Algorithm\n",
    "\n",
    "The K-Means algorithm is a simple and efficient clustering algorithm. You must specify the number of clusters, $k$, that the algorithm must find.\n",
    "\n",
    "The algorithm works as follows:\n",
    "1.  **Initialize:** Place $k$ centroids randomly (e.g., by picking $k$ instances at random).\n",
    "2.  **Assign:** Label all instances by assigning each of them to the cluster with the closest centroid.\n",
    "3.  **Update:** Update the centroids by computing the mean (average) of all instances assigned to each cluster.\n",
    "4.  **Iterate:** Repeat steps 2 and 3 until the centroids stop moving (i.e., the algorithm converges).\n",
    "\n",
    "This algorithm is guaranteed to converge, but it may converge to a *local optimum*, not the global one. This depends heavily on the centroid initialization. To mitigate this, the standard solution is to run the algorithm multiple times (controlled by `n_init`) with different random initializations and keep the best solution. The best solution is the one with the lowest **inertia** (the mean squared distance between each instance and its closest centroid).\n",
    "\n",
    "Scikit-Learn uses a smarter initialization method called **K-Means++** by default. This method tends to select centroids that are distant from one another, which makes the algorithm much less likely to converge to a suboptimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: K-Means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# First, let's create a simple dataset with 5 blobs\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8], [-2.8,  2.8], [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "\n",
    "# Helper function to plot the blobs\n",
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "plt.title(\"A dataset with 5 blobs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a K-Means clusterer\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# The labels_ attribute contains the cluster index for each instance\n",
    "print(y_pred)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "# The cluster_centers_ attribute holds the 5 centroids\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can predict the cluster for new instances\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plotting the decision boundaries (a Voronoi tessellation)\n",
    "\n",
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() * 0.1]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel2)\n",
    "    plt.contour(xx, yy, Z, linewidths=1, colors='k')\n",
    "    \n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.title(\"K-Means Decision Boundaries (Voronoi Tessellation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Explanation: Finding the Optimal Number of Clusters\n",
    "\n",
    "How can we know the right number of clusters $k$? If we set $k$ too low, separate clusters will be merged. If we set $k$ too high, some clusters will be split into multiple pieces.\n",
    "\n",
    "1.  **Elbow Method (Inertia):** We can't just pick the $k$ with the lowest inertia, because inertia always gets lower as $k$ increases (if $k$ equals the number of instances, inertia is 0).\n",
    "    Instead, we plot inertia as a function of $k$. The curve will often have an \"elbow\" shape. The inertia drops very quickly as we increase $k$ up to a point, and then it decreases much more slowly. This elbow (e.g., at $k=4$ in the book's example) is a good choice for $k$. It's a bit coarse, but a good first guess.\n",
    "\n",
    "2.  **Silhouette Score:** A more precise approach is to use the **silhouette score**. This score is the mean **silhouette coefficient** over all instances.\n",
    "    * An instance's silhouette coefficient is $ (b - a) / \\max(a, b) $.\n",
    "    * $a$ is the mean distance to other instances *in the same cluster* (mean intra-cluster distance).\n",
    "    * $b$ is the mean distance to the instances of the *next closest cluster* (mean nearest-cluster distance).\n",
    "    * The coefficient ranges from -1 to +1.\n",
    "        * **+1:** The instance is well inside its own cluster and far from others.\n",
    "        * **0:** The instance is close to a cluster boundary.\n",
    "        * **-1:** The instance may have been assigned to the wrong cluster.\n",
    "\n",
    "We can plot the silhouette score as a function of $k$. A high score (a peak in the graph) suggests a good value for $k$. We can also plot **silhouette diagrams** to visualize the coefficient for every instance, which is even more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Finding the optimal k (Inertia)\n",
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow', \n",
    "             xy=(4, inertias[3]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.title(\"Inertia vs. k (Elbow Method)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Finding the optimal k (Silhouette Score)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]] # Start from k=2\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.title(\"Silhouette Score vs. k\")\n",
    "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette score suggests $k=4$ is a good choice, and $k=5$ is also quite good (and better than 6 or 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Explanation: Limits of K-Means\n",
    "\n",
    "K-Means is fast and scalable, but it's not perfect:\n",
    "1.  You must run it several times to avoid a suboptimal solution.\n",
    "2.  You must specify the number of clusters $k$.\n",
    "3.  It does not behave well when the clusters have **varying sizes**, **different densities**, or **nonspherical shapes**. It struggles with ellipsoidal or elongated clusters because it only cares about the distance to the centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Explanation: Using Clustering for Image Segmentation\n",
    "\n",
    "**Image segmentation** is the task of partitioning an image into multiple segments. In **color segmentation**, we simply assign pixels to the same segment if they have a similar color.\n",
    "\n",
    "We can use K-Means to do this:\n",
    "1.  Treat each pixel's RGB color as a 3D vector.\n",
    "2.  Reshape the image into a long list of these color vectors.\n",
    "3.  Use K-Means to cluster these colors into $k$ groups.\n",
    "4.  Replace each pixel's color with the mean color (the centroid) of its cluster.\n",
    "\n",
    "This reduces the total number of colors in the image to $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Image Segmentation\n",
    "\n",
    "# Download the ladybug image\n",
    "import urllib.request\n",
    "images_path = os.path.join(PROJECT_ROOT_DIR, \"images\", \"unsupervised_learning\")\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "filename = \"ladybug.png\"\n",
    "url = DOWNLOAD_ROOT + \"images/unsupervised_learning/\" + filename\n",
    "urllib.request.urlretrieve(url, os.path.join(images_path, filename))\n",
    "\n",
    "from matplotlib.image import imread\n",
    "image = imread(os.path.join(images_path, filename))\n",
    "print(\"Original image shape:\", image.shape)\n",
    "\n",
    "# Reshape the image to a 2D array (list of RGB pixels)\n",
    "X = image.reshape(-1, 3)\n",
    "\n",
    "# Cluster the colors\n",
    "kmeans = KMeans(n_clusters=8, random_state=42).fit(X)\n",
    "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "segmented_img = segmented_img.reshape(image.shape)\n",
    "\n",
    "# Plot the result\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original image\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(segmented_img)\n",
    "plt.title(f\"Segmented image ({k} colors)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Explanation: Using Clustering for Preprocessing\n",
    "\n",
    "Clustering can be a powerful preprocessing step before a supervised learning algorithm (like a classifier). It acts as an efficient **dimensionality reduction** technique.\n",
    "\n",
    "Instead of using the original features, we can replace them with a vector of each instance's **distances to the $k$ cluster centroids**. This new $k$-dimensional feature vector can be much lower-dimensional than the original, yet preserve enough information for the classifier to work well.\n",
    "\n",
    "The code below demonstrates this on the digits dataset. We will train a Logistic Regression classifier on the raw data, and then another one on the K-Means preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=42)\n",
    "\n",
    "# 1. Train on raw data\n",
    "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "print(\"Accuracy on raw data:\", log_reg.score(X_test, y_test))\n",
    "\n",
    "# 2. Train on K-Means preprocessed data\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# We set k=50. The best k is simply the one that results in the best classification performance.\n",
    "pipeline = Pipeline([\n",
    "    (\"kmeans\", KMeans(n_clusters=50, random_state=42)),\n",
    "    (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Accuracy with K-Means preprocessing:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a significant accuracy boost! The error rate dropped by over 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Explanation: Using Clustering for Semi-Supervised Learning\n",
    "\n",
    "Clustering can also be used for **semi-supervised learning**, which is when we have plenty of unlabeled instances and very few labeled ones.\n",
    "\n",
    "1.  Cluster all instances (labeled and unlabeled) into $k$ clusters.\n",
    "2.  For each cluster, find the most representative instance (the one closest to the centroid).\n",
    "3.  Manually label only these $k$ representative instances. This is much less work than labeling the whole dataset.\n",
    "4.  **Propagate the labels:** Assign the label of each representative instance to all other instances in the same cluster. This is called **label propagation**.\n",
    "\n",
    "This gives us a fully \"pseudo-labeled\" dataset that we can use to train a supervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Semi-Supervised Learning\n",
    "\n",
    "# 1. Let's see the accuracy with only 50 labeled instances\n",
    "n_labeled = 50\n",
    "log_reg_semi = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg_semi.fit(X_train[:n_labeled], y_train[:n_labeled])\n",
    "print(\"Accuracy with 50 labeled instances:\", log_reg_semi.score(X_test, y_test))\n",
    "\n",
    "# 2. Let's use clustering to find 50 representative instances\n",
    "k = 50\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "X_digits_dist = kmeans.fit_transform(X_train) # Get distances to all centroids\n",
    "representative_digit_idx = np.argmin(X_digits_dist, axis=0) # Get index of instance closest to each centroid\n",
    "X_representative_digits = X_train[representative_digit_idx]\n",
    "\n",
    "# 3. Manually label these 50 instances\n",
    "# (Here we just grab the real labels for the demo)\n",
    "y_representative_digits = y_train[representative_digit_idx]\n",
    "\n",
    "# 4. Train a model on these 50 representative instances\n",
    "log_reg_semi_rep = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg_semi_rep.fit(X_representative_digits, y_representative_digits)\n",
    "print(\"Accuracy with 50 representative instances:\", log_reg_semi_rep.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy jumped from 72.4% to 88.9%! This shows it's much better to label representative instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Let's try label propagation\n",
    "y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]\n",
    "\n",
    "log_reg_propagated = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg_propagated.fit(X_train, y_train_propagated)\n",
    "print(\"Accuracy with propagated labels:\", log_reg_propagated.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is even better. The problem is that we propagated labels to *all* instances, including those near cluster boundaries, which are more likely to be mislabeled.\n",
    "\n",
    "A better approach is to only propagate labels to the instances closest to the centroid, and leave the others unlabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Propagate labels to the 20% of instances closest to the centroid\n",
    "\n",
    "percentile_closest = 20\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "\n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
    "\n",
    "log_reg_partial_prop = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg_partial_prop.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "print(\"Accuracy with partial propagation:\", log_reg_partial_prop.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 50 labeled instances, we got 92% accuracy, which is very close to the 94.7% we got on the fully labeled dataset. This shows the power of combining clustering with supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "#### Theoretical Explanation\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a very different algorithm that can find clusters of arbitrary shapes. It defines clusters as **continuous regions of high density**.\n",
    "\n",
    "Here's how it works:\n",
    "1.  For each instance, the algorithm counts how many instances are located within a small distance **`eps`** (epsilon) from it. This region is called the instance’s *ε-neighborhood*.\n",
    "2.  If an instance has at least `min_samples` instances in its ε-neighborhood (including itself), it is considered a **core instance**. Core instances are those in dense regions.\n",
    "3.  All instances in the neighborhood of a core instance belong to the same cluster. This can include other core instances, so a long sequence of neighboring core instances forms a single cluster.\n",
    "4.  Any instance that is not a core instance and does not have one in its neighborhood is considered an **anomaly** (labeled -1).\n",
    "\n",
    "This algorithm is powerful and robust to outliers, but it can be slow (roughly $O(m \\log m)$ or $O(m^2)$) and doesn't work well if clusters have very different densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# The labels_ attribute shows the cluster index, or -1 for anomalies\n",
    "print(\"Labels (first 10):\", dbscan.labels_[:10])\n",
    "\n",
    "# The core_sample_indices_ and components_ attributes show the core instances\n",
    "print(\"Core indices count:\", len(dbscan.core_sample_indices_))\n",
    "print(\"Core components shape:\", dbscan.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try again with a larger eps\n",
    "dbscan2 = DBSCAN(eps=0.2, min_samples=5)\n",
    "dbscan2.fit(X)\n",
    "\n",
    "# Helper function to plot DBSCAN results\n",
    "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
    "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "    core_mask[dbscan.core_sample_indices_] = True\n",
    "    anomalies_mask = dbscan.labels_ == -1\n",
    "    non_core_mask = ~(core_mask | anomalies_mask)\n",
    "\n",
    "    plt.scatter(X[core_mask, 0], X[core_mask, 1],\n",
    "                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n",
    "    plt.scatter(X[non_core_mask, 0], X[non_core_mask, 1],\n",
    "                c=dbscan.labels_[non_core_mask], marker='.', s=size, cmap=\"Paired\")\n",
    "    plt.scatter(X[anomalies_mask, 0], X[anomalies_mask, 1],\n",
    "                c=\"r\", marker=\"x\", s=size)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\", fontsize=14)\n",
    "    plt.grid()\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "plt.figure(figsize=(9, 3.2))\n",
    "plt.subplot(121)\n",
    "plot_dbscan(dbscan, X, size=100)\n",
    "plt.subplot(122)\n",
    "plot_dbscan(dbscan2, X, size=100, show_ylabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN does not have a `predict()` method, only `fit_predict()`. This means it cannot predict which cluster a new instance belongs to. A simple solution is to train a classifier on the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "# Train the KNN classifier on the core instances\n",
    "knn.fit(dbscan2.components_, dbscan2.labels_[dbscan2.core_sample_indices_])\n",
    "\n",
    "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
    "print(\"Predictions for new instances:\", knn.predict(X_new))\n",
    "print(\"Probabilities for new instances:\\n\", knn.predict_proba(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models (GMMs)\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "A **Gaussian Mixture Model (GMM)** is a probabilistic model that assumes the instances were generated from a *mixture* of several Gaussian distributions whose parameters are unknown.\n",
    "\n",
    "All instances generated from a single Gaussian distribution form a cluster that typically looks like an **ellipsoid**. Each cluster can have a different ellipsoidal shape, size, density, and orientation. K-Means is a bit like a special case of GMM where all clusters
