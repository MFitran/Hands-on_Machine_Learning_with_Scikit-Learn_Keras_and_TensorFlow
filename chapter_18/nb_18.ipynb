
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Reinforcement Learning\n",
    "\n",
    "This notebook contains the code reproductions and theoretical explanations for Chapter 18 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter provides an introduction to **Reinforcement Learning (RL)**, a field of Machine Learning where an **agent** learns to interact with an **environment** to maximize a cumulative **reward**.\n",
    "\n",
    "Key topics covered include:\n",
    "\n",
    "* **Core Concepts:** Defining the RL framework, including the agent, environment, actions, observations, rewards, and the **policy** (the agent's strategy).\n",
    "* **Policy Search:** The idea of optimizing the policy's parameters directly to maximize rewards. This leads to **Policy Gradients (PG)**, the first major algorithm we implement.\n",
    "* **OpenAI Gym:** A toolkit for developing and comparing RL agents. We use its `CartPole` environment to test our algorithms.\n",
    "* **Credit Assignment Problem:** The challenge of determining which past actions were responsible for a given reward, especially when rewards are delayed. This is solved using **discounted rewards** (returns).\n",
    "* **Markov Decision Processes (MDPs):** The formal framework for modeling RL problems, involving states, transitions, actions, and rewards. We explore the **Bellman Optimality Equation** and the **Value Iteration** algorithm.\n",
    "* **Q-Learning:** A popular model-free, off-policy algorithm that learns the optimal state-action values (Q-Values) by iteratively updating them based on observed transitions (TD Learning).\n",
    "* **Deep Q-Networks (DQN):** We move from tabular Q-Learning to **Approximate Q-Learning**, using a deep neural network (a DQN) to estimate Q-Values. This allows us to handle complex environments with large state spaces (like Atari games). We explore several key variants that stabilize training:\n",
    "    * Fixed Q-Value Targets\n",
    "    * Double DQN\n",
    "    * Prioritized Experience Replay (PER)\n",
    "    * Dueling DQN\n",
    "* **TF-Agents:** A powerful, scalable, and flexible library from Google for implementing, training, and deploying RL agents. We use it to build a complete DQN agent to solve the `Breakout` Atari game, covering all the components of a production-ready system (environments, preprocessing, agents, replay buffers, drivers, and metrics).\n",
    "* **Overview of Other Algorithms:** A brief look at modern algorithms like Actor-Critic (A2C/A3C), PPO, and SAC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Common setup for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Optimize Rewards\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "In Reinforcement Learning, an **agent** (e.g., a robot, a game-playing program) interacts with an **environment** (e.g., the real world, a game simulator). \n",
    "\n",
    "At each step, the agent:\n",
    "1.  Makes an **observation** of the environment's state.\n",
    "2.  Takes an **action** based on that observation.\n",
    "3.  Receives a **reward** (positive or negative) from the environment.\n",
    "\n",
    "The agent's goal is to learn a strategy, called a **policy**, that maximizes its total cumulative reward over time. This policy is the algorithm the agent uses to choose its actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "**Policy Search** is a method of finding a good policy by directly optimizing the policy's parameters. The \"policy space\" is the set of all possible parameter values. We can explore this space using techniques like:\n",
    "\n",
    "* **Brute Force:** Only feasible for a tiny number of parameters.\n",
    "* **Genetic Algorithms:** An evolutionary approach where \"generations\" of policies are tested, and the best ones \"reproduce\" (are copied with random variations) to create the next generation.\n",
    "* **Policy Gradients (PG):** An optimization technique that evaluates the gradients of the rewards with regard to the policy parameters. The agent then follows the gradients toward higher rewards. This is what we will focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI Gym\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "**OpenAI Gym** is a toolkit that provides a wide variety of simulated environments (like Atari games and physics-based simulations) for training and testing RL agents. This is crucial because training in the real world is often slow, expensive, and risky.\n",
    "\n",
    "The core Gym interface is simple:\n",
    "* `env = gym.make(ENV_NAME)`: Creates an environment.\n",
    "* `obs = env.reset()`: Resets the environment to its initial state and returns the first observation.\n",
    "* `obs, reward, done, info = env.step(action)`: Executes an action, steps the environment forward, and returns the new observation, the reward, a boolean `done` flag (which is `True` when the episode is over), and an `info` dictionary (for debugging).\n",
    "* `env.render()`: Displays the environment (e.g., in a popup window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gym. We also need box2d for the LunarLander exercise at the end.\n",
    "# !python3 -m pip install -U gym[box2d]\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To render the environment in a notebook, you may need a virtual display.\n",
    "# try:\n",
    "#     import pyvirtualdisplay\n",
    "#     display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
    "# except ImportError:\n",
    "#     pass\n",
    "\n",
    "# env.render()\n",
    "# img = env.render(mode=\"rgb_array\")\n",
    "# print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the action space\n",
    "env.action_space # Discrete(2) means 0 (left) or 1 (right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an action\n",
    "action = 1 # accelerate right\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a simple hardcoded policy\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "print(f\"Mean: {np.mean(totals)}, Std: {np.std(totals)}, Min: {np.min(totals)}, Max: {np.max(totals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Policies\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "A **policy** can be represented by a neural network. It takes an observation as input and outputs the action to be executed. \n",
    "\n",
    "For discrete actions, the network outputs a *probability* for each action. We then **sample** an action from this distribution (e.g., if it outputs 70% for Left, 30% for Right, we'll pick Left 70% of the time). This is a **stochastic policy**.\n",
    "\n",
    "This randomness is crucial. It allows the agent to find a balance between **exploration** (trying new actions to see if they are good) and **exploitation** (using the actions that are already known to be good)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"), # output p(left)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Actions: The Credit Assignment Problem\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "How do we train this network? We don't have \"correct\" labels for each action. The only feedback is a **reward**, which is often **sparse** (e.g., only at the end of the game) and **delayed**.\n",
    "\n",
    "This is the **credit assignment problem**: if you win a game, which of the 100 actions you took were responsible? \n",
    "\n",
    "**Solution:**\n",
    "1.  Run the agent for several episodes.\n",
    "2.  For every action taken, estimate its value by summing all the **discounted rewards** that come after it. This sum is called the **return**. \n",
    "3.  The **discount factor (γ)** (e.g., 0.95) makes future rewards count less than immediate rewards.\n",
    "    `Return = R_t + γ*R_t+1 + γ²*R_t+2 + ...`\n",
    "4.  Once we have the returns for all actions, we **normalize** them (subtract the mean, divide by std dev). This gives us the **action advantage**.\n",
    "5.  We can then assume that actions with a positive advantage were \"good\" and actions with a negative advantage were \"bad\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradients\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The **Policy Gradient (PG)** algorithm (e.g., REINFORCE) uses this \"good\" vs. \"bad\" heuristic to train the policy network.\n",
    "\n",
    "1.  **Play several episodes:** Let the current policy play the game, and at each step, compute the gradients that would make the *chosen* action even more likely. Don't apply them yet.\n",
    "2.  **Compute advantages:** After all episodes, compute the advantage for every action taken.\n",
    "3.  **Modulate gradients:** Multiply the gradients from step 1 by the action's advantage. \n",
    "    * If the advantage is positive (a \"good\" action), the gradients are kept as-is. This reinforces the action.\n",
    "    * If the advantage is negative (a \"bad\" action), the gradients are *reversed*. This discourages the action.\n",
    "4.  **Update weights:** Compute the mean of all these modulated gradients and use it to perform a Gradient Descent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # The \"ideal\" prob\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return obs, reward, done, grads\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute discounted rewards (returns)\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "# Function to compute and normalize the returns (advantages)\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions\n",
    "print(discount_rewards([10, 0, -50], discount_factor=0.8))\n",
    "print(discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_factor=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for iteration in range(n_iterations):\n",
    "    # 1. Play episodes and get rewards/gradients\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    \n",
    "    # 2. Compute normalized advantages\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        # 3. Modulate gradients by the advantage\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    \n",
    "    # 4. Apply the gradients\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "A **Markov Decision Process (MDP)** is the mathematical framework for RL. It assumes that the future state depends only on the current state and action, not on the past. This is called the **Markov property**.\n",
    "\n",
    "An MDP is defined by:\n",
    "* A set of **states** (s)\n",
    "* A set of **actions** (a)\n",
    "* A **transition probability** *T(s, a, s')*: the probability of moving from state *s* to state *s'* given action *a*.\n",
    "* A **reward** *R(s, a, s')*: the reward received after the transition.\n",
    "* A **discount factor (γ)**.\n",
    "\n",
    "The goal is to find the **optimal policy** π*(s), which is the policy that maximizes the expected return (sum of discounted future rewards). \n",
    "\n",
    "#### Bellman Optimality Equation\n",
    "Richard Bellman found an equation to find the optimal **state value** *V*(s) (the expected return if the agent starts in state *s* and acts optimally).\n",
    "\n",
    "`V*(s) = max_a Σ_{s'} T(s, a, s') * [R(s, a, s') + γ * V*(s')]`\n",
    "\n",
    "This equation means: \"The optimal value of a state is the reward you get for taking the best action, plus the discounted optimal value of the next state you land in.\"\n",
    "\n",
    "This equation can be solved with the **Value Iteration** algorithm.\n",
    "\n",
    "#### Q-Value Iteration\n",
    "It's often more useful to find the **state-action value** or **Q-Value** *Q*(s, a). This is the expected return if the agent starts in state *s*, chooses action *a*, and then acts optimally *after* that.\n",
    "\n",
    "The algorithm to find the optimal Q-Values is called **Q-Value Iteration**. Once we have the optimal Q-Values, the optimal policy is simple: always pick the action with the highest Q-Value.\n",
    "\n",
    "`π*(s) = argmax_a Q*(s, a)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example MDP (from the book)\n",
    "transition_probabilities = [\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "    [None, [0.8, 0.1, 0.1], None]]\n",
    "rewards = [\n",
    "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "\n",
    "# Initialize Q-Values to 0 for possible actions, -inf for impossible ones\n",
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Value Iteration\n",
    "gamma = 0.90 # discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                transition_probabilities[s][a][sp] *\n",
    "                (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])\n",
    "\n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal policy is to take the action with the highest Q-Value in each state\n",
    "print(np.argmax(Q_values, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference (TD) Learning\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The Value Iteration algorithm requires knowing the transition probabilities *T* and rewards *R*. What if we don't? \n",
    "\n",
    "**TD Learning** is an algorithm that learns *without* knowing *T* or *R*. It's very similar to Value Iteration, but it updates the state value *V(s)* based on the transition it *actually observes* (the `TD target`).\n",
    "\n",
    "`V_{k+1}(s) = (1-α)V_k(s) + α(r + γV_k(s'))`\n",
    "\n",
    "This is a *running average*. It updates the current value `V_k(s)` by moving it slightly in the direction of the `TD target` (`r + γV_k(s')`). *r* and *s'* are the reward and next state that were actually observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "**Q-Learning** is the adaptation of Q-Value Iteration for when *T* and *R* are unknown. It is a **model-free** algorithm.\n",
    "\n",
    "Like TD Learning, it updates the Q-Values based on the transitions and rewards the agent actually observes as it explores the environment.\n",
    "\n",
    "The Q-Learning update rule:\n",
    "`Q_{k+1}(s, a) = (1-α)Q_k(s, a) + α(r + γ * max_{a'} Q_k(s', a'))`\n",
    "\n",
    "This algorithm is **off-policy**, meaning the policy being trained (the optimal, greedy policy) is *not* the one being executed (the exploration policy, e.g., a random one). It learns the optimal policy even by watching a sub-optimal agent explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a function to simulate the environment step\n",
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward\n",
    "\n",
    "# We also need an exploration policy\n",
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Q-Values\n",
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0\n",
    "\n",
    "# Q-Learning algorithm\n",
    "alpha0 = 0.05 # initial learning rate\n",
    "decay = 0.005 # learning rate decay\n",
    "gamma = 0.90 # discount factor\n",
    "state = 0 # initial state\n",
    "\n",
    "for iteration in range(10000):\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = np.max(Q_values[next_state]) # This is the max_a' part\n",
    "    \n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "print(Q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Policies\n",
    "\n",
    "A purely random policy is inefficient. A better approach is the **ε-greedy (epsilon-greedy)** policy: \n",
    "* With probability **ε**: act randomly (explore).\n",
    "* With probability **1-ε**: act greedily (exploit by picking the action with the highest Q-Value).\n",
    "\n",
    "Epsilon (ε) is typically started high (e.g., 1.0) and gradually decayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Q-Learning and Deep Q-Learning\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The Q-Learning algorithm requires storing a Q-Value for every possible `(state, action)` pair in a table. This is impossible for complex problems like Atari games, where the number of states is astronomical (e.g., 2¹⁵⁰).\n",
    "\n",
    "**Approximate Q-Learning** solves this by using a function (a model) to *estimate* the Q-Values, *Q<sub>θ</sub>(s, a)*, using a manageable number of parameters (θ).\n",
    "\n",
    "**Deep Q-Learning** is when we use a **Deep Q-Network (DQN)** (a deep neural net) as this function. Typically, the network takes the *state* (e.g., a game screenshot) as input and outputs *one Q-Value for every possible action*.\n",
    "\n",
    "To train it, we use the Bellman equation. We define a **target Q-Value**:\n",
    "`y(s, a) = r + γ * max_{a'} Q_θ(s', a')`\n",
    "\n",
    "This is the reward *r* we actually got, plus the discounted *estimated* optimal Q-Value of the next state *s'*. We then train the network to minimize the MSE (or Huber loss) between its prediction `Q_θ(s, a)` and this target `y(s, a)`.\n",
    "\n",
    "To break the correlation between consecutive experiences, we use a **replay buffer (or replay memory)**. We store all experiences `(s, a, r, s')` in the buffer, and at each training step, we sample a random batch from this buffer to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the CartPole environment again\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "obs = env.reset()\n",
    "\n",
    "# Define the DQN model\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "from collections import deque\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "# Function to sample a batch of experiences\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "# Function to play one step and store the experience\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training step function\n",
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Compute target Q-Values\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards + \n",
    "                       (1 - dones) * discount_factor * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    \n",
    "    # Compute predicted Q-Values and loss\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main training loop\n",
    "for episode in range(600):\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if episode > 50: # Start training after 50 episodes to fill the buffer\n",
    "        training_step(batch_size)\n",
    "\n",
    "    # Print progress (optional)\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode {episode + 1}/600, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning Variants\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The basic DQN algorithm is unstable. Several variants were introduced to stabilize and improve it:\n",
    "\n",
    "* **Fixed Q-Value Targets:** The basic DQN uses the *same* network to both predict `Q(s, a)` and to calculate the target `y(s, a)`. This creates a feedback loop (a dog chasing its tail) and leads to instability. \n",
    "    * **Solution:** Use two DQNs. The **online model** is trained at every step. The **target model** is a clone of the online model that is only used to *calculate the target Q-Values*. Its weights are frozen for long periods (e.g., copied from the online model every 10,000 steps). This breaks the feedback loop and stabilizes training.\n",
    "\n",
    "* **Double DQN:** The target model tends to overestimate Q-Values (it always picks the `max` action, even if its value is high due to random error). \n",
    "    * **Solution:** Use the *online* model to *select* the best next action, but use the *target* model to *estimate the Q-Value* of that action. \n",
    "    * `target_Q_value = r + γ * Q_{target}(s', argmax_{a'} Q_{online}(s', a'))`\n",
    "\n",
    "* **Prioritized Experience Replay (PER):** Instead of sampling experiences *uniformly* from the replay buffer, sample \"important\" experiences more often. \"Important\" is defined as experiences with a large **TD error** (i.e., `target Q-Value - predicted Q-Value`). These are experiences the model was very \"surprised\" by. To compensate for the sampling bias, we down-weight these samples during the gradient update.\n",
    "\n",
    "* **Dueling DQN:** A different network architecture that explicitly separates the estimation of the *state value* `V(s)` (how good is this state?) from the *action advantage* `A(s, a)` (how much better is this action than the others?). The final Q-Value is then `Q = V + A`. This helps the network learn `V(s)` without having to care about the impact of every action, leading to more robust estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Double DQN (Fixed Q-Value Targets is similar)\n",
    "# We need a target model\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())\n",
    "\n",
    "def training_step_double_dqn(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Use online model to SELECT the best next action\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    \n",
    "    # Use target model to EVALUATE that action\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
    "    \n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_factor * next_best_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    \n",
    "    # Rest is the same\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Dueling DQN (architecture only)\n",
    "K = keras.backend\n",
    "input_states = keras.layers.Input(shape=[4])\n",
    "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
    "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
    "\n",
    "# State value (V(s))\n",
    "state_values = keras.layers.Dense(1)(hidden2)\n",
    "\n",
    "# Action advantages (A(s, a))\n",
    "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
    "\n",
    "# Combine: Q = V + (A - mean(A))\n",
    "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
    "Q_values = state_values + advantages\n",
    "\n",
    "model_dueling = keras.Model(inputs=[input_states], outputs=[Q_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TF-Agents Library\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "Implementing all these variants is complex. The **TF-Agents** library from Google provides a flexible and scalable framework for RL that includes pre-built, state-of-the-art algorithms, environments, and training components.\n",
    "\n",
    "The typical architecture of a TF-Agents program involves:\n",
    "* **Environments:** The world the agent lives in. TF-Agents provides wrappers for Gym, Atari, etc. `TFPyEnvironment` wraps a Python env to be used in TF graphs.\n",
    "* **Networks:** The neural nets (e.g., `QNetwork`).\n",
    "* **Agents:** The \"brain\" of the agent (`DqnAgent`). It holds the network, the policy, and the training logic.\n",
    "* **Replay Buffers:** Stores experiences (e.g., `TFUniformReplayBuffer`).\n",
    "* **Drivers:** Orchestrates the interaction. A **driver** uses a **policy** to take actions in the **environment**, collects the resulting **trajectories**, and sends them to **observers** (like the replay buffer).\n",
    "* **Dataset:** The `ReplayBuffer` can be converted to a `tf.data.Dataset` for efficient, asynchronous training.\n",
    "* **Training Loop:** The main loop that collects new experiences (using the driver) and trains the agent (by sampling from the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tf-agents and gym[atari]\n",
    "# !python3 -m pip install -U tf-agents\n",
    "# !python3 -m pip install -U 'gym[atari]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the components\n",
    "import tensorflow_datasets as tfds\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.utils.common import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the Environment (with preprocessing wrappers)\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames\n",
    "environment_name = \"BreakoutNoFrameskip-v4\"\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    # AtariPreprocessing grayscales and resizes to 84x84\n",
    "    # FrameStack4 stacks 4 consecutive frames (channels dimension)\n",
    "    gym_env_wrappers=[AtariPreprocessing, FrameStack4])\n",
    "\n",
    "# Wrap it in a TFPyEnvironment\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the Deep Q-Network (DQN)\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "    lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the DQN Agent\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # train the model every 4 steps\n",
    "optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0,\n",
    "                                   epsilon=0.00001, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=250000 // update_period, \n",
    "    end_learning_rate=0.01) # final ε\n",
    "\n",
    "agent = DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    target_update_period=2000, # target net update frequency\n",
    "    td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "    gamma=0.99, # discount factor\n",
    "    train_step_counter=train_step,\n",
    "    epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create the Replay Buffer and Observer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=1000000) # 1M trajectories\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create Training Metrics\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create the Collect Driver\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Populate the replay buffer with an initial random policy (warmup)\n",
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                      tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps=20000) # 20,000 steps\n",
    "\n",
    "final_time_step, final_policy_state = init_driver.run()\n",
    "print(\"\\nReplay buffer populated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create the Dataset\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2, # 2 steps = 1 transition (s, a, r, s')\n",
    "    num_parallel_calls=3).prefetch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Create the Training Loop\n",
    "\n",
    "# Convert the main functions to TF Functions for speed\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Collect new experiences\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        \n",
    "        # Sample a batch of trajectories and train the agent\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "\n",
    "        # Log metrics every 1000 iterations\n",
    "        if iteration % 1000 == 0:\n",
    "            # This requires tf_agents.eval.metric_utils.log_metrics\n",
    "            # log_metrics(train_metrics) \n",
    "            # We'll just print one metric for simplicity:\n",
    "            print(f\"\\nIteration: {iteration}, Avg Return: {train_metrics[2].result().numpy()}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a very long time to run!\n",
    "# train_agent(10000000)\n",
    "print(\"Training loop defined. Ready to run (but it will take a long time).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Some Popular RL Algorithms\n",
    "\n",
    "* **Actor-Critic (A2C/A3C):** Combines Policy Gradients and Q-Learning. The "Critic" (a DQN) learns the value of states, and the "Actor" (a policy network) uses the Critic's estimates to learn its policy. A3C is an *asynchronous* version where multiple agents learn in parallel.\n",
    "* **Proximal Policy Optimization (PPO):** An algorithm based on A2C that is very popular and stable. It clips the loss function to prevent excessively large policy updates, which stabilizes training.\n",
    "* **Soft Actor-Critic (SAC):** A modern Actor-Critic algorithm that aims to *maximize reward* and also *maximize the entropy* of its actions. This encourages the agent to be as unpredictable as possible (while still getting rewards), which greatly improves exploration and speeds up learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "See Appendix A in the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
