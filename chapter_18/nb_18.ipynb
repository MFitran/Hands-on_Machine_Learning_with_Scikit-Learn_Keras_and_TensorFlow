{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Reinforcement Learning\n",
    "\n",
    "**Based on \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\" by Aurélien Géron**\n",
    "\n",
    "This notebook reproduces the code from Chapter 18 and provides theoretical explanations for each concept, as required by the individual task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter introduces **Reinforcement Learning (RL)**, a distinct branch of machine learning where an **agent** learns to make optimal decisions by interacting with an **environment**.\n",
    "\n",
    "1.  **Core Concepts:** We start with the fundamental concepts of RL. The agent observes the environment, takes **actions**, and receives **rewards** (or penalties). The agent's goal is to learn a **policy** (a strategy) that maximizes its total expected reward over time. This involves the **credit assignment problem** (figuring out which actions led to a reward) and the use of a **discount factor (γ)** to value immediate rewards more than distant ones.\n",
    "\n",
    "2.  **Policy Search & Policy Gradients (PG):** The first major technique covered is **Policy Gradients**. Here, the agent's policy is a neural network that directly outputs action probabilities. The agent learns by running through episodes, calculating the advantage of each action (based on the total future discounted rewards, or **returns**), and then running a gradient descent step that *increases* the probability of actions that led to good returns and *decreases* the probability of actions that led to bad returns.\n",
    "\n",
    "3.  **Markov Decision Processes (MDPs):** To understand more advanced techniques, we explore the formal framework of MDPs. This involves states, actions, transition probabilities, and rewards. We learn about the **Bellman Optimality Equation**, which leads to algorithms like **Value Iteration** and **Q-Value Iteration** for finding the optimal policy when the environment's dynamics are known.\n",
    "\n",
    "4.  **Q-Learning:** For cases where the environment is unknown (the typical RL problem), we use **Q-Learning**. This is a **Temporal Difference (TD) learning** algorithm where the agent learns the optimal **Q-Value** (expected return) for every state-action pair by observing transitions and rewards as it explores the environment.\n",
    "\n",
    "5.  **Deep Q-Learning (DQN):** Q-Learning doesn't scale to problems with large state spaces (like video games). **Deep Q-Learning** solves this by using a deep neural network (a **DQN**) to *approximate* the Q-Values. This section also introduces two critical components for stabilizing training:\n",
    "    * **Experience Replay (Replay Buffer):** Stores the agent's experiences (state, action, reward, next_state) so it can train on random batches of past experiences, breaking temporal correlations.\n",
    "    * **Fixed Q-Value Targets:** Uses a separate **target model** to generate the target Q-Values, which stabilizes the training process by preventing the model from \"chasing its own tail.\"\n",
    "\n",
    "6.  **The TF-Agents Library:** We put all these concepts into practice using TF-Agents, Google's official library for Reinforcement Learning. We learn its components:\n",
    "    * **Environments:** How to wrap OpenAI Gym environments for use with TensorFlow.\n",
    "    * **Agents:** How to instantiate a `DqnAgent`.\n",
    "    * **Replay Buffers:** How to create a buffer to store experiences.\n",
    "    * **Drivers:** How to create a `DynamicStepDriver` to run the agent in the environment and collect experiences.\n",
    "    * **Training Loop:** How to build the full training pipeline to train an agent to play the Atari game Breakout.\n",
    "\n",
    "Finally, the chapter gives a brief overview of more advanced algorithms like Actor-Critic (A3C, A2C), PPO, and SAC, which form the basis of modern RL research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We will need to install `gym` for the environments and `tf-agents` for the main RL library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U gym\n",
    "# !pip install -q -U tf-agents\n",
    "# !pip install -q -U 'gym[atari]'\n",
    "# !pip install -q -U 'gym[box2d]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "\n",
    "# Common imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "# A helper function to plot a gym environment\n",
    "def plot_environment(env, figsize=(5,4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to OpenAI Gym\n",
    "\n",
    "> **Theoretical Deep-Dive: The Agent-Environment Loop**\n",
    "> \n",
    "> Reinforcement Learning consists of an **agent** interacting with an **environment**. This interaction forms a loop:\n",
    "> 1.  The agent observes the environment's current **state** (or **observation**).\n",
    "> 2.  Based on this observation, the agent's **policy** chooses an **action**.\n",
    "> 3.  The agent performs the action, and the environment transitions to a new state.\n",
    "> 4.  The environment gives the agent a **reward** (a positive or negative number) for this transition.\n",
    "> 5.  The loop repeats. The agent's goal is to learn a policy that maximizes the total cumulative reward.\n",
    ">\n",
    "> **OpenAI Gym** is a toolkit that provides a wide variety of simulated environments (like Atari games, classic control problems, and robotics) with a standard API. This lets us build agents without having to build a new environment/simulator every time.\n",
    ">\n",
    "> The key Gym methods are:\n",
    "> -   `make(env_name)`: Creates an environment.\n",
    "> -   `reset()`: Resets the environment to its initial state and returns the first observation.\n",
    "> -   `step(action)`: Performs the given action and returns a tuple: `(observation, reward, done, info)`.\n",
    ">     -   `observation`: The new state of the environment.\n",
    ">     -   `reward`: The reward from the last action.\n",
    ">     -   `done`: A boolean that is `True` when the episode (e.g., the game) is over.\n",
    ">     -   `info`: A dictionary with extra, environment-specific information (we usually ignore this).\n",
    "> -   `render()`: Displays the environment (e.g., in a pop-up window).\n",
    "> -   `action_space`: The object that describes the set of possible actions (e.g., `Discrete(2)` for two discrete actions).\n",
    "> -   `observation_space`: The object that describes the shape and type of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "print(\"Initial Observation:\", obs)\n",
    "plot_environment(env);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space:\", env.action_space)\n",
    "print(\"Observation Space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcode a simple policy: accelerate left if pole leans left, right if pole leans right.\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "# Run 500 episodes and see how long it lasts\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "print(f\"Mean: {np.mean(totals)}, Std: {np.std(totals)}, Min: {np.min(totals)}, Max: {np.max(totals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple policy isn't great. The cart oscillates too much and the pole falls. We need a model that can learn a better strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Policies (Policy Gradients)\n",
    "\n",
    "> **Theoretical Deep-Dive: Policy Gradients**\n",
    ">\n",
    "> A **policy** is a neural network that takes an observation as input and outputs action probabilities (a **stochastic policy**). We then *sample* an action from this probability distribution. This sampling allows the agent to **explore** new actions, rather than just **exploiting** the actions it already knows are good.\n",
    ">\n",
    "> But how do we train this network? We don't have target labels. We only have rewards, which are often delayed (e.g., you only win a game at the very end). This is the **credit assignment problem**.\n",
    ">\n",
    "> **Policy Gradients (PG)** solve this with three steps:\n",
    "> 1.  **Run & Collect:** Let the policy play the game for several full episodes and collect all observations, actions, and rewards.\n",
    "> 2.  **Evaluate Actions:** For every action taken, estimate how \"good\" it was. We do this by looking at all the rewards that came *after* it. A common strategy is to calculate the **return** (sum of future discounted rewards) for each time step. We then **normalize** these returns (subtract the mean, divide by std dev). This normalized score is called the **action advantage**. A positive advantage means the action was good, and a negative one means it was bad.\n",
    "> 3.  **Train (REINFORCE Algorithm):** We run a training step (like in supervised learning), but with a trick. We pretend the action the agent *actually* took was the \"correct\" one. But we modulate the loss. We multiply the loss by the action's **advantage**.\n",
    ">     -   If the advantage was **positive** (a good action), the gradients are applied as-is. This *increases* the probability of taking that action in the future (e.g., `y_target = 1 - action`).\n",
    ">     -   If the advantage was **negative** (a bad action), the gradients are *reversed*. This *decreases* the probability of taking that action in the future.\n",
    ">\n",
    "> This pushes the policy to take more actions that lead to high rewards and fewer actions that lead to low rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the PG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"), # Outputs the probability of going left (action 0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plays one step in the environment.\n",
    "# It computes the gradients that would make the chosen action *more* likely.\n",
    "# We don't apply these gradients yet; we just return them.\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba) # Sample an action\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # The \"fake\" target\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return obs, reward, done, grads\n",
    "\n",
    "# This function plays multiple full episodes and returns all rewards and gradients.\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the discounted returns and normalizes them.\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    # 1. Run & Collect\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    \n",
    "    # 2. Evaluate Actions\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        # 3. Train (Modulate gradients)\n",
    "        # Multiply each gradient by its action's advantage (the final_reward)\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    \n",
    "    # 3. Train (Apply gradients)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n",
    "    print(f\"Iteration: {iteration+1}/{n_iterations}, Mean Rewards: {np.mean([sum(rewards) for rewards in all_rewards])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Markov Decision Processes (MDPs)\n",
    "\n",
    "> **Theoretical Deep-Dive: MDPs, Value Iteration, and Q-Value Iteration**\n",
    ">\n",
    "> **MDPs** are the classic framework for RL problems. An MDP is defined by:\n",
    "> 1.  A set of **states** (s).\n",
    "> 2.  A set of **actions** (a).\n",
    "> 3.  **Transition probabilities** T(s, a, s'): the probability of moving from state `s` to state `s'` given action `a`.\n",
    "> 4.  **Rewards** R(s, a, s'): the reward received for this transition.\n",
    ">\n",
    "> The **Bellman Optimality Equation** provides a recursive definition of the optimal **state value** V*(s) (the expected return if the agent starts in state `s` and acts optimally forever after):\n",
    ">\n",
    "> $V^*(s) = \\max_{a} \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma \\cdot V^*(s')]$ \n",
    ">\n",
    "> This equation says: \"The optimal value of a state is the expected value of the best action you can take from it.\"\n",
    ">\n",
    "> This leads to the **Value Iteration** algorithm, which solves the equation iteratively.\n",
    ">\n",
    "> A more useful value is the **Q-Value** (or state-action value) Q(s, a), which is the expected return if the agent starts in state `s`, performs action `a`, and *then* acts optimally forever after. The **Q-Value Iteration** algorithm finds these optimal Q-Values:\n",
    ">\n",
    "> $Q_{k+1}(s, a) = \\sum_{s'} T(s, a, s') [R(s, a, s') + \\gamma \\cdot \\max_{a'} Q_k(s', a')]$ \n",
    ">\n",
    "> Once you have the optimal Q-Values, the optimal policy $\\pi^*(s)$ is simply to choose the action with the highest Q-Value in that state: $\\pi^*(s) = \\argmax_{a} Q^*(s, a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small, simple MDP (from Figure 18-8 in the book)\n",
    "transition_probabilities = [ # shape=[s, a, s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "    [None, [0.8, 0.1, 0.1], None]]\n",
    "rewards = [ # shape=[s, a, s']\n",
    "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "\n",
    "# Initialize Q-Values\n",
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions\n",
    "\n",
    "# Run the Q-Value Iteration algorithm\n",
    "gamma = 0.90 # discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                transition_probabilities[s][a][sp] *\n",
    "                (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])\n",
    "\n",
    "print(\"Final Q-Values:\\n\", Q_values)\n",
    "print(\"Optimal Policy (best action for each state):\", np.argmax(Q_values, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q-Learning\n",
    "\n",
    "> **Theoretical Deep-Dive: Q-Learning (Off-Policy TD Learning)**\n",
    ">\n",
    "> The Q-Value Iteration algorithm assumes we *know* the transition probabilities (T) and rewards (R). This is almost never the case. \n",
    ">\n",
    "> **Q-Learning** is an algorithm that finds the optimal Q-Values *without* knowing T or R. It is a **Temporal Difference (TD) learning** algorithm, meaning it learns by observing transitions and rewards as they happen.\n",
    ">\n",
    "> The agent explores the environment (e.g., using an **$\\\\[Epsilon]$-greedy policy**). When it transitions from state `s` to `s'` by performing action `a` and receiving reward `r`, it updates the Q-Value for (s, a) using this update rule:\n",
    ">\n",
    "> $Q_{new}(s, a) = (1 - \\alpha) \\cdot Q_{old}(s, a) + \\alpha \\cdot (r + \\gamma \\cdot \\max_{a'} Q_{old}(s', a'))$\n",
    ">\n",
    "> -   $\\\\[Alpha]$ is the **learning rate**.\n",
    "> -   $(r + \\gamma \\cdot \\max_{a'} Q_{old}(s', a'))$ is the **TD Target**, or the *estimated* future value.\n",
    ">\n",
    "> In simple terms: we nudge the old Q-Value a little bit in the direction of our new, better estimate (the TD Target).\n",
    ">\n",
    "> Q-Learning is an **off-policy** algorithm because it learns about the *optimal* policy (the `max` part of the equation) regardless of what policy it is *actually* following to explore (e.g., the $\\\\[Epsilon]$-greedy policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple exploration policy (random)\n",
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "# Helper function to simulate a step\n",
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward\n",
    "\n",
    "# Reset Q-Values\n",
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0\n",
    "\n",
    "alpha0 = 0.05 # initial learning rate\n",
    "decay = 0.005 # learning rate decay\n",
    "gamma = 0.90 # discount factor\n",
    "state = 0 # initial state\n",
    "\n",
    "for iteration in range(10000):\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = np.max(Q_values[next_state]) # This is the max_a' Q(s', a') part\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    \n",
    "    # This is the Q-Learning update rule\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "print(\"Final Q-Values (after 10,000 iterations):\\n\", Q_values)\n",
    "print(\"Optimal Policy (best action for each state):\", np.argmax(Q_values, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep Q-Learning (DQN)\n",
    "\n",
    "> **Theoretical Deep-Dive: Approximate Q-Learning**\n",
    ">\n",
    "> The Q-Learning algorithm requires a table to store the Q-Values for *every possible state-action pair*. This is impossible for complex problems like Atari games, where the number of states (pixel combinations) is astronomical.\n",
    ">\n",
    "> **Deep Q-Learning (DQN)** solves this. Instead of a Q-table, we use a **Deep Q-Network (DQN)** to *approximate* the Q-Values. This network takes a state (e.g., a stack of game frames) as input and outputs an estimated Q-Value for *every possible action* in that state.\n",
    ">\n",
    "> **Experience Replay:** To train the DQN, we use an **Experience Replay Buffer (or Replay Memory)**. This is a `deque` (a fast list-like structure) that stores the agent's experiences (transitions) as tuples: `(state, action, reward, next_state, done)`.\n",
    "> At each training step, we sample a random batch of experiences from this buffer. This breaks the temporal correlation between consecutive experiences, which is critical for stabilizing DQN training.\n",
    ">\n",
    "> **Fixed Q-Value Targets (Target Network):** A regular DQN is unstable because its own predictions are used to define its targets (it's \"chasing its own tail\"). To fix this, we use *two* DQNs:\n",
    "> 1.  The **Online Model:** This is the model we train at every step. It is used to choose actions (the `max_a'` part of the Bellman equation).\n",
    "> 2.  The **Target Model:** This is a clone of the online model. It is used to *calculate* the target Q-Values. Its weights are frozen for many steps and only updated (by copying the online model's weights) periodically. This provides stable targets for the online model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for DQN\n",
    "\n",
    "We need an $\\\\[Epsilon]$-greedy policy, a replay buffer, and a function to sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # Explore\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0]) # Exploit\n",
    "\n",
    "from collections import deque\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)\n",
    "    ]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Training Step\n",
    "\n",
    "This is the core of the DQN algorithm. We use the **Double DQN** and **Fixed Q-Value Targets** logic from the book (which are key improvements over the original DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "# Create the target network\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Double DQN logic: \n",
    "    # 1. Use the *online* model to pick the best *action* for the next state\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    \n",
    "    # 2. Use the *target* model to get the Q-Value for that chosen action\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
    "    \n",
    "    # 3. Compute the target Q-Value\n",
    "    # If done=True (1.0), the future reward is 0. If done=False (0.0), we add the future reward.\n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_factor * next_best_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "\n",
    "    # 4. Train the *online* model to predict these target Q-Values\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards_list = []\n",
    "\n",
    "for episode in range(600):\n",
    "    obs = env.reset()\n",
    "    episode_rewards = 0\n",
    "    for step in range(200):\n",
    "        # Epsilon decays from 1.0 down to 0.01 over 500 episodes\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    episode_rewards_list.append(episode_rewards)\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode: {episode + 1}, Mean Rewards: {np.mean(episode_rewards_list[-50:])}\")\n",
    "\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "        if episode % 50 == 0: # Update target network every 50 episodes\n",
    "            target.set_weights(model.get_weights())\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(episode_rewards_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Sum of Rewards\")\n",
    "plt.title(\"DQN Training on CartPole\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The TF-Agents Library\n",
    "\n",
    "Implementing all the DQN improvements (Double, Dueling, Prioritized Replay) is complex. The **TF-Agents** library provides robust, well-tested implementations of these algorithms and more.\n",
    "\n",
    "> **Theoretical Deep-Dive: TF-Agents Architecture**\n",
    ">\n",
    "> TF-Agents has a clear, modular architecture:\n",
    "> 1.  **Environments:** Wraps Gym environments (e.g., `suite_gym.load()`). `TFPyEnvironment` makes them usable by TensorFlow.\n",
    "> 2.  **Networks:** The policy/Q-networks (e.g., `QNetwork`).\n",
    "> 3.  **Agents:** The \"brain\" that holds the network and the learning algorithm (e.g., `DqnAgent`). The agent has a `collect_policy` (for exploration) and a `policy` (for evaluation).\n",
    "> 4.  **Replay Buffers:** Stores trajectories (e.g., `TFUniformReplayBuffer`).\n",
    "> 5.  **Drivers:** The `DynamicStepDriver` is the \"worker\" that uses the `collect_policy` to play in the environment and sends the collected trajectories to an `observer` (like the replay buffer).\n",
    "> 6.  **Dataset:** The `replay_buffer.as_dataset()` method creates a `tf.data.Dataset` to efficiently sample batches for training.\n",
    "> 7.  **Training:** The `agent.train()` method pulls a batch from the dataset and runs one training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Environment and Wrappers\n",
    "\n",
    "We will train an agent to play the Atari game Breakout. We need to load the environment with several wrappers:\n",
    "-   `AtariPreprocessing`: Converts the image to grayscale, downsamples it, and stacks the last 4 frames (to give the agent a sense of motion).\n",
    "-   `FrameStack4`: Stacks 4 consecutive frames into one observation.\n",
    "-   `TFPyEnvironment`: Makes the Gym environment usable by TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.utils import common\n",
    "\n",
    "max_episode_steps = 27000 # 108k ALE frames (1 step = 4 frames)\n",
    "environment_name = \"BreakoutNoFrameskip-v4\"\n",
    "\n",
    "# Load the base Gym env\n",
    "env = suite_gym.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    gym_env_wrappers=[AtariPreprocessing, FrameStack4])\n",
    "\n",
    "# Wrap it in a TF-Agents environment\n",
    "tf_env = TFPyEnvironment(env)\n",
    "\n",
    "print(\"Observation Spec:\", tf_env.observation_spec())\n",
    "print(\"Action Spec:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating the Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing layer to normalize pixels\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "    lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "\n",
    "# Convolutional layers as defined in the 2015 DQN paper\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # Train the model every 4 steps\n",
    "\n",
    "# Use the same optimizer parameters as the 2015 DQN paper\n",
    "optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "\n",
    "# Epsilon decays from 1.0 to 0.01 over 1 million ALE frames (250k steps)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, \n",
    "    decay_steps=250000 // update_period, \n",
    "    end_learning_rate=0.01)\n",
    "\n",
    "agent = DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    target_update_period=2000, # 8k steps = 32k ALE frames\n",
    "    td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "    gamma=0.99, # Discount factor\n",
    "    train_step_counter=train_step,\n",
    "    epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Replay Buffer and Observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=1000000) # 1 million experiences\n",
    "\n",
    "# The observer is a function that adds a trajectory to the replay buffer\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the Training Metrics and Collect Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_agents.metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "# The collect driver uses the agent's 'collect_policy' (which is epsilon-greedy)\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # Collect 4 steps at a time\n",
    "\n",
    "# We also create a separate driver with a *random* policy to pre-fill the buffer\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                          tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch],\n",
    "    num_steps=20000) # 20k steps = 80k ALE frames\n",
    "\n",
    "print(\"Populating Replay Buffer...\")\n",
    "init_driver.run() # This will take a moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating the Dataset and Training Loop\n",
    "\n",
    "We use `as_dataset()` to create a `tf.data.Dataset` that efficiently samples batches of 2-step trajectories from the buffer. This is what the `agent.train()` method will consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 64 trajectories, each 2 steps long (1 transition)\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2, \n",
    "    num_parallel_calls=3).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the main functions to TF Functions for speed\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # 1. Collect experience\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        \n",
    "        # 2. Sample a batch from the buffer\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        \n",
    "        # 3. Train the agent on that batch\n",
    "        train_loss = agent.train(trajectories)\n",
    "        \n",
    "        if iteration % 1000 == 0:\n",
    "            print(f\"\\rIteration: {iteration}, Loss: {train_loss.loss.numpy():.5f}\")\n",
    "            # You could also log metrics here\n",
    "\n",
    "# Let's train for a few steps to see it working.\n",
    "# A full training run would be 250,000+ iterations.\n",
    "print(\"Starting training...\")\n",
    "train_agent(n_iterations=2000)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Overview of Popular RL Algorithms\n",
    "\n",
    "> **Theoretical Deep-Dive:**\n",
    ">\n",
    "> -   **Actor-Critic (A3C/A2C):** Combines Policy Gradients and DQNs. An **Actor** (the policy) decides which action to take, and a **Critic** (the Q-Network) evaluates how good that action was. The Critic's evaluation is used to train the Actor, which is much more efficient than the PG (REINFORCE) method.\n",
    "> -   **Proximal Policy Optimization (PPO):** A more recent Policy Gradient algorithm from OpenAI that is more stable and less complex than its predecessors. It works by clipping the loss function to prevent destructively large policy updates.\n",
    "> -   **Soft Actor-Critic (SAC):** A modern off-policy algorithm that learns to maximize not only the reward but also the **entropy** of its actions. This encourages exploration and leads to very stable and efficient learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
