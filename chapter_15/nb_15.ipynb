{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 – Processing Sequences Using RNNs and CNNs\n",
    "\n",
    "This notebook contains all the code samples and solutions to the exercises in chapter 15 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition* (O'Reilly). *Note: all code examples are based on the author's original GitHub repository.* \n",
    "\n",
    "**Assignment Instructions:**\n",
    "Per the assignment guidelines, this notebook reproduces the code from Chapter 15. It also includes theoretical explanations and summaries for each concept, as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter introduces **Recurrent Neural Networks (RNNs)**, a class of networks that can handle sequences of arbitrary lengths, making them ideal for tasks involving time series data, natural language, or audio samples[cite: 1397].\n",
    "\n",
    "Key concepts covered include:\n",
    "* **Recurrent Neurons:** The basic building block of an RNN. A recurrent neuron receives inputs from the current time step as well as its *own* output from the previous time step. This feedback loop gives it a form of memory[cite: 1398, 1400].\n",
    "* **RNN Architectures:** We explore different ways to use RNNs:\n",
    "    * **Sequence-to-Vector:** Takes a sequence as input and outputs a single vector (e.g., for sentiment analysis)[cite: 1402].\n",
    "    * **Vector-to-Sequence:** Takes a single vector as input and outputs a sequence (e.g., for image captioning)[cite: 1402].\n",
    "    * **Sequence-to-Sequence (Seq2Seq):** Takes a sequence and outputs a sequence (e.g., for forecasting)[cite: 1401].\n",
    "    * **Encoder-Decoder:** A seq-to-vec model (the encoder) followed by a vec-to-seq model (the decoder), used for tasks like machine translation[cite: 1402].\n",
    "* **Training RNNs:** RNNs are trained using **Backpropagation Through Time (BPTT)**, which is essentially backpropagation applied to the unrolled network[cite: 1403].\n",
    "* **Challenges in Training:** We discuss the two main difficulties of training deep RNNs:\n",
    "    1.  **Unstable Gradients:** The vanishing/exploding gradients problem is very common. We explore solutions like Gradient Clipping, Layer Normalization, and recurrent dropout [cite: 1412-1414].\n",
    "    2.  **Limited Short-Term Memory:** Basic RNNs struggle to learn long-term patterns[cite: 1412].\n",
    "* **Advanced RNN Cells:** To solve the memory problem, we introduce more powerful cells:\n",
    "    * **LSTM (Long Short-Term Memory):** Uses two state vectors (`c` and `h`) and three gates (input, forget, output) to learn what to remember and what to discard [cite: 514-517].\n",
    "    * **GRU (Gated Recurrent Unit):** A simplified and often faster variant of the LSTM that performs just as well [cite: 518-519].\n",
    "* **CNNs for Sequences:** We show that **1D convolutional layers** can be a powerful and fast alternative to RNNs for processing sequences. We build a **WaveNet**-style architecture by stacking `Conv1D` layers with increasing *dilation rates* to capture patterns at different scales [cite: 521-522]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 3.7 or later is required for the latest versions of Scikit-Learn), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neurons and Layers\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "A **recurrent neuron** is a neuron that receives not only inputs from the current time step ($x_{(t)}$) but also its own output from the *previous* time step ($y_{(t-1)}$)[cite: 1398]. This creates a loop, allowing the network to have a form of memory. This internal state (or memory) is called the **hidden state** ($h_{(t)}$)[cite: 1400].\n",
    "\n",
    "A **recurrent layer** is simply a layer of recurrent neurons. At each time step $t$, every neuron in the layer receives the input vector $\\mathbf{x}_{(t)}$ and the output vector from the previous time step, $\\mathbf{y}_{(t-1)}$[cite: 1399]. The output of the whole layer at time step $t$, $\\mathbf{Y}_{(t)}$, is a function of the inputs at time $t$ and the hidden state at time $t-1$[cite: 1400].\n",
    "\n",
    "$$ \\mathbf{Y}_{(t)} = \\phi(\\mathbf{X}_{(t)} \\cdot \\mathbf{W}_x + \\mathbf{Y}_{(t-1)} \\cdot \\mathbf{W}_y + \\mathbf{b}) $$ \n",
    "\n",
    "Because $\\mathbf{Y}_{(t)}$ is a function of $\\mathbf{Y}_{(t-1)}$, which is a function of $\\mathbf{Y}_{(t-2)}$, and so on, the final output is a function of all inputs since the beginning[cite: 1400].\n",
    "\n",
    "**RNN Architectures:**\n",
    "* **Sequence-to-Vector (seq-to-vec):** Feeds in a sequence, ignores all outputs except the last one. Used for tasks like sentiment analysis (feeding in a review, outputting a score)[cite: 1402].\n",
    "* **Vector-to-Sequence (vec-to-seq):** Feeds in the same vector at every time step and outputs a sequence. Used for tasks like image captioning[cite: 1402].\n",
    "* **Sequence-to-Sequence (seq-to-seq):** Feeds in a sequence and outputs a sequence at each step. Used for forecasting time series[cite: 1401].\n",
    "* **Encoder-Decoder:** A seq-to-vec model (Encoder) followed by a vec-to-seq model (Decoder). This is used for tasks like machine translation, where the full input sequence must be read before an output sequence can be generated[cite: 1402]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RNNs\n",
    "\n",
    "### Theoretical Explanation: Backpropagation Through Time (BPTT)\n",
    "\n",
    "To train an RNN, we use **Backpropagation Through Time (BPTT)**. This is a simple and elegant algorithm:\n",
    "\n",
    "1.  The RNN is **unrolled through time** for a given number of steps, creating a regular, deep feedforward network.\n",
    "2.  A forward pass is performed on this unrolled network[cite: 1403].\n",
    "3.  The cost function is evaluated based on the output sequence.\n",
    "4.  The gradients are propagated backward through the unrolled network (the \"backward pass\")[cite: 1403].\n",
    "5.  The model parameters (which are shared across all time steps) are updated by summing the gradients computed at each time step and performing a Gradient Descent step[cite: 1403]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting a Time Series\n",
    "\n",
    "First, let's create a function to generate our time series data. We'll create 10,000 series, each 51 time steps long. We will use the first 50 steps as the input features and the last step as the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Metrics\n",
    "\n",
    "Before building a complex RNN, it's crucial to establish a baseline. \n",
    "1.  **Naive Forecasting:** The simplest approach is to predict the last value in the time series. This gives us an MSE of about 0.02[cite: 1405].\n",
    "2.  **Simple Linear Model:** A fully connected network (Linear Regression) that flattens the sequence and predicts a value based on all 50 steps. This performs much better[cite: 1405]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive forecasting\n",
    "y_pred_naive = X_valid[:, -1]\n",
    "print(\"Naive forecasting MSE:\", np.mean(keras.losses.mean_squared_error(y_valid, y_pred_naive)))\n",
    "\n",
    "# Simple Linear Model\n",
    "model_linear = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50, 1]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_linear.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history_linear = model_linear.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "print(\"Linear model MSE:\", model_linear.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNNs\n",
    "\n",
    "Now let's build our first simple RNN. A `SimpleRNN` layer in Keras with one unit is the simplest RNN we can build[cite: 1406]. By default, recurrent layers in Keras only return the output of the final time step. This is exactly what we want for a sequence-to-vector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1]) # `None` allows sequences of any length\n",
    "])\n",
    "\n",
    "model_simple_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history_simple_rnn = model_simple_rnn.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple RNN doesn't beat the linear model. It's too simple. Let's build a **Deep RNN** by stacking recurrent layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Deep RNNs\n",
    "\n",
    "It is common to stack multiple layers of cells to create a Deep RNN[cite: 1407]. To do this, all recurrent layers (except the last one) must set `return_sequences=True`[cite: 1408]. This tells them to output a sequence (a 3D tensor) for each time step, rather than just the final output (a 2D tensor). The next recurrent layer will then receive this sequence as its input.\n",
    "\n",
    "It is often preferable to replace the final recurrent layer with a `Dense` layer. This is faster, has similar accuracy, and allows us to choose any output activation function we want[cite: 1408]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deep_rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20), # No return_sequences=True, only outputs the last step\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_deep_rnn.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history_deep_rnn = model_deep_rnn.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs much better and beats the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Several Time Steps Ahead\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "What if we want to predict the next 10 values, not just one? \n",
    "\n",
    "**Strategy 1: Single-step predictions, one at a time**\n",
    "Use the trained model to predict the next value. Then, add that value to the input sequence and use the model again to predict the *next* value. Repeat this 10 times[cite: 1409]. This is simple, but errors can accumulate.\n",
    "\n",
    "**Strategy 2: Seq-to-Vec (predict all 10 at once)**\n",
    "Train the RNN to predict all 10 next values at once. We change the targets to be vectors containing the next 10 values, and we change the output layer to have 10 units[cite: 1410].\n",
    "\n",
    "**Strategy 3: Seq-to-Seq (predict 10 steps at *every* step)**\n",
    "This is the most powerful method. We train the model to forecast the next 10 values at *every* time step. \n",
    "* At time step 0, it predicts steps 1 to 10.\n",
    "* At time step 1, it predicts steps 2 to 11.\n",
    "* ...\n",
    "This creates a loss at every time step, which provides more gradients to the model, stabilizing and speeding up training[cite: 1410]. To implement this, all RNN layers must have `return_sequences=True`, and the output layer must be wrapped in a `TimeDistributed` layer. This layer applies the `Dense` layer to every time step of the input sequence[cite: 1411]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Strategy 2 (Seq-to-Vec, 10 steps)\n",
    "\n",
    "# Prepare new targets: Y_train shape will be [7000, 10]\n",
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train_10 = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid_10 = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, Y_test_10 = series[9000:, :n_steps], series[9000:, -10:, 0]\n",
    "\n",
    "model_seq_to_vec = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10) # Output 10 values\n",
    "])\n",
    "\n",
    "model_seq_to_vec.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history_seq_to_vec = model_seq_to_vec.fit(X_train, Y_train_10, epochs=20, \n",
    "                                       validation_data=(X_valid, Y_valid_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Strategy 3 (Seq-to-Seq, 10 steps)\n",
    "\n",
    "# Prepare new targets: Y_train shape will be [7000, 50, 10]\n",
    "Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train_seq = Y[:7000]\n",
    "Y_valid_seq = Y[7000:9000]\n",
    "Y_test_seq = Y[9000:]\n",
    "\n",
    "model_seq_to_seq = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True), # All layers return sequences\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10)) # Apply Dense layer at each time step\n",
    "])\n",
    "\n",
    "# We only care about the MSE at the very last time step for evaluation\n",
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "model_seq_to_seq.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.01), \n",
    "                         metrics=[last_time_step_mse])\n",
    "\n",
    "history_seq_to_seq = model_seq_to_seq.fit(X_train, Y_train_seq, epochs=20,\n",
    "                                      validation_data=(X_valid, Y_valid_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last model is the best performer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Long Sequences\n",
    "\n",
    "### Fighting the Unstable Gradients Problem\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "Training on long sequences means BPTT has to backpropagate through many time steps, making the RNN a very deep network. This can lead to **unstable gradients**[cite: 1412].\n",
    "\n",
    "* **Activation Functions:** Nonsaturating functions like ReLU can make RNNs *more* unstable. The `tanh` function (which saturates) is often the default as it helps control the exploding outputs and gradients[cite: 1412].\n",
    "* **Gradient Clipping:** A common solution to the exploding gradients problem[cite: 1412].\n",
    "* **Batch Normalization:** Cannot be used between time steps, only between recurrent layers (i.e., \"vertically\"). It doesn't yield good results [cite: 512-513].\n",
    "* **Layer Normalization:** A better alternative for RNNs. It normalizes *across the features dimension* at each time step [cite: 512-513]. This is very effective and allows the network to converge much faster.\n",
    "* **Recurrent Dropout:** All recurrent layers in Keras have `dropout` (for inputs) and `recurrent_dropout` (for hidden states) hyperparameters to regularize the model[cite: 514]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tackling the Short-Term Memory Problem\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "As a simple RNN processes a long sequence, it tends to forget the first inputs[cite: 514]. This makes it impossible to learn long-term patterns. To solve this, more complex cells with long-term memory were introduced.\n",
    "\n",
    "#### LSTM (Long Short-Term Memory) Cells\n",
    "The **LSTM cell** is the most popular solution[cite: 514]. Its architecture (pictured in Figure 15-9) is designed to manage a long-term state vector, $\\mathbf{c}_{(t)}$.\n",
    "\n",
    "It uses three **gates** (small neural networks with logistic activation) to control this long-term state:\n",
    "1.  **Forget Gate ($f_{(t)}$):** Decides which parts of the long-term state $\\mathbf{c}_{(t-1)}$ should be erased [cite: 516-517].\n",
    "2.  **Input Gate ($i_{(t)}$):** Decides which parts of the *candidate* state ($\mathbf{g}_{(t)}$) should be added to the long-term state [cite: 516-517].\n",
    "3.  **Output Gate ($o_{(t)}$):** Decides which parts of the long-term state $\\mathbf{c}_{(t)}$ should be read and output as the short-term state $\\mathbf{h}_{(t)}$ [cite: 516-517].\n",
    "\n",
    "This allows the cell to learn to recognize important inputs (via the input gate), store them in the long-term state, preserve them as long as needed (via the forget gate), and extract them when required (via the output gate)[cite: 517].\n",
    "\n",
    "To use it in Keras, just replace `SimpleRNN` with `LSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history_lstm = model_lstm.fit(X_train, Y_train_seq, epochs=20, \n",
    "                            validation_data=(X_valid, Y_valid_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: GRU (Gated Recurrent Unit) Cells\n",
    "\n",
    "The **GRU cell** is a simplified version of the LSTM cell that often performs just as well and is computationally faster [cite: 518-519].\n",
    "\n",
    "It has two main simplifications:\n",
    "1.  It merges the short-term and long-term state vectors into a single vector $\\mathbf{h}_{(t)}$[cite: 519].\n",
    "2.  It uses a single **update gate** ($\mathbf{z}_{(t)}$) to control both the forget and input gates. If the gate outputs 1, the forget gate is open and the input gate is closed, and vice versa. It also has a **reset gate** ($\mathbf{r}_{(t)}$) that controls which part of the previous state is shown to the main layer[cite: 519].\n",
    "\n",
    "To use it in Keras, replace `SimpleRNN` or `LSTM` with `GRU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = keras.models.Sequential([\n",
    "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "model_gru.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history_gru = model_gru.fit(X_train, Y_train_seq, epochs=20, \n",
    "                          validation_data=(X_valid, Y_valid_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 1D Convolutional Layers to Process Sequences\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "While LSTMs and GRUs are powerful, they still struggle with sequences over 100 time steps. A different approach is to use **1D convolutional layers**.\n",
    "\n",
    "A `Conv1D` layer slides several filters (kernels) across a sequence, producing 1D feature maps. Each kernel learns to detect a single, short sequential pattern (e.g., 5 time steps long)[cite: 1420].\n",
    "\n",
    "By stacking `Conv1D` layers with an exponentially increasing **dilation rate** (e.g., 1, 2, 4, 8, ...), the network can learn patterns at different scales: lower layers learn short-term patterns, while higher layers learn long-term patterns. This is the architecture used in **WaveNet**[cite: 521].\n",
    "\n",
    "This approach is very efficient: it can be parallelized (unlike RNNs) and it can handle extremely long sequences[cite: 521].\n",
    "\n",
    "We can also use a `Conv1D` layer as a preprocessing step *before* an RNN. By using a `stride` > 1, the convolutional layer will downsample the sequence, which will both speed up training and help the RNN layers detect longer patterns[cite: 520]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: WaveNet-style model\n",
    "\n",
    "# We use 'causal' padding to ensure the layer does not peek into the future.\n",
    "model_wavenet = keras.models.Sequential()\n",
    "model_wavenet.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "for rate in (1, 2, 4, 8) * 2: # Stack two blocks of layers\n",
    "    model_wavenet.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
    "                                      activation=\"relu\", dilation_rate=rate))\n",
    "model_wavenet.add(keras.layers.Conv1D(filters=10, kernel_size=1)) # Output layer\n",
    "\n",
    "model_wavenet.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history_wavenet = model_wavenet.fit(X_train, Y_train_seq, epochs=20, \n",
    "                                validation_data=(X_valid, Y_valid_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This WaveNet model (and the `Conv1D` + `GRU` model from the notebook) offers the best performance so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "From Chapter 15, page 523:\n",
    "\n",
    "1.  Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN, and a vector-to-sequence RNN?\n",
    "2.  How many dimensions must the inputs of an RNN layer have? What does each dimension represent? What about its outputs?\n",
    "3.  If you want to build a deep sequence-to-sequence RNN, which RNN layers should have `return_sequences=True`? What about a sequence-to-vector RNN?\n",
    "4.  Suppose you have a daily univariate time series, and you want to forecast the next seven days. Which RNN architecture should you use?\n",
    "5.  What are the main difficulties when training RNNs? How can you handle them?\n",
    "6.  Can you sketch the LSTM cell’s architecture?\n",
    "7.  Why would you want to use 1D convolutional layers in an RNN?\n",
    "8.  Which neural network architecture could you use to classify videos?\n",
    "9.  Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets.\n",
    "10. Download the Bach chorales dataset... Train a model... that can predict the next time step (four notes)... Then use this model to generate Bach-like music, one note at a time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
