{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 – Support Vector Machines\n",
    "\n",
    "This notebook contains all the code samples and solutions to the exercises in chapter 5 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition* (O'Reilly). *Note: all code examples are based on the author's original GitHub repository.* \n",
    "\n",
    "**Assignment Instructions:**\n",
    "Per the assignment guidelines, this notebook reproduces the code from Chapter 5. It also includes theoretical explanations and summaries for each concept, as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter dives into **Support Vector Machines (SVMs)**, a powerful and versatile set of machine learning models. SVMs are capable of performing linear or nonlinear classification, regression, and even outlier detection.\n",
    "\n",
    "Readability:    "Key concepts covered include:\n",
    "* **Large Margin Classification:** The fundamental idea behind SVMs is to fit the \"widest possible street\" between classes. This is known as large margin classification.\n",
    "* **Support Vectors:** The instances located on the edge of the street, which control the decision boundary. SVMs are sensitive only to these points.\n",
    "* **Hard vs. Soft Margins:** Hard margin classification strictly imposes that all instances be off the street. Soft margin classification allows for some *margin violations* (instances on or on the wrong side of the street), which is more flexible and generalizes better. This is controlled by the `C` hyperparameter.\n",
    "* **Nonlinear SVMs:** To handle nonlinear datasets, SVMs can use the **kernel trick**. This technique allows the model to get the same result as if it had added many polynomial features, or similarity features (like the Gaussian RBF kernel), but without the huge computational cost of actually adding them.\n",
    "* **SVM Regression (SVR):** The objective is reversed: SVR tries to fit as many instances as possible *on* the street (within the margin) while limiting margin violations. The width of the street is controlled by the `epsilon` (ε) hyperparameter.\n",
    "* **Under the Hood:** We briefly explore the math behind SVMs, including the decision function, the primal vs. dual problem, and how the kernel trick works mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 3.7 or later is required for the latest versions of Scikit-Learn), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"svm\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification\n",
    "\n",
    "### Theoretical Explanation: Large Margin Classification\n",
    "\n",
    "The core idea of an SVM is to find a decision boundary that best separates two classes. Instead of just drawing a line (or hyperplane) between them, an SVM finds the line that is as far as possible from the nearest instances of both classes. This is called **large margin classification**.\n",
    "\n",
    "* The \"street\" is the area between two parallel lines that are pushed as far apart as possible while still separating the classes.\n",
    "* The decision boundary is the line in the middle of this street.\n",
    "* The instances that lie on the edge of the street are called **support vectors**. The SVM's decision boundary is *entirely* determined by these instances. Adding more instances *off* the street will not change the boundary at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Soft Margin vs. Hard Margin\n",
    "\n",
    "* **Hard Margin Classification:** This is the original idea, where we strictly impose that all instances must be *off* the street and on the correct side. This has two major drawbacks:\n",
    "    1.  It only works if the data is perfectly linearly separable.\n",
    "    2.  It is very sensitive to outliers.\n",
    "\n",
    "* **Soft Margin Classification:** This is a more flexible approach that finds a balance between keeping the street as wide as possible and limiting the number of *margin violations* (instances that end up on the street or even on the wrong side).\n",
    "\n",
    "In Scikit-Learn's SVM classes, this balance is controlled by the `C` hyperparameter:\n",
    "* A **low `C`** value makes the street wider, but leads to more margin violations (more regularization).\n",
    "* A **high `C`** value makes the street narrower, but leads to fewer margin violations (less regularization). If `C` is very high, the model approaches a hard margin classification.\n",
    "\n",
    "If your SVM model is overfitting, you should try **reducing `C`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Reproduction: LinearSVC on the Iris Dataset\n",
    "\n",
    "The following code loads the Iris dataset, scales the features (SVMs are sensitive to feature scales), and trains a `LinearSVC` model to detect Iris-Virginica flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Alternatives for Linear SVM\n",
    "\n",
    "The `LinearSVC` class is highly optimized for linear SVMs. However, you can also use two other classes for linear classification:\n",
    "\n",
    "1.  `SVC(kernel=\"linear\", C=1)`: Using the `SVC` class with a linear kernel. It is much slower than `LinearSVC` but can be useful if you need to use the kernel trick (which `LinearSVC` does not support).\n",
    "2.  `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`: Using the `SGDClassifier`. It does not converge as fast as `LinearSVC` but can be useful for huge datasets that do not fit in memory (out-of-core training) or for online classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "\n",
    "### Theoretical Explanation: Handling Nonlinear Data\n",
    "\n",
    "Many datasets are not linearly separable. One approach to handle them is to add more features, such as polynomial features. Adding these can make the dataset linearly separable.\n",
    "\n",
    "The code below demonstrates this by creating a `PolynomialFeatures` transformer, followed by a `StandardScaler` and a `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42, max_iter=10000))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "# Helper function to plot the dataset and decision boundaries\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Polynomial Kernel\n",
    "\n",
    "Adding polynomial features is effective, but at a high degree, it creates a huge number of features, making the model slow.\n",
    "\n",
    "This is where the **kernel trick** comes in. It's a \"miraculous mathematical technique\" that makes it possible to get the same result as if you had added many polynomial features, but *without actually adding them*. This avoids the combinatorial explosion of features.\n",
    "\n",
    "The `SVC` class implements the kernel trick. The code below trains an SVM using a 3rd-degree polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(r\"degree=3, coef0=1, C=5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model is overfitting, you might want to **reduce** the polynomial degree. Conversely, if it is underfitting, you can try **increasing** it. The hyperparameter `coef0` controls how much the model is influenced by high-degree polynomials versus low-degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Gaussian RBF Kernel\n",
    "\n",
    "Another approach to handle nonlinear problems is to add features computed using a **similarity function**. This function measures how much each instance resembles a particular \"landmark\". A common similarity function is the **Gaussian Radial Basis Function (RBF)**.\n",
    "\n",
    "Just like the polynomial features, this can be computationally expensive. But once again, the **kernel trick** works its magic: you can get a similar result as if you had added many similarity features by using the `kernel=\"rbf\"` in the `SVC` class.\n",
    "\n",
    "This kernel is controlled by two hyperparameters: `gamma` (γ) and `C`.\n",
    "* **Increasing `gamma`** makes the bell-shaped curve narrower. This means each instance's range of influence is smaller. The decision boundary ends up being more irregular, wiggling around individual instances. A high `gamma` can lead to **overfitting**.\n",
    "* **Decreasing `gamma`** makes the bell-shaped curve wider. Instances have a larger range of influence, and the decision boundary is smoother. A low `gamma` can lead to **underfitting**.\n",
    "\n",
    "So, `gamma` acts as a regularization hyperparameter. If your model is overfitting, you should **reduce `gamma`**. If it is underfitting, you should **increase `gamma`** (similar to `C`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "    ])\n",
    "rbf_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "plot_predictions(rbf_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(r\"gamma=5, C=0.001\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Rule of thumb:** As a rule of thumb, you should always try the linear kernel first (`LinearSVC` is much faster than `SVC(kernel=\"linear\")`). If the training set is not too large, you should also try the Gaussian RBF kernel; it works well in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "### Theoretical Explanation: SVR\n",
    "\n",
    "SVMs can also be used for regression. The trick is to reverse the objective: instead of trying to fit the *widest possible street between* two classes, SVM Regression (SVR) tries to fit as many instances as possible *on* the street.\n",
    "\n",
    "The width of the street is controlled by a hyperparameter, `epsilon` (ε). The model is said to be **ε-insensitive** because adding more training instances *within* the margin does not affect the model’s predictions.\n",
    "\n",
    "Margin violations (instances *off* the street) are penalized, and this is controlled by the `C` hyperparameter, just like in SVM classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Generate some linear data\n",
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = (4 + 3 * X + np.random.randn(m, 1)).ravel()\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Reproduction: Nonlinear SVR with Kernels\n",
    "\n",
    "To tackle nonlinear regression tasks, you can use a kernelized SVM model. The code below uses Scikit-Learn’s `SVR` class (the regression equivalent of `SVC`) with a 2nd-degree polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Generate some quadratic data\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the Hood: A Theoretical Deep-Dive\n",
    "\n",
    "This section provides the theoretical explanations required by the assignment, summarizing the mathematical concepts behind SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Function and Predictions\n",
    "\n",
    "A linear SVM classifier predicts the class of a new instance $\\mathbf{x}$ by computing the decision function $\\mathbf{w}^\\top \\cdot \\mathbf{x} + b$. If the result is positive, the predicted class $\\hat{y}$ is 1 (positive class); otherwise, it is 0 (negative class).\n",
    "\n",
    "$$\\hat{y} = \\begin{cases} 0 & \\text{if } \\mathbf{w}^\\top \\cdot \\mathbf{x} + b < 0, \\\\ 1 & \\text{if } \\mathbf{w}^\\top \\cdot \\mathbf{x} + b \\ge 0 \\end{cases}$$ \n",
    "\n",
    "The decision boundary is the set of points where the decision function is equal to 0. The margin is defined by the points where the decision function is equal to 1 or –1. Training a linear SVM means finding the values of $\\mathbf{w}$ (weight vector) and $b$ (bias term) that make this margin as wide as possible while avoiding or limiting margin violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Objective: Primal Problem\n",
    "\n",
    "We want to minimize the norm of the weight vector, $\\|\\mathbf{w}\\|$, to get a large margin. It is simpler to minimize $\\frac{1}{2}\\|\\mathbf{w}\\|^2 = \\frac{1}{2}\\mathbf{w}^\\top \\cdot \\mathbf{w}$, which gives the same result.\n",
    "\n",
    "**Hard Margin Objective:**\n",
    "We want to find $\\mathbf{w}$ and $b$ that minimize $\\frac{1}{2}\\mathbf{w}^\\top \\cdot \\mathbf{w}$ subject to $t^{(i)}(\\mathbf{w}^\\top \\cdot \\mathbf{x}^{(i)} + b) \\ge 1$ for all $i=1, \\dots, m$.\n",
    "*(Here, $t^{(i)} = -1$ for negative instances and $t^{(i)} = 1$ for positive instances)*.\n",
    "\n",
    "**Soft Margin Objective:**\n",
    "To allow for margin violations, we introduce a **slack variable** $\\zeta^{(i)} \\ge 0$ for each instance. $\\zeta^{(i)}$ measures how much the $i$-th instance is allowed to violate the margin.\n",
    "We now have two conflicting objectives: make the slack variables as small as possible (to reduce margin violations) and make $\\frac{1}{2}\\mathbf{w}^\\top \\cdot \\mathbf{w}$ as small as possible (to increase the margin). The `C` hyperparameter sets the trade-off.\n",
    "\n",
    "The objective function becomes:\n",
    "Minimize $\\frac{1}{2}\\mathbf{w}^\\top \\cdot \\mathbf{w} + C \\sum_{i=1}^{m} \\zeta^{(i)}$\n",
    "Subject to $t^{(i)}(\\mathbf{w}^\\top \\cdot \\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)}$ and $\\zeta^{(i)} \\ge 0$ for all $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dual Problem\n",
    "\n",
    "This optimization problem is a **Quadratic Programming (QP)** problem. It can be solved directly (the *primal problem*), but it is often easier and faster to solve its **dual problem**.\n",
    "\n",
    "The dual problem is faster to solve than the primal when the number of training instances $m$ is smaller than the number of features $n$. More importantly, it makes the **kernel trick** possible, while the primal does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized SVMs\n",
    "\n",
    "The **kernel trick** is the core idea that allows SVMs to handle nonlinear datasets efficiently.\n",
    "\n",
    "Let's say we apply a 2nd-degree polynomial transformation $\\phi$ to two 2D vectors $\\mathbf{a}$ and $\\mathbf{b}$. The dot product of the *transformed* vectors is:\n",
    "\n",
    "$$\\phi(\\mathbf{a})^\\top \\cdot \\phi(\\mathbf{b}) = (a_1^2 + \\sqrt{2}a_1a_2 + a_2^2) \\cdot (b_1^2 + \\sqrt{2}b_1b_2 + b_2^2) = (a_1b_1 + a_2b_2)^2 = (\\mathbf{a}^\\top \\cdot \\mathbf{b})^2$$ \n",
    "\n",
    "**This is the key insight:** The dot product of the transformed vectors is equal to the square of the dot product of the original vectors.\n",
    "\n",
    "A **kernel** is a function $K(\\mathbf{a}, \\mathbf{b})$ that can compute the dot product $\\phi(\\mathbf{a})^\\top \\cdot \\phi(\\mathbf{b})$ based only on the original vectors $\\mathbf{a}$ and $\\mathbf{b}$, without having to compute the transformation $\\phi$ at all.\n",
    "\n",
    "Common kernels include:\n",
    "* **Linear:** $K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^\\top \\cdot \\mathbf{b}$\n",
    "* **Polynomial:** $K(\\mathbf{a}, \\mathbf{b}) = (\\gamma \\mathbf{a}^\\top \\cdot \\mathbf{b} + r)^d$\n",
    "* **Gaussian RBF:** $K(\\mathbf{a}, \\mathbf{b}) = \\exp(-\\gamma \\|\\mathbf{a} - \\mathbf{b}\\|^2)$\n",
    "* **Sigmoid:** $K(\\mathbf{a}, \\mathbf{b}) = \\tanh(\\gamma \\mathbf{a}^\\top \\cdot \\mathbf{b} + r)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "From Chapter 5, page 174:\n",
    "\n",
    "1.  What is the fundamental idea behind Support Vector Machines?\n",
    "2.  What is a support vector?\n",
    "3.  Why is it important to scale the inputs when using SVMs?\n",
    "4.  Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
    "5.  Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
    "6.  Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease γ (gamma)? What about C?\n",
    "7.  How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
    "8.  Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.\n",
    "9.  Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-the-rest to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?\n",
    "10. Train an SVM regressor on the California housing dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
