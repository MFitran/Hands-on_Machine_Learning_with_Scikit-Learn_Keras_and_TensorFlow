{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Support Vector Machines (SVMs)\n",
    "\n",
    "## üìö Assignment Summary\n",
    "\n",
    "This notebook serves as the submission for **Chapter 5** of the book *\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.\"*\n",
    "\n",
    "**Chapter Objective:** This chapter explores Support Vector Machines, a powerful and versatile set of ML models. \n",
    "\n",
    "We will cover:\n",
    "* **Linear SVM Classification:** The fundamental idea of finding the largest possible \"street\" between classes (large margin classification).\n",
    "* **Hard Margin vs. Soft Margin:** How SVMs handle data that isn't perfectly separable and how the `C` hyperparameter controls the trade-off.\n",
    "* **Nonlinear SVM Classification:** Using polynomial features and the **kernel trick** to classify complex, nonlinear datasets.\n",
    "* **Polynomial Kernels** and **Gaussian RBF Kernels**.\n",
    "* **SVM Regression:** How the same concepts can be applied to regression tasks.\n",
    "* **Under the Hood:** The mathematics behind the SVM's decision function, training objective, and the dual problem.\n",
    "\n",
    "This notebook reproduces all code from the chapter and provides theoretical explanations for each concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary modules and define helper functions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"svm\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear SVM Classification\n",
    "\n",
    "### üßë‚Äçüè´ Theoretical Deep-Dive: Large Margin Classification\n",
    "\n",
    "The fundamental idea behind SVMs is to find a decision boundary that not only separates the classes but also stays as far away from the closest training instances as possible. This is called **large margin classification**.\n",
    "\n",
    "* The decision boundary is the solid line in the middle of a \"street.\"\n",
    "* The instances located on the edge of the street are called the **support vectors**. They are the critical instances that \"support\" the decision boundary. Adding more instances *off* the street will not change the boundary at all.\n",
    "\n",
    "#### Hard Margin vs. Soft Margin\n",
    "\n",
    "1.  **Hard Margin Classification:** Strictly imposes that all instances must be off the street and on the correct side. This has two problems:\n",
    "    * It only works if the data is perfectly linearly separable.\n",
    "    * It is very sensitive to outliers.\n",
    "\n",
    "2.  **Soft Margin Classification:** A more flexible model that finds a balance between keeping the street as large as possible and limiting the **margin violations** (i.e., instances that end up in the middle of the street or on the wrong side).\n",
    "\n",
    "In Scikit-Learn's SVM classes, this balance is controlled by the `C` hyperparameter:\n",
    "* **Low `C`:** A wider street, but more margin violations (more regularization).\n",
    "* **High `C`:** A narrower street, but fewer margin violations (less regularization).\n",
    "\n",
    "If your SVM model is overfitting, you should try reducing `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example: Linear SVM\n",
    "\n",
    "We will use the Iris dataset, which we saw in Chapter 4. We'll build a classifier to detect *Iris virginica* flowers based on petal length and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "# We create a pipeline to scale the features and then apply the SVM\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42))\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "# Now we can make predictions\n",
    "print(\"Prediction for [5.5, 1.7]:\", svm_clf.predict([[5.5, 1.7]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `LinearSVC` is generally faster than `SVC(kernel=\"linear\")`. It also regularizes the bias term, so you should center the data first (which `StandardScaler` does). The `loss=\"hinge\"` is the standard loss function for SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nonlinear SVM Classification\n",
    "\n",
    "Many datasets are not linearly separable. One approach to handle them is to add more features, such as polynomial features. This can sometimes make the dataset linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42, max_iter=10000))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "# Helper function to plot the decision boundary\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"Polynomial SVM classification\")\n",
    "save_fig(\"moons_polynomial_svm_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. The Kernel Trick\n",
    "\n",
    "#### üßë‚Äçüè´ Theoretical Deep-Dive: The Kernel Trick\n",
    "\n",
    "Adding polynomial features is simple, but at a high degree, it creates a *huge* number of features, making the model very slow.\n",
    "\n",
    "This is where the **kernel trick** comes in. It's a mathematical technique that allows you to get the same result as if you had added many high-degree polynomial features, *without actually having to add them*. This avoids the combinatorial explosion of features, making training much more efficient.\n",
    "\n",
    "The kernel trick is implemented by the `SVC` class (as opposed to `LinearSVC`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel\n",
    "\n",
    "Let's test the polynomial kernel on the moons dataset. This trains an SVM classifier using a 3rd-degree polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"SVC with Polynomial Kernel (degree=3)\")\n",
    "save_fig(\"moons_kernel_svm_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `coef0` hyperparameter controls how much the model is influenced by high-degree polynomials versus low-degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian RBF Kernel\n",
    "\n",
    "Another popular kernel is the **Gaussian RBF kernel**. It works by adding features based on a *similarity function* that measures how much each instance resembles a particular \"landmark.\"\n",
    "\n",
    "Like the polynomial kernel, the RBF kernel uses the kernel trick to get the benefit of adding many similarity features without the computational cost.\n",
    "\n",
    "It is controlled by two hyperparameters:\n",
    "* `gamma` ($\gamma$): Acts like a regularization hyperparameter. \n",
    "    * **Increasing $\\gamma$** makes the bell-shaped curve narrower, making the decision boundary more irregular and wiggly. This can lead to **overfitting**.\n",
    "    * **Decreasing $\\gamma$** makes the curve wider, resulting in a smoother decision boundary. This can lead to **underfitting**.\n",
    "* `C`: The same as in `LinearSVC`. A large `C` reduces regularization, while a small `C` increases it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "    ])\n",
    "rbf_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "plot_predictions(rbf_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"SVC with RBF Kernel (gamma=5, C=0.001)\")\n",
    "save_fig(\"moons_rbf_svm_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule of thumb:** When training SVMs, you should always try the linear kernel first. If the dataset is not too large, you should also try the Gaussian RBF kernel; it works well in most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM Regression\n",
    "\n",
    "SVMs can also be used for regression tasks. The trick is to reverse the objective:\n",
    "\n",
    "* **SVM Classification** tries to fit the *largest possible street* between two classes.\n",
    "* **SVM Regression** tries to fit *as many instances as possible on* the street, while limiting margin violations (instances *off* the street).\n",
    "\n",
    "The width of the street is controlled by a hyperparameter $\\epsilon$ (epsilon). Adding more instances *within* the margin (on the street) does not affect the model's predictions. The model is therefore said to be **$\\epsilon$-insensitive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random linear data\n",
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = (4 + 3 * X + np.random.randn(m, 1)).ravel()\n",
    "\n",
    "# Train a LinearSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, we can use a kernelized SVM model (with the `SVR` class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random quadratic data\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()\n",
    "\n",
    "# Train an SVR with a polynomial kernel\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Under the Hood (Theoretical Deep-Dive)\n",
    "\n",
    "This section briefly explains the math behind SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Decision Function and Predictions\n",
    "\n",
    "A linear SVM classifier predicts the class of a new instance $\\mathbf{x}$ by computing the decision function: $\\mathbf{w}^T \\mathbf{x} + b$. If the result is positive, the predicted class $\\hat{y}$ is 1 (positive class), otherwise it is 0 (negative class).\n",
    "\n",
    "The decision boundary is the set of points where $\\mathbf{w}^T \\mathbf{x} + b = 0$. The margins are the points where the decision function is equal to 1 or -1. Training a linear SVM means finding the weight vector $\\mathbf{w}$ and bias term $b$ that make this margin as wide as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Training Objective\n",
    "\n",
    "The slope of the decision function is equal to $||\\mathbf{w}||$. The smaller the weight vector $||\\mathbf{w}||$, the larger the margin. \n",
    "\n",
    "We want to minimize $||\\mathbf{w}||$ while ensuring all instances are off the street. This is a **constrained optimization** problem.\n",
    "\n",
    "* **Hard Margin Objective:** We want to minimize $\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}$ (which minimizes $||\\mathbf{w}||$) subject to the constraint $t^{(i)}(\\mathbf{w}^T\\mathbf{x}^{(i)} + b) \\ge 1$ for all instances. (Here $t^{(i)}$ is -1 for negative instances and 1 for positive instances).\n",
    "\n",
    "* **Soft Margin Objective:** To allow for some margin violations, we introduce a **slack variable** $\\zeta^{(i)} \\ge 0$ for each instance. This variable measures how much an instance is allowed to violate the margin. We now have to minimize a cost function that balances a large margin (small $||\\mathbf{w}||$) with few margin violations (small $\\zeta^{(i)}$). The `C` hyperparameter controls this trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. The Dual Problem\n",
    "\n",
    "The SVM optimization problem can be solved in its *primal form* (as described above) or its *dual form*. The dual problem is faster to solve when the number of training instances is smaller than the number of features. \n",
    "\n",
    "Most importantly, the **dual problem is what makes the kernel trick possible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Kernelized SVMs (The Kernel Trick Explained)\n",
    "\n",
    "When we solved the SVM dual problem, we found that the decision function and the objective function only depend on the *dot products* of the input vectors, not the vectors themselves.\n",
    "\n",
    "Suppose we want to apply a 2nd-degree polynomial transformation $\\phi$ to our vectors. The dot product of the *transformed* vectors $\\phi(\\mathbf{a})^T \\phi(\\mathbf{b})$ would be very slow to compute.\n",
    "\n",
    "The **kernel trick** is this: we can use a **kernel function** $K(\\mathbf{a}, \\mathbf{b})$ that gives the *exact same result* as the dot product of the transformed vectors, but without ever having to compute the transformation $\\phi$.\n",
    "\n",
    "For a 2nd-degree polynomial transformation, the kernel function is simply $K(\\mathbf{a}, \\mathbf{b}) = (\\mathbf{a}^T \\mathbf{b})^2$.\n",
    "\n",
    "So, we can get all the benefits of a high-dimensional feature transformation (like making a nonlinear dataset separable) without the massive computational cost, just by replacing the dot product operation with the kernel function. This is the magic of the kernel trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 Conclusion\n",
    "\n",
    "This chapter covered Support Vector Machines in depth. We learned:\n",
    "\n",
    "* The core idea of **large margin classification** and the difference between **hard** and **soft** margin classifiers.\n",
    "* How to use `LinearSVC`, `SVC(kernel=\"linear\")`, and `SGDClassifier(loss=\"hinge\")` for linear classification.\n",
    "* How to use **Polynomial Features** or the **Polynomial Kernel** to handle nonlinear data.\n",
    "* How the **Gaussian RBF kernel** can also handle complex nonlinear datasets, and how its $\\gamma$ and `C` hyperparameters work.\n",
    "* How to use `LinearSVR` and `SVR` for regression tasks.\n",
    "* A high-level overview of the mathematics behind SVMs, including the **dual problem** and the **kernel trick**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
