{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: End-to-End Machine Learning Project\n",
    "\n",
    "## üìö Assignment Summary\n",
    "\n",
    "This notebook serves as the submission for **Chapter 2** of the book *\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.\"*\n",
    "\n",
    "**Chapter Objective:** This chapter walks through an example project end-to-end, pretending to be a data scientist at a real estate company. The goal is to build a model of housing prices in California using census data.\n",
    "\n",
    "This notebook will reproduce all the code from the chapter while also providing theoretical explanations for each step, as required by the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Look at the Big Picture\n",
    "\n",
    "Before diving into the code, it's essential to understand the project's objectives.\n",
    "\n",
    "### 1.1. Frame the Problem\n",
    "\n",
    "The first task is to define the business objective. In this case, the model's output (a prediction of a district's median housing price) will be fed into another Machine Learning system to determine whether it's worth investing in a given area.\n",
    "\n",
    "Based on this, we can frame the problem:\n",
    "* **Supervised Learning:** Yes, because we are given labeled training examples (each instance has a known median housing price).\n",
    "* **Regression Task:** Yes, because we are asked to predict a continuous numeric value (the median housing price). This is a *multiple regression* problem as we will use multiple features to make the prediction.\n",
    "* **Batch Learning:** Yes, because there is no continuous flow of new data, and the dataset is small enough to fit in memory. Plain batch learning is fine.\n",
    "\n",
    "### 1.2. Select a Performance Measure\n",
    "\n",
    "A typical performance measure for regression problems is the **Root Mean Square Error (RMSE)**. It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.\n",
    "\n",
    "$$RMSE(\\mathbf{X}, h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (h(\\mathbf{x}^{(i)}) - y^{(i)})^2}$$\n",
    "\n",
    "If there are many outliers, we might prefer the **Mean Absolute Error (MAE)**:\n",
    "\n",
    "$$MAE(\\mathbf{X}, h) = \\frac{1}{m} \\sum_{i=1}^{m} |h(\\mathbf{x}^{(i)}) - y^{(i)}|$$\n",
    "\n",
    "For this project, we will use the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the Data\n",
    "\n",
    "Now, let's set up the workspace, download the data, and create the functions to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup for downloading the data\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \"\"\"Creates the housing_path directory, downloads and extracts the .tgz file.\"\"\"\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    \"\"\"Loads the housing.csv file from the specified path using pandas.\"\"\"\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the data\n",
    "fetch_housing_data()\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Take a Quick Look at the Data Structure\n",
    "\n",
    "After loading the data, the first step is to inspect it to understand its structure, features, and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 5 rows\n",
    "print(\"Top 5 rows:\")\n",
    "display(housing.head())\n",
    "\n",
    "# Get a quick description of the data (total rows, attributes, non-null values)\n",
    "print(\"\\nData Info:\")\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßë‚Äçüè´ Theoretical Deep-Dive: Data Inspection\n",
    "\n",
    "From the `.info()` output, we can observe:\n",
    "* There are 20,640 instances in the dataset.\n",
    "* The `total_bedrooms` attribute has only 20,433 non-null values, meaning 207 districts are missing this feature. We will need to handle these missing values.\n",
    "* All attributes are numerical, *except* `ocean_proximity`, which is an `object` type. This suggests it is a text-based categorical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out what categories exist in 'ocean_proximity' and how many districts belong to each\n",
    "print(\"\\nOcean Proximity Categories:\")\n",
    "display(housing[\"ocean_proximity\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the numerical attributes\n",
    "print(\"\\nNumerical Attribute Summary:\")\n",
    "display(housing.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram for each numerical attribute to get a feel for the data distribution\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßë‚Äçüè´ Theoretical Deep-Dive: Observations from Histograms\n",
    "\n",
    "1.  **Capped Values:** The `housing_median_age` and `median_house_value` attributes are capped. The `median_house_value` (our target) being capped at $500,000 is a problem, as our model may learn that prices never go beyond that limit. We might need to remove these districts.\n",
    "2.  **Different Scales:** The attributes have very different scales. This will be an issue for many ML algorithms, and we will need to address it with **feature scaling**.\n",
    "3.  **Tail-Heavy Distributions:** Many histograms are *tail-heavy* (they extend much farther to the right). This can make it harder for algorithms to detect patterns. We may need to transform these attributes later (e.g., by computing their logarithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create a Test Set\n",
    "\n",
    "This is one of the most important steps. We must create a test set and set it aside *before* we do any more data exploration to avoid **data snooping bias**. This bias occurs when your brain (an amazing pattern-detection system) spots patterns in the test set, which can lead you to select a model that is biased towards the test set. Your model's generalization error will be too optimistic.\n",
    "\n",
    "#### üßë‚Äçüè´ Theoretical Deep-Dive: Random vs. Stratified Sampling\n",
    "\n",
    "If we use purely random sampling, we risk introducing **sampling bias**. For example, if our dataset is 51% female and 49% male, a purely random sample of 1,000 people might end up with 40% females, which is not representative.\n",
    "\n",
    "To avoid this, we use **stratified sampling**. The population is divided into homogeneous subgroups called **strata**, and the right number of instances are sampled from each stratum to ensure the test set is representative of the overall population.\n",
    "\n",
    "In our case, the `median_income` is a very important attribute. We need to ensure the test set is representative of the various income categories. We will create an `income_cat` attribute to perform stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Scikit-Learn's train_test_split for a simple random split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 'random_state' ensures the split is reproducible\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Simple random split: {len(train_set)} train + {len(test_set)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an income category attribute with 5 categories\n",
    "# pd.cut() creates 5 bins based on the specified ranges\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "housing[\"income_cat\"].hist()\n",
    "plt.title(\"Income Category Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform stratified sampling based on the income category\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# Check if the stratified split worked as expected\n",
    "print(\"Income category proportions in test set (stratified):\")\n",
    "display(strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set))\n",
    "\n",
    "# Now we can drop the 'income_cat' column as it's no longer needed\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover and Visualize the Data to Gain Insights\n",
    "\n",
    "Now that we have a clean training set (and our test set is safely locked away), we can explore the data. We'll work on a copy to avoid harming the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Visualizing Geographical Data\n",
    "\n",
    "Since we have latitude and longitude, a scatterplot is a good way to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatterplot\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "plt.title(\"California District Locations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set alpha=0.1 to see density\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "plt.title(\"California Population Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with population (s) and median house value (c)\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"California Housing Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image clearly shows that housing prices are very much related to location (e.g., close to the ocean and in high-density areas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Looking for Correlations\n",
    "\n",
    "Since the dataset is not too large, we can compute the **standard correlation coefficient** (Pearson's r) between every pair of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "\n",
    "# Check how much each attribute correlates with the median house value\n",
    "print(\"Correlation with Median House Value:\")\n",
    "display(corr_matrix[\"median_house_value\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `median_income` shows a strong positive correlation (0.68) with the `median_house_value`. This means that as median income goes up, median house prices tend to go up.\n",
    "\n",
    "We can also use the `scatter_matrix` function from pandas to plot every numerical attribute against every other numerical attribute. We will focus on the most promising attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zoom in on the median_income vs median_house_value plot\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1)\n",
    "plt.title(\"Median Income vs. Median House Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot reveals a few things:\n",
    "1.  The correlation is very strong.\n",
    "2.  The price cap at $500,000 is clearly visible as a horizontal line.\n",
    "3.  There are other, less obvious straight lines (e.g., at $450k, $350k). We may want to remove those districts to prevent the model from learning these quirks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Experimenting with Attribute Combinations\n",
    "\n",
    "The last step of data exploration is to try combining attributes to create new, potentially more useful features (feature engineering). For example, `total_rooms` is not very useful if we don't know the number of households. `rooms_per_household` is more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "# Let's look at the correlation matrix again\n",
    "corr_matrix = housing.corr()\n",
    "print(\"Correlation with new features:\")\n",
    "display(corr_matrix[\"median_house_value\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The new `bedrooms_per_room` attribute is much more correlated with the median house value than `total_rooms` or `total_bedrooms` (it's a negative correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "It's time to write functions to prepare the data. We do this for several reasons:\n",
    "* To easily reproduce transformations on any dataset (e.g., the test set).\n",
    "* To build a library of reusable transformation functions.\n",
    "* To use these functions in our live system to transform new data.\n",
    "\n",
    "First, let's revert to a clean training set and separate the predictors (`X`) from the labels (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Data Cleaning\n",
    "\n",
    "We need to handle the missing `total_bedrooms` features. We have three options:\n",
    "1.  Get rid of the corresponding districts.\n",
    "2.  Get rid of the whole attribute.\n",
    "3.  Set the values to some value (zero, the mean, the median, etc.).\n",
    "\n",
    "We will use option 3. Scikit-Learn provides a handy class to take care of this: `SimpleImputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# The median can only be computed on numerical attributes, so we drop 'ocean_proximity'\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# The imputer stores the median of each attribute in its 'statistics_' variable\n",
    "print(\"Imputer medians:\", imputer.statistics_)\n",
    "\n",
    "# Now we can use the \"trained\" imputer to transform the training set\n",
    "X = imputer.transform(housing_num)\n",
    "\n",
    "# The result is a plain NumPy array. Let's put it back into a DataFrame.\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Handling Text and Categorical Attributes\n",
    "\n",
    "We need to convert the `ocean_proximity` text attribute to numbers. Since it's a categorical attribute, we cannot use an `OrdinalEncoder` (which maps `\"<1H OCEAN\"` to 0, `\"INLAND\"` to 1, etc.) because ML algorithms will assume that two nearby values are more similar than two distant values (e.g., it would think 0 and 1 are more similar than 0 and 4).\n",
    "\n",
    "The solution is **one-hot encoding**. This creates one binary attribute per category. A \"<1H OCEAN\" district would have a `1` in the \"<1H OCEAN\" column and `0`s in all other columns. We can use Scikit-Learn's `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]] # Needs to be a 2D array\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "\n",
    "# The output is a SciPy sparse matrix (to save memory)\n",
    "print(\"One-hot encoded data (sparse matrix):\")\n",
    "print(housing_cat_1hot.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Custom Transformers\n",
    "\n",
    "We need to create a custom transformer to add the combined attributes we experimented with earlier. We create a class that implements Scikit-Learn's required `fit()` and `transform()` methods. We use `BaseEstimator` and `TransformerMixin` for `get_params()` and `set_params()` methods, which are useful for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Column indices for total_rooms, total_bedrooms, population, households\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "print(\"Data with extra attributes:\")\n",
    "print(housing_extra_attribs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Feature Scaling and Transformation Pipelines\n",
    "\n",
    "**Feature Scaling** is one of the most important transformations. ML algorithms don't perform well when numerical attributes have very different scales. There are two common methods:\n",
    "* **Min-max scaling (Normalization):** Values are rescaled to range from 0 to 1. Use `MinMaxScaler`.\n",
    "* **Standardization:** Values are rescaled to have a mean of 0 and a variance of 1. It is less affected by outliers. Use `StandardScaler`.\n",
    "\n",
    "A **Pipeline** is a Scikit-Learn class that helps chain multiple transformation steps in the correct order. We will also use a `ColumnTransformer` to apply different transformations to different columns (e.g., `SimpleImputer` and `StandardScaler` for numbers, `OneHotEncoder` for categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create a pipeline for numerical attributes\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# Get list of numerical and categorical attribute names\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Create a single pipeline that handles all columns\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "# Run the full pipeline\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "print(\"Shape of prepared data:\", housing_prepared.shape)\n",
    "print(\"One row of prepared data:\", housing_prepared[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select and Train a Model\n",
    "\n",
    "We are finally ready to train some models! We'll start with a few simple ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Training a Linear Regression Model\n",
    "\n",
    "Let's first train a simple Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Let's try it on a few instances from the training set\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(\"Linear Regression RMSE:\", lin_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RMSE of $68,628 is not great. This is a clear sign that the model is **underfitting** the training data. This means the features don't provide enough information, or the model is not powerful enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Training a Decision Tree Regressor\n",
    "\n",
    "Let's try a more powerful model, a `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(\"Decision Tree RMSE:\", tree_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RMSE of 0.0 means the model **overfit** the data perfectly. This is not a useful model. We can't touch the test set yet, so we need a way to evaluate the model on the training set. The solution is **K-fold cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Better Evaluation Using Cross-Validation\n",
    "\n",
    "**K-fold cross-validation** randomly splits the training set into K distinct subsets (e.g., 10), then it trains and evaluates the model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array of 10 evaluation scores.\n",
    "\n",
    "**Note:** Scikit-Learn's cross-validation features expect a *utility function* (greater is better) rather than a *cost function* (lower is better), so it computes the *negative* MSE. We must flip the sign of the scores before calculating the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the Decision Tree model\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "print(\"--- Decision Tree Cross-Validation ---\")\n",
    "display_scores(tree_rmse_scores)\n",
    "\n",
    "# Now let's evaluate the Linear Regression model for comparison\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "\n",
    "print(\"\\n--- Linear Regression Cross-Validation ---\")\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree model is overfitting so badly that it performs worse (mean RMSE ~71,407) than the Linear Regression model (mean RMSE ~69,052)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Training a Random Forest Regressor\n",
    "\n",
    "Let's try one last model: `RandomForestRegressor`. This is an **Ensemble Learning** method that works by training many Decision Trees on random subsets of the features, then averaging their predictions. Ensembles often perform better than individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "# Note: The book trains the model first, then does cross-validation. We'll just do cross-validation.\n",
    "\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "\n",
    "print(\"--- Random Forest Cross-Validation ---\")\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! The mean RMSE is around 50,000. This is our most promising model so far. Note that the training score would still be much lower than the validation score, meaning the model is still overfitting, but it's a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-Tune Your Model\n",
    "\n",
    "We now have a shortlist of promising models. The next step is to fine-tune them. We will focus on the `RandomForestRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Grid Search\n",
    "\n",
    "One way to fine-tune is to fiddle with hyperparameters manually, but this is tedious. A better way is to use Scikit-Learn's `GridSearchCV`. You tell it which hyperparameters and values to try, and it uses cross-validation to evaluate all possible combinations and find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# This param_grid tells GridSearchCV to evaluate 3 x 4 = 12 combinations from the first dict,\n",
    "# then 2 x 3 = 6 combinations from the second dict (with bootstrap=False)\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best combination of parameters\n",
    "print(\"Best parameters found by Grid Search:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator (model)\n",
    "print(\"\\nBest estimator:\")\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the evaluation scores\n",
    "cvres = grid_search.cv_results_\n",
    "print(\"\\nScores for each combination:\")\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model has an RMSE of 49,682, which is slightly better than the 50,182 we got earlier with the default hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Randomized Search\n",
    "\n",
    "When the hyperparameter search space is large, `GridSearchCV` is too slow. `RandomizedSearchCV` is a better option. It evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Analyze the Best Models and Their Errors\n",
    "\n",
    "We can gain insights by inspecting the best models. For example, `RandomForestRegressor` can indicate the relative importance of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "# Get the attribute names (a bit tricky with ColumnTransformer)\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information, we might want to try dropping some of the less useful features (e.g., the `ISLAND` ocean_proximity category)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Your System on the Test Set\n",
    "\n",
    "After tweaking models for a while, we finally have a model that performs well. Now is the time to evaluate the final model on the test set.\n",
    "\n",
    "**Important:** We must call `transform()` on the test set, *not* `fit_transform()`. We do not want to fit the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "print(\"Final RMSE on Test Set:\", final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launch, Monitor, and Maintain Your System\n",
    "\n",
    "This is the final step, which is just as important as the previous ones.\n",
    "\n",
    "* **Launch:** Get your solution ready for production. This involves polishing the code, writing documentation and tests, and deploying the model (e.g., as a web service).\n",
    "* **Monitor:** You need to write monitoring code to check your system's live performance. Models \"rot\" over time as data evolves, so you must monitor for performance degradation.\n",
    "* **Maintain:** You need to have processes in place to retrain your model on fresh data, evaluate it, and deploy it (automating as much as possible)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
