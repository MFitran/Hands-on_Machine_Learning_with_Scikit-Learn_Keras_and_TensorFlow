{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
    "\n",
    "This notebook contains the code reproductions and theoretical explanations for Chapter 13 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter focuses on building efficient, scalable, and production-ready data pipelines using TensorFlow.\n",
    "\n",
    "Key topics covered include:\n",
    "\n",
    "* **The Data API (`tf.data`):** We learn how to build high-performance data pipelines by creating a `Dataset` object and chaining transformations. This includes reading from files, shuffling, batching, mapping (preprocessing), and prefetching.\n",
    "* **The TFRecord Format:** This is TensorFlow's preferred binary format for storing large datasets. We learn how to create, write to, read from, and parse TFRecord files, including those containing `tf.train.Example` protocol buffers.\n",
    "* **Preprocessing Layers:** We explore how to preprocess features *within* the model itself. This ensures that the same preprocessing logic is applied during training and inference, preventing training-serving skew. This includes:\n",
    "    * Standardization and normalization.\n",
    "    * Encoding categorical features (using one-hot encoding or embeddings).\n",
    "    * Encoding text features (using tokenization and bag-of-words or embeddings).\n",
    "* **TF Transform:** A library from TensorFlow Extended (TFX) that allows you to define a single preprocessing function which can be run efficiently in batch (e.g., on Apache Beam) before training, and also exported for on-the-fly preprocessing in a deployed model.\n",
    "* **TensorFlow Datasets (TFDS):** A high-level library that provides a simple way to download and use many common public datasets, already in the `tf.data.Dataset` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Common setup for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data API\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The Data API revolves around the `tf.data.Dataset` object, which represents a sequence of data items. This API is designed to handle large datasets that may not fit in memory. It allows you to build a pipeline by chaining transformations.\n",
    "\n",
    "* **Source:** You first create a dataset from a source (e.g., from tensors in memory, from files on disk).\n",
    "* **Transformations:** You then apply a series of transformations to this dataset. Each transformation method (like `.batch()` or `.map()`) returns a *new* dataset object. This enables method chaining and lazy evaluation.\n",
    "* **Iteration:** Finally, you iterate over the dataset (e.g., in a `for` loop or by passing it to a Keras model's `fit()` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from a tensor in RAM\n",
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset contains 10 items: tensors 0, 1, 2, ..., 9\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Transformations\n",
    "\n",
    "Each method returns a new dataset, so you can chain methods together to build a processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.repeat(3).batch(7, drop_remainder=False)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common transformation methods:\n",
    "* `map()`: Applies a custom function to each item. This is used for preprocessing.\n",
    "* `apply()`: Applies a transformation to the dataset as a whole.\n",
    "* `filter()`: Filters the dataset, keeping only items that pass a test.\n",
    "* `take()`: Creates a new dataset with only the first *n* items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset dataset for new examples\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "\n",
    "# map()\n",
    "dataset_map = dataset.map(lambda x: x * 2) \n",
    "print(\"Mapped:\", list(dataset_map.as_numpy_iterator()))\n",
    "\n",
    "# filter()\n",
    "dataset_filter = dataset.filter(lambda x: x < 5)\n",
    "print(\"Filtered:\", list(dataset_filter.as_numpy_iterator()))\n",
    "\n",
    "# take()\n",
    "dataset_take = dataset.take(3)\n",
    "print(\"Taken:\", list(dataset_take.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Data\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "Gradient Descent works best when the training instances are independent and identically distributed (IID). A `shuffle()` method helps achieve this by creating a **shuffle buffer**. \n",
    "\n",
    "It works as follows:\n",
    "1.  It fills a buffer (of `buffer_size`) with the first items from the source dataset.\n",
    "2.  When an item is requested, it pulls one out *randomly* from the buffer.\n",
    "3.  It then replaces the pulled item with the *next* item from the source dataset.\n",
    "\n",
    "For this to be effective, the `buffer_size` must be large enough. A common practice is to set it to the size of the training set, but if the dataset is too large for RAM, you can use a smaller buffer (e.g., 10,000). It's also crucial to shuffle the source files themselves first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaving lines from multiple files\n",
    "\n",
    "For very large datasets, a common pattern is to split the data into multiple files. This allows you to shuffle them at the file level. The Data API can read from these files in parallel and interleave their lines. This further improves shuffling and performance.\n",
    "\n",
    "1.  `Dataset.list_files()`: Creates a dataset of filenames (shuffles them by default).\n",
    "2.  `dataset.interleave()`: Reads from multiple files at once and interleaves their records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to create the CSV files for this example\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([str(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "# Prepare and save the data\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)\n",
    "\n",
    "print(\"Filepaths created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a dataset of filepaths\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "# 2. Interleave the lines from 5 files at a time\n",
    "#    We also skip the header row (skip(1)) from each file\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)\n",
    "\n",
    "# Let's check the result\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "\n",
    "The data is loaded as byte strings. We need a function to parse and scale it. We can use `tf.io.decode_csv` to parse the lines and `tf.stack` to re-form tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8 # housing.data.shape[1]\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / (X_std + keras.backend.epsilon()), y\n",
    "\n",
    "# Test the preprocess function\n",
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "We can build a final, efficient pipeline by chaining all these steps. The key to performance is to make sure operations run in parallel.\n",
    "\n",
    "* `interleave(..., num_parallel_calls=...)`: Reads multiple files in parallel.\n",
    "* `map(..., num_parallel_calls=...)`: Preprocesses multiple items in parallel.\n",
    "* `shuffle()`: Shuffles the items.\n",
    "* `batch()`: Groups items into batches.\n",
    "* `prefetch(1)`: This is a crucial performance optimization. It creates a dataset that will always prepare one batch ahead of time. While the model is training on batch N, the CPU is already preparing batch N+1. This prevents the GPU from "starving" for data.\n",
    "* `cache()`: If your dataset is small enough to fit in RAM, you can add `.cache()` after preprocessing (but before shuffling/batching) to store the preprocessed data in memory, avoiding repeated work every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final dataset objects for training, validation, and testing\n",
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Dataset with `tf.keras`\n",
    "\n",
    "You can now pass these `Dataset` objects directly to the `fit()`, `evaluate()`, and `predict()` methods of a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "# We pass the datasets directly to fit()\n",
    "# Since train_set repeats indefinitely, we must set steps_per_epoch.\n",
    "model.fit(train_set, epochs=10, \n",
    "          validation_data=valid_set,\n",
    "          steps_per_epoch=len(X_train) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also pass the dataset to evaluate()\n",
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And to predict()\n",
    "# Note: new_set should not contain labels\n",
    "new_set = test_set.take(3).map(lambda X, y: X) \n",
    "model.predict(new_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The TFRecord format is TensorFlow's preferred format for storing large datasets. It's a simple binary format composed of a sequence of binary records. Each record contains its length, a CRC checksum, the actual data, and a final CRC checksum for the data.\n",
    "\n",
    "This format is efficient to read and works very well with the Data API. It's particularly useful for data that isn't line-based, like images or audio.\n",
    "\n",
    "The records themselves often contain serialized **protocol buffers** (protobufs). A protobuf is a portable, extensible, and efficient binary format. The standard protobuf used in TFRecords is the `tf.train.Example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a TFRecord file\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")\n",
    "\n",
    "# Read from a TFRecord file\n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also compress TFRecord files with GZIP\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")\n",
    "\n",
    "# And read them by specifying the compression type\n",
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
    "                                compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Protobufs\n",
    "\n",
    "The `tf.train.Example` protobuf is a flexible message type that represents one instance. It's a dictionary of named features, where each feature can be a list of byte strings, a list of floats, or a list of integers.\n",
    "\n",
    "Here is its definition (simplified):\n",
    "```\n",
    "message BytesList { repeated bytes value = 1; }\n",
    "message FloatList { repeated float value = 1; }\n",
    "message Int64List { repeated int64 value = 1; }\n",
    "\n",
    "message Feature {\n",
    "  oneof kind {\n",
    "    BytesList bytes_list = 1;\n",
    "    FloatList float_list = 2;\n",
    "    Int64List int64_list = 3;\n",
    "  }\n",
    "};\n",
    "\n",
    "message Features { map<string, Feature> feature = 1; };\n",
    "message Example { Features features = 1; };\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.train.Example protobuf\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        })\n",
    ")\n",
    "\n",
    "# Serialize it and write to a TFRecord file\n",
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Parsing Examples\n",
    "\n",
    "To read and parse the `Example` protobufs, you use `tf.io.parse_single_example()`. This is a TF operation, so it can be part of your `tf.data` pipeline. It requires a dictionary that describes the features you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature description\n",
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string), # VarLenFeature for variable length\n",
    "}\n",
    "\n",
    "# Create the dataset and parse the examples\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"])\n",
    "for serialized_example in dataset:\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
    "                                                  feature_description)\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VarLenFeatures are parsed as SparseTensors\n",
    "print(parsed_example[\"emails\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can convert a sparse tensor to a dense tensor\n",
    "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on `SequenceExample`:** For more complex data, like lists of lists (e.g., a document as a list of sentences, where each sentence is a list of words), you can use the `SequenceExample` protobuf. It's parsed using `tf.io.parse_single_sequence_example()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "It's crucial to preprocess your features (e.g., scale numerical features, encode categorical features) before feeding them to your neural network. Instead of doing this *before* training, you can do it *inside* the model by creating preprocessing layers.\n",
    "\n",
    "**Advantages:**\n",
    "1.  **Simplicity:** The model expects raw data, simplifying your data pipeline.\n",
    "2.  **Prevents Training-Serving Skew:** By bundling the preprocessing inside the model, you guarantee that the *exact same* preprocessing is applied to new data during inference as was applied to the training data. This is a common source of bugs.\n",
    "3.  **Portability:** The saved model contains the preprocessing, making it easy to deploy to mobile, web, or servers.\n",
    "\n",
    "You can create custom layers or use the new standard Keras preprocessing layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating a custom Standardization layer\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        # Compute the mean and std dev from a data sample\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer = Standardization()\n",
    "# We 'adapt' the layer by showing it a sample of the training data\n",
    "std_layer.adapt(X_train)\n",
    "\n",
    "# Now we can include this layer in a model\n",
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2, \n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "# Note: We fit on scaled data just for this example. \n",
    "# In a real project, you would fit on the *unscaled* X_train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using One-Hot Vectors\n",
    "\n",
    "To encode categorical strings, we can use a lookup table to convert them to integer IDs, then use `tf.one_hot` to encode those IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the vocabulary\n",
    "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "\n",
    "# 2. Create the lookup table\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2 # For out-of-vocabulary (oov) categories\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
    "\n",
    "# 3. Test the table\n",
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "print(\"Indices:\", cat_indices)\n",
    "\n",
    "# 4. One-hot encode the indices\n",
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "print(\"One-hot:\", cat_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using Embeddings\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "One-hot encoding is inefficient for large vocabularies (e.g., 50,000+ words). A more efficient approach is to use **embeddings**. \n",
    "\n",
    "An embedding is a **trainable**, dense vector that represents a category. For example, a word might be represented by a 128-dimensional vector instead of a 50,000-dimensional one-hot vector.\n",
    "\n",
    "Initially, these vectors are random. During training, the network learns to place similar categories close to each other in the *embedding space*. This is a form of **representation learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding layer does two things:\n",
    "# 1. It stores the embedding matrix (initialized randomly).\n",
    "# 2. When given category indices, it looks up the corresponding embedding vectors.\n",
    "\n",
    "embedding_dim = 2\n",
    "embedding = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets,\n",
    "                                   output_dim=embedding_dim)\n",
    "\n",
    "print(embedding(cat_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Keras model using both numerical and embedding inputs\n",
    "# This requires the Functional API\n",
    "\n",
    "regular_inputs = keras.layers.Input(shape=[8]) # Assumes 8 numerical features\n",
    "categories = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "# Preprocessing part\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets, \n",
    "                                   output_dim=embedding_dim)(cat_indices)\n",
    "\n",
    "# Concatenate numerical features with categorical embeddings\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "\n",
    "# Regular part of the model\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories],\n",
    "                             outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Preprocessing Layers\n",
    "\n",
    "TensorFlow is standardizing a set of preprocessing layers in `keras.layers` that work like our custom `Standardization` layer: you `adapt()` them to a data sample, then include them in your model. \n",
    "\n",
    "Examples will include:\n",
    "* `keras.layers.Normalization`\n",
    "* `keras.layers.TextVectorization` (for tokenizing, indexing, and bag-of-words)\n",
    "* `keras.layers.Discretization` (for binning continuous data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Transform\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "A major challenge in production is **training-serving skew**. This happens when the preprocessing you do in your training pipeline (e.g., in Apache Beam) is slightly different from the preprocessing you do in your live application (e.g., in Java or JavaScript).\n",
    "\n",
    "`TF Transform` (part of TFX) solves this. You define your preprocessing function *once* using `tft` ops. \n",
    "1.  This function is run in batch over your entire training set using a tool like Apache Beam. This computes all necessary statistics (like mean, std, vocabulary) and preprocesses the data for fast training.\n",
    "2.  It also generates an equivalent **TensorFlow graph** (a `TF Function`) that includes the *computed statistics* (e.g., the mean and std are now constants in the graph).\n",
    "\n",
    "You can then include this graph as the first layer of your model, guaranteeing that the exact same preprocessing logic is applied everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow_transform as tft\n",
    "\n",
    "    def preprocess(inputs):  # inputs = a batch of input features\n",
    "        median_age = inputs[\"housing_median_age\"]\n",
    "        ocean_proximity = inputs[\"ocean_proximity\"]\n",
    "        standardized_age = tft.scale_to_z_score(median_age)\n",
    "        ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "        return {\n",
    "            \"standardized_median_age\": standardized_age,\n",
    "            \"ocean_proximity_id\": ocean_proximity_id\n",
    "        }\n",
    "except ImportError:\n",
    "    print(\"TF Transform is not installed. Skipping this code block.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TensorFlow Datasets (TFDS) Project\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "TFDS is a library that provides a simple way to download and access many common datasets (e.g., MNIST, ImageNet, `imdb_reviews`). It downloads the data, caches it, and returns `tf.data.Dataset` objects, ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load MNIST dataset\n",
    "dataset = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset items are dictionaries\n",
    "for item in mnist_train.take(1):\n",
    "    print(item[\"image\"].shape)\n",
    "    print(item[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can load it directly as a (features, label) tuple for Keras\n",
    "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
    "mnist_train = dataset[\"train\"].prefetch(1)\n",
    "\n",
    "# This dataset can be passed directly to fit()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Lambda(lambda x: x / 255.),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.fit(mnist_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "See Appendix A in the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
