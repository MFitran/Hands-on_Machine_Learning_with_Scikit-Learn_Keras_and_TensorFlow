{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 – Loading and Preprocessing Data with TensorFlow\n",
    "\n",
    "This notebook contains all the code samples and solutions to the exercises in chapter 13 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition* (O'Reilly). *Note: all code examples are based on the author's original GitHub repository.* \n",
    "\n",
    "**Assignment Instructions:**\n",
    "Per the assignment guidelines, this notebook reproduces the code from Chapter 13. It also includes theoretical explanations and summaries for each concept, as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter focuses on building efficient and scalable data pipelines, a critical part of any real-world machine learning project, especially when datasets are too large to fit in memory.\n",
    "\n",
    "Key concepts covered include:\n",
    "\n",
    "1.  **The Data API (`tf.data`)**: This is TensorFlow's high-level API for building complex and efficient input pipelines. We explore its core component, `tf.data.Dataset`, and learn how to chain transformations to:\n",
    "    * Read from files (like CSVs or text files).\n",
    "    * Interleave data from multiple files.\n",
    "    * Transform data (using `map()`).\n",
    "    * Shuffle the data (using `shuffle()`).\n",
    "    * Create batches (using `batch()`).\n",
    "    * Optimize the pipeline for speed (using `prefetch()` and `cache()`).\n",
    "\n",
    "2.  **The TFRecord Format**: We learn about TensorFlow's preferred format for storing data. It's a flexible and efficient binary format that's ideal for large datasets. We see how to create, save, load, and parse `TFRecord` files, which often contain **Protocol Buffers** like `tf.train.Example`.\n",
    "\n",
    "3.  **Preprocessing Features**: We explore how to prepare data for a neural network directly within the model or data pipeline, including:\n",
    "    * **One-Hot Encoding**: For categorical features with a small number of categories.\n",
    "    * **Embeddings**: A much more efficient and powerful way to represent categorical features with many categories (like words). We build and use `keras.layers.Embedding`.\n",
    "    * **Keras Preprocessing Layers**: We discuss the new Keras layers (like `Normalization` and `TextVectorization`) that can be included directly in a model and use an `adapt()` method to learn from the data.\n",
    "\n",
    "4.  **The TensorFlow Ecosystem**:\n",
    "    * **TF Transform (TFX)**: A tool to define preprocessing operations once, run them in batch on the full training set (using Apache Beam), and also export them as a layer to be embedded in the model, preventing training/serving skew.\n",
    "    * **TensorFlow Datasets (TFDS)**: A high-level library to easily download and use hundreds of common datasets, which are returned as `tf.data.Dataset` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 3.7 or later is required for the latest versions of Scikit-Learn), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"data\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data API\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    [cite_start]"The core of the Data API is the `tf.data.Dataset` object, which represents a sequence of data items[cite: 1314]. It's incredibly efficient because it can read data from disk (or other sources) on the fly, so it works perfectly for datasets that are too large to fit in RAM. You just create a dataset object and tell it where to get the data and how to transform it.\n",
    "\n",
    "You can easily create a dataset from a tensor in RAM using `from_tensor_slices()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "print(dataset)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Chaining Transformations\n",
    "\n",
    "Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a *new* dataset, so you can chain transformations together to create a pipeline.\n",
    "\n",
    "* `repeat()`: Repeats the items in the dataset. Without an argument, it repeats forever, which is useful during training with `model.fit()`.\n",
    "* `batch()`: Groups the items into batches of a given size. `drop_remainder=True` ensures all batches have the exact same size.\n",
    "* `map()`: Applies a custom function to each item (e.g., for preprocessing). You can set `num_parallel_calls` to speed this up.\n",
    "* `filter()`: Filters the dataset, keeping only items that pass a test.\n",
    "* `take()`: Takes the first *n* items from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the map() method to transform items\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Shuffling the Data\n",
    "\n",
    "Gradient Descent works best when training instances are independent and identically distributed (IID). A simple way to achieve this is to shuffle the instances. The `shuffle()` method creates a new dataset that fills a buffer (of size `buffer_size`) with the first items. Then, when it's asked for an item, it pulls one out randomly from the buffer and replaces it with a new one from the source dataset.\n",
    "\n",
    "**Important:** To be effective, the `buffer_size` must be large enough. A good rule of thumb is to make it at least as large as the training set, if possible (without exceeding RAM). It's also crucial to shuffle *before* batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "# We set a seed for reproducibility\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Reproduction: Interleaving lines from multiple files\n",
    "\n",
    "For large datasets, it's common to split the data into multiple files. The Data API can read from multiple files in parallel.\n",
    "\n",
    "1.  We start by creating a `Dataset` of file paths using `tf.data.Dataset.list_files()`.\n",
    "2.  We then use the `interleave()` method. This method will create a dataset that pulls file paths from the `filepath_dataset` (e.g., 5 at a time, set by `cycle_length`) and, for each one, calls a function to create a new dataset (in this case, `TextLineDataset`). It then reads one line at a time from each `TextLineDataset`, cycling through them. This interleaves the records from multiple files, which is great for shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create some dummy CSV files first for the example\n",
    "# (This is taken from the book's notebook setup)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([str(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "# Save the data as CSVs\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset of file paths\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "# Interleave the files\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # skip header row\n",
    "    cycle_length=n_readers\n",
    ")\n",
    "\n",
    "# Check the first few lines\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Preprocessing the Data\n",
    "\n",
    "The data is loaded as byte strings. We need to parse and scale it. We can write a `preprocess` function and apply it to our dataset using the `map()` method.\n",
    "\n",
    "The `tf.io.decode_csv()` function is perfect for this. It takes a line and a list of `record_defaults`, which tells TensorFlow the type and default value for each column. We then `stack` the feature tensors into a single tensor, scale them, and return the `(features, label)` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8 # Number of features in California housing\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)] # 8 features, 1 label\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / (X_std + 1e-10), y # Scale features (add epsilon to avoid /0)\n",
    "\n",
    "# Test the preprocessing function\n",
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Putting It All Together & Prefetching\n",
    "\n",
    "Let's create a helper function that builds a full pipeline for us: it lists files, interleaves, preprocesses, shuffles, batches, and repeats.\n",
    "\n",
    "The most important optimization is adding `prefetch(1)` at the end. This creates a dataset that will do its best to always be **one batch ahead**. While the training algorithm (on the GPU) is working on one batch, the dataset (on the CPU) will already be working in parallel on getting the next batch ready. This dramatically improves performance by overlapping CPU and GPU work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=None, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "# Now we can create the train, valid, and test datasets\n",
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Dataset with tf.keras\n",
    "\n",
    "We can now simply pass these `Dataset` objects to the `fit()`, `evaluate()`, and `predict()` methods of a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "model.fit(train_set, epochs=10, validation_data=valid_set, steps_per_epoch=len(X_train) // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make predictions, we can just use the test set\n",
    "# Keras will ignore the labels\n",
    "new_set = test_set.take(3) \n",
    "predictions = model.predict(new_set)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "The `TFRecord` format is TensorFlow's preferred format for storing large datasets efficiently. It is a simple binary format that contains a sequence of binary records. These records can be compressed (e.g., using `\"GZIP\"`).\n",
    "\n",
    "TFRecord files usually contain serialized **protocol buffers** (protobufs). A protobuf is a portable, efficient, and extensible binary format.\n",
    "\n",
    "The main protobuf used in `TFRecord` files is `tf.train.Example`. An `Example` is a flexible message that represents one instance. It contains a dictionary of `Features`, where each `Feature` can be a:\n",
    "* `BytesList` (e.g., for strings or raw image data)\n",
    "* `FloatList`\n",
    "* `Int64List`\n",
    "\n",
    "We can create a `tf.train.Example` protobuf, serialize it using `SerializeToString()`, and write it to a `.tfrecord` file using `tf.io.TFRecordWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Writing to a TFRecord file\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    person_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "                \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "                \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Loading and Parsing TFRecords\n",
    "\n",
    "To read a `TFRecord` file, we use `tf.data.TFRecordDataset`. This returns a dataset of *serialized* protobuf strings.\n",
    "\n",
    "We must then parse these strings. We use `tf.io.parse_single_example()` and provide a `feature_description` dictionary that tells TensorFlow the shape, type, and default value for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Reading and parsing a TFRecord file\n",
    "\n",
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string), # Use VarLenFeature for lists\n",
    "}\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"])\n",
    "for serialized_example in dataset:\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
    "                                                  feature_description)\n",
    "    print(parsed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VarLenFeatures are parsed as SparseTensors\n",
    "print(\"Emails (Sparse):\", parsed_example[\"emails\"])\n",
    "# You can convert them to dense tensors\n",
    "print(\"Emails (Dense):\", tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features\n",
    "\n",
    "### Theoretical Explanation: Keras Preprocessing Layers\n",
    "\n",
    "Instead of preprocessing data in the `tf.data` pipeline, we can add preprocessing layers directly into our model. This has a huge advantage: the preprocessing logic is bundled with the model, which simplifies deployment. When you save this model, it includes the preprocessing layers, so you can feed it raw data during inference.\n",
    "\n",
    "Keras is introducing a set of preprocessing layers (e.g., `Normalization`, `TextVectorization`) that follow a simple pattern:\n",
    "1.  Create the layer.\n",
    "2.  Call its `adapt()` method, feeding it a data sample (e.g., the training set). This layer will learn the necessary statistics (like mean, std, or vocabulary) from the data.\n",
    "3.  Use the layer normally in your model.\n",
    "\n",
    "Since these layers aren't all finalized, we can create our own custom standardization layer to see the principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Custom Standardization Layer\n",
    "\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        # Learn the mean and std from a data sample\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        # Apply the transformation\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "\n",
    "# 1. Create the layer\n",
    "std_layer = Standardization()\n",
    "\n",
    "# 2. Adapt it (using a data sample from the training set)\n",
    "data_sample = X_train[:1000]\n",
    "std_layer.adapt(data_sample)\n",
    "\n",
    "# 3. Use it in a model\n",
    "model = keras.models.Sequential([\n",
    "    std_layer, # The first layer is now the preprocessing layer\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "# Now we can fit the model on the *unscaled* data!\n",
    "# model.fit(X_train_unscaled, y_train, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Encoding Categorical Features\n",
    "\n",
    "**1. One-Hot Vectors**\n",
    "If you have a categorical feature with few categories (e.t., <10), one-hot encoding is a good choice. We first need a lookup table to map category strings to integer indices. `tf.lookup.StaticVocabularyTable` is perfect for this. We can also specify `num_oov_buckets` (out-of-vocabulary) to handle unknown categories. Once we have the indices, `tf.one_hot()` can create the vectors.\n",
    "\n",
    "**2. Embeddings**\n",
    "When the number of categories is large (e.g., zip codes, words, users), one-hot vectors become huge and inefficient. A much better solution is to use an **embedding**.\n",
    "\n",
    "An embedding is a **trainable, dense vector** that represents a category. For example, a word might be represented by a 128-dimensional vector. Initially, these vectors are random. During training, the network learns to place similar categories (e.g., synonyms like \"awesome\" and \"amazing\") close to each other in the embedding space. This is a form of **representation learning**.\n",
    "\n",
    "In Keras, this is as simple as adding a `keras.layers.Embedding` layer. This layer takes integer indices as input and outputs the corresponding embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Reproduction: Embedding Layer\n",
    "\n",
    "# Assume we have a vocabulary of 10,000 words + 1000 OOV buckets\n",
    "vocab_size = 10000\n",
    "num_oov_buckets = 1000\n",
    "embed_size = 128\n",
    "\n",
    "embedding = keras.layers.Embedding(input_dim=vocab_size + num_oov_buckets,\n",
    "                                 output_dim=embed_size)\n",
    "\n",
    "# Let's create some dummy word indices\n",
    "some_indices = tf.constant([[10, 5, 3], [7, 8, 0]]) # 2 sentences, 3 words each\n",
    "embeddings = embedding(some_indices)\n",
    "\n",
    "print(\"Shape of embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Datasets (TFDS)\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "The TensorFlow Datasets (TFDS) project is a library that makes it trivial to download and use common datasets. It returns `tf.data.Dataset` objects directly, ready for your pipeline.\n",
    "\n",
    "You can load a dataset using `tfds.load()`. You can also get info about the dataset (`with_info=True`) and get it as `(features, label)` tuples (`as_supervised=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must first install tensorflow-datasets:\n",
    "# !pip install -q tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]\n",
    "\n",
    "print(\"Dataset info:\", info.description)\n",
    "\n",
    "# Print the first review and its label\n",
    "for review, label in train_set.take(1):\n",
    "    print(\"Review:\", review.numpy()[:100], \"...\")\n",
    "    print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "From Chapter 13, page 442:\n",
    "\n",
    "1.  Why would you want to use the Data API?\n",
    "2.  What are the benefits of splitting a large dataset into multiple files?\n",
    "3.  During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "4.  Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "5.  Why would you go through the hassle of converting all your data to the `Example` protobuf format? Why not use your own protobuf definition?\n",
    "6.  When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
    "7.  Data can be preprocessed directly when writing the data files, or within the `tf.data` pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?\n",
    "8.  Name a few common techniques you can use to encode categorical features. What about text?\n",
    "9.  Load the Fashion MNIST dataset... split it... shuffle... and save each dataset to multiple TFRecord files. Each record should be a serialized `Example` protobuf... Then use `tf.data` to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer...\n",
    "10. Download the Large Movie Review Dataset... split it... create a `tf.data.Dataset`... then build and train a binary classification model containing an `Embedding` layer..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
