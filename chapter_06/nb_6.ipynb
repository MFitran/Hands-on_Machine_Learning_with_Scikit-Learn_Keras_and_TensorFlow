{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 – Decision Trees\n",
    "\n",
    "This notebook contains all the code samples and solutions to the exercises in chapter 6 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition* (O'Reilly). *Note: all code examples are based on the author's original GitHub repository.* \n",
    "\n",
    "**Assignment Instructions:**\n",
    "Per the assignment guidelines, this notebook reproduces the code from Chapter 6. It also includes theoretical explanations and summaries for each concept, as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter introduces **Decision Trees**, a versatile and powerful machine learning algorithm capable of performing both classification and regression tasks. Unlike models like SVMs, Decision Trees are intuitive, and their decisions are easy to interpret, making them \"white box\" models.\n",
    "\n",
    "Key concepts covered include:\n",
    [cite_start]"* **Training and Visualization:** How to train a `DecisionTreeClassifier` and visualize the resulting tree structure using `export_graphviz`. [cite: 1077, 1078]\n",
    "* **Making Predictions:** We trace the path from the root node down to a leaf node to make a prediction. [cite_start]The chapter explains key attributes like `samples`, `value`, and `gini`. [cite: 1079, 1080]\n",
    "* **Gini Impurity:** This is the cost function used by the CART algorithm. It measures the \"purity\" of a node. [cite_start]A node is pure (Gini=0) if all instances it applies to belong to the same class. [cite: 1080]\n",
    [cite_start]"* **Estimating Probabilities:** A Decision Tree can estimate the probability of a class by returning the ratio of training instances of that class in the leaf node the instance falls into. [cite: 1082]\n",
    [cite_start]"* **The CART Algorithm:** Scikit-Learn uses the Classification and Regression Tree (CART) algorithm, which is a *greedy algorithm* that searches for the feature and threshold that produce the purest subsets. [cite: 1083, 1085]\n",
    "* **Regularization:** Decision Trees are prone to overfitting if left unconstrained. [cite_start]Regularization is achieved by setting hyperparameters like `max_depth`, `min_samples_split`, `min_samples_leaf`, and `max_leaf_nodes`. [cite: 1086, 1087]\n",
    "* **Regression:** Decision Trees can also perform regression (`DecisionTreeRegressor`). Instead of predicting a class, each node predicts a value, which is the average target value of the training instances in that node. [cite_start]The CART algorithm works similarly but aims to minimize the Mean Squared Error (MSE) instead of impurity. [cite: 1089, 1090]\n",
    [cite_start]"* **Instability:** The chapter concludes by highlighting a key limitation of Decision Trees: they are very sensitive to small variations in the training data and to data rotation. [cite: 1091]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 3.7 or later is required for the latest versions of Scikit-Learn), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Visualizing a Decision Tree\n",
    "\n",
    "### Theoretical Explanation: Training\n",
    [cite_start]"A Decision Tree is a flowchart-like structure where each internal node represents a test on a feature (e.g., \"petal length <= 2.45 cm\"), each branch represents the outcome of the test, and each leaf node (or terminal node) holds a class label. [cite: 1079]\n",
    "\n",
    "The code below trains a `DecisionTreeClassifier` on the Iris dataset. [cite_start]We limit the tree's depth to 2 to keep it simple and easy to visualize. [cite: 1077]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Visualization\n",
    [cite_start]"We can visualize the trained Decision Tree by using the `export_graphviz()` method to output a graph definition file (called `iris_tree.dot`). [cite: 1078]\n",
    "\n",
    [cite_start]"We can then use the `dot` command-line tool (from the Graphviz package) to convert this `.dot` file to a `.png` image or other formats. [cite: 1078]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "# You can run this in your terminal to convert the .dot file to .png:\n",
    "# $ dot -Tpng iris_tree.dot -o iris_tree.png\n",
    "\n",
    "# Or, if you have the graphviz Python library installed:\n",
    "try:\n",
    "    import graphviz\n",
    "    from graphviz import Source\n",
    "\n",
    "    Source.from_file(os.path.join(IMAGES_PATH, \"iris_tree.dot\"))\n",
    "except ImportError:\n",
    "    print(\"Install graphviz to display the tree, or run the dot command line tool.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Explanation: Making Predictions\n",
    "\n",
    [cite_start]"Let’s see how the tree in the figure above makes predictions. [cite: 1079]\n",
    "\n",
    "1.  [cite_start]**Start at the root node (depth 0):** This node asks whether the flower’s **petal length is ≤ 2.45 cm**. [cite: 1079]\n",
    "2.  **Case 1 (True):** If it is, you move down to the root’s left child (depth 1, left). This is a **leaf node** (it has no children). [cite_start]It predicts the class `setosa`. [cite: 1079, 1080]\n",
    "3.  **Case 2 (False):** If the petal length is > 2.45 cm, you move to the root’s right child (depth 1, right). [cite_start]This is not a leaf node, so it asks another question: **is the petal width ≤ 1.75 cm**? [cite: 1080]\n",
    "4.  **Case 2a (True):** If it is, you move to the right node's left child (depth 2, left). [cite_start]This is a leaf node that predicts `versicolor`. [cite: 1080]\n",
    "5.  **Case 2b (False):** If it is not, you move to the right node's right child (depth 2, right). [cite_start]This is a leaf node that predicts `virginica`. [cite: 1080]\n",
    "\n",
    "**Node Attributes:**\n",
    [cite_start]"* `samples`: Counts how many training instances the node applies to. [cite: 1080]\n",
    [cite_start]"* `value`: Shows how many training instances of each class this node applies to. [cite: 1080]\n",
    "* `gini`: Measures the node's **impurity**. A node is \"pure\" (gini=0) if all its instances belong to the same class. [cite_start]The formula is: [cite: 1080]\n",
    "    $$G_i = 1 - \\sum_{k=1}^{n} p_{i,k}^2$$\n",
    [cite_start]"    where $p_{i,k}$ is the ratio of class *k* instances among the training instances in the $i$-th node. [cite: 1080]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities\n",
    "\n",
    "### Theoretical Explanation\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class *k*. [cite_start]It first finds the leaf node for the instance, then it returns the ratio of training instances of class *k* in that node. [cite: 1082]\n",
    "\n",
    "For example, if a flower has petals 5 cm long and 1.5 cm wide, it falls into the depth-2 left node (petal length > 2.45 cm, petal width ≤ 1.75 cm). [cite_start]This node's `value` is `[0, 49, 5]`. [cite: 1082]\n",
    [cite_start]"So, it will output the following probabilities: [cite: 1082]\n",
    "* Iris-Setosa: 0/54 (0%)\n",
    "* Iris-Versicolor: 49/54 (90.7%)\n",
    "* Iris-Virginica: 5/54 (9.3%)\n",
    "\n",
    [cite_start]"The predicted class is the one with the highest probability (Iris-Versicolor). [cite: 1082]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    [cite_start]"Scikit-Learn uses the **Classification and Regression Tree (CART)** algorithm to train Decision Trees. [cite: 1083]\n",
    "\n",
    "1.  [cite_start]**Splitting:** The algorithm first splits the training set into two subsets using a single feature *k* and a threshold *t_k* (e.g., \"petal length ≤ 2.45 cm\"). [cite: 1083]\n",
    "2.  **Choosing *k* and *t_k*:** How does it choose? It searches for the pair (*k*, *t_k*) that produces the **purest subsets** (weighted by their size). [cite_start]It tries to minimize this cost function: [cite: 1083]\n",
    "\n",
    "    $$J(k, t_k) = \\frac{m_{\\text{left}}}{m}G_{\\text{left}} + \\frac{m_{\\text{right}}}{m}G_{\\text{right}}$$ \n",
    "    \n",
    [cite_start]"    * $G_{\\text{left/right}}$ is the impurity (e.g., Gini) of the left/right subset. [cite: 1083]\n",
    [cite_start]"    * $m_{\\text{left/right}}$ is the number of instances in the left/right subset. [cite: 1083]\n",
    "\n",
    "3.  [cite_start]**Recursion:** Once the set is split, the algorithm splits the subsets using the same logic, then the sub-subsets, and so on, recursively. [cite: 1083]\n",
    "4.  **Stopping:** It stops recursing when it reaches the maximum depth (`max_depth`) or if it cannot find a split that will reduce impurity. [cite_start]Other hyperparameters like `min_samples_split` and `min_samples_leaf` also control stopping. [cite: 1083]\n",
    "\n",
    "The CART algorithm is a **greedy algorithm**: it greedily searches for the best split at the current level and repeats. [cite_start]It does not check if this split will lead to the best possible tree several levels down. [cite: 1085]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "Decision Trees make very few assumptions about the training data. [cite_start]If left unconstrained, the tree will adapt itself very closely to the training data, which leads to **overfitting**. [cite: 1086]\n",
    "\n",
    "To avoid overfitting, we must restrict the Decision Tree’s freedom. [cite_start]This is done using regularization hyperparameters. [cite: 1086]\n",
    "\n",
    [cite_start]"The `DecisionTreeClassifier` class has several parameters to control this: [cite: 1086]\n",
    "* `max_depth`: The maximum depth of the tree. [cite_start]Reducing this will regularize the model. [cite: 1086]\n",
    [cite_start]"* `min_samples_split`: The minimum number of samples a node must have *before* it can be split. [cite: 1086]\n",
    [cite_start]"* `min_samples_leaf`: The minimum number of samples a *leaf* node must have. [cite: 1086]\n",
    [cite_start]"* `max_leaf_nodes`: The maximum number of leaf nodes. [cite: 1087]\n",
    [cite_start]"* `max_features`: The maximum number of features that are evaluated for splitting at each node. [cite: 1087]\n",
    "\n",
    [cite_start]"**Increasing `min_*` or decreasing `max_*` hyperparameters will regularize the model.** [cite: 1087]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example: Regularization using min_samples_leaf\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "deep_tree_clf2.fit(Xm, ym)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_predictions(deep_tree_clf1, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(Xm, ym, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"No restrictions\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions(deep_tree_clf2, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(Xm, ym, [-1.5, 2.5, -1, 1.5])\n",
    "plt.title(\"min_samples_leaf = 4\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "save_fig(\"min_samples_leaf_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot on the left (no restrictions) is clearly overfitting. [cite_start]The plot on the right (`min_samples_leaf=4`) shows a model that will likely generalize much better. [cite: 1088]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    [cite_start]"Decision Trees can also perform regression tasks using the `DecisionTreeRegressor` class. [cite: 1089]\n",
    "\n",
    "The main difference is that instead of predicting a class in each node, it **predicts a value**. [cite_start]The predicted value for a leaf node is the **average target value** of the training instances in that node. [cite: 1089]\n",
    "\n",
    [cite_start]"The CART algorithm works similarly, but instead of minimizing impurity, it now tries to split the training set to **minimize the MSE** (Mean Squared Error). [cite: 1090]\n",
    "\n",
    "The cost function to minimize is:\n",
    "$$J(k, t_k) = \\frac{m_{\\text{left}}}{m}\\text{MSE}_{\\text{left}} + \\frac{m_{\\text{right}}}{m}\\text{MSE}_{\\text{right}}$$ \n",
    "\n",
    [cite_start]"Just like for classification, Decision Tree regressors are prone to overfitting. [cite: 1090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Quadratic training set + noise\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5)**2\n",
    "y = y + np.random.randn(m, 1) / 10\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "# Helper function to plot the regression predictions\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-W\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n",
    "save_fig(\"tree_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of an unregularized (overfitting) regression tree\n",
    "tree_reg_overfit = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg_regularized = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg_overfit.fit(X, y)\n",
    "tree_reg_regularized.fit(X, y)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_regression_predictions(tree_reg_overfit, X, y)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_regression_predictions(tree_reg_regularized, X, y, ylabel=None)\n",
    "plt.title(\"min_samples_leaf=10\", fontsize=14)\n",
    "\n",
    "save_fig(\"tree_regression_regularization_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot on the left is overfitting badly. [cite_start]The plot on the right (`min_samples_leaf=10`) is a much more reasonable model. [cite: 1090]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "Decision Trees have a few limitations. \n",
    "\n",
    "1.  **Sensitivity to Rotation:** They love orthogonal decision boundaries ( perpendicular to an axis). This makes them sensitive to training set rotation. [cite_start]The code below shows how a simple 45° rotation makes the decision boundary unnecessarily convoluted. [cite: 1091]\n",
    "2.  **Sensitivity to Small Variations:** More generally, Decision Trees are very sensitive to small variations in the training data. [cite_start]For example, removing one instance can sometimes lead to a completely different tree being trained. [cite: 1091]\n",
    "\n",
    [cite_start]"**Random Forests** (covered in Chapter 7) can limit this instability by averaging predictions over many trees. [cite: 1092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity to rotation\n",
    "np.random.seed(6)\n",
    "Xs = np.random.rand(100, 2) - 0.5\n",
    "ys = (Xs[:, 0] > 0).astype(np.float32) * 2\n",
    "\n",
    "angle = np.pi / 4\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xsr = Xs.dot(rotation_matrix)\n",
    "\n",
    "tree_clf_s = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_s.fit(Xs, ys)\n",
    "tree_clf_sr = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_sr.fit(Xsr, ys)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_predictions(tree_clf_s, [-0.7, 0.7, -0.7, 0.7])\n",
    "plot_dataset(Xs, ys, [-0.7, 0.7, -0.7, 0.7])\n",
    "plt.title(\"Original dataset\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions(tree_clf_sr, [-0.7, 0.7, -0.7, 0.7])\n",
    "plot_dataset(Xsr, ys, [-0.7, 0.7, -0.7, 0.7])\n",
    "plt.title(\"Dataset rotated by 45°\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "save_fig(\"sensitivity_to_rotation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "From Chapter 6, page 186:\n",
    "\n",
    "1.  What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances?\n",
    "2.  Is a node’s Gini impurity generally lower or greater than its parent’s? Is it *generally* lower/greater, or *always* lower/greater?\n",
    "3.  If a Decision Tree is overfitting the training set, is it a good idea to try decreasing `max_depth`?\n",
    "4.  If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
    "5.  If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?\n",
    "6.  If your training set contains 100,000 instances, will setting `presort=True` speed up training?\n",
    "7.  Train and fine-tune a Decision Tree for the moons dataset (e.g., using `make_moons(n_samples=10000, noise=0.4)`).\n",
    "8.  Grow a forest (an ensemble of Decision Trees) on the moons dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
