{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
    "\n",
    "**Based on \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\" by Aurélien Géron**\n",
    "\n",
    "This notebook reproduces the code from Chapter 16 and provides theoretical explanations for each concept, as required by the individual task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter dives into the techniques required to process sequential data, with a special focus on Natural Language Processing (NLP).\n",
    "\n",
    "1.  **Recurrent Neural Networks (RNNs) for Text:** We begin by building a **Character RNN (Char-RNN)** to generate original text, one character at a time. This introduces key data preparation techniques for sequential data, such as creating a `tf.data.Dataset` from text, using the `window()` method to create sequential windows, and building both **stateless** and **stateful RNNs**.\n",
    "\n",
    "2.  **Sentiment Analysis:** We then move from character-level to word-level models. We build an RNN to perform sentiment analysis on the IMDb movie review dataset. This section covers crucial NLP preprocessing steps, including:\n",
    "    * Word-level tokenization.\n",
    "    * Encoding words to integers using a vocabulary lookup table.\n",
    "    * Using **masking** (via `mask_zero=True`) to handle variable-length sequences (i.e., padding) efficiently.\n",
    "    * Using **word embeddings** (`Embedding` layer) to represent words as dense, trainable vectors.\n",
    "    * Reusing **pretrained word embeddings** from TensorFlow Hub for transfer learning.\n",
    "\n",
    "3.  **Neural Machine Translation (NMT):** The chapter introduces the **Encoder-Decoder** architecture, a powerful model for sequence-to-sequence (seq2seq) tasks like translation. We explore:\n",
    "    * The concept of an encoder (reading the source sentence) and a decoder (generating the target sentence).\n",
    "    * **Bidirectional RNNs** to capture information from both directions of a sequence.\n",
    "    * **Beam search** to improve prediction quality by keeping track of several candidate translations instead of just one.\n",
    "\n",
    "4.  **Attention Mechanisms:** We explore the key limitation of the basic Encoder-Decoder model (the bottleneck of the final hidden state) and solve it with **attention**.\n",
    "    * Attention allows the decoder to look back at the *entire* input sequence at each step, focusing on the most relevant parts.\n",
    "    * We discuss the two main types: **Bahdanau (additive)** and **Luong (multiplicative)** attention.\n",
    "\n",
    "5.  **The Transformer Architecture:** Finally, we move beyond RNNs entirely to the **Transformer**, the model introduced in the \"Attention Is All You Need\" paper. This is the state-of-the-art architecture that powers models like BERT and GPT.\n",
    "    * It relies exclusively on attention mechanisms (no recurrent or convolutional layers).\n",
    "    * Key components include **Positional Embeddings** (to inject word order information) and **Multi-Head Attention** (which allows the model to pay attention to multiple things simultaneously).\n",
    "\n",
    "The chapter concludes with a look at recent state-of-the-art language models like BERT, ELMo, and GPT-2, which are all built upon the concepts of pretraining and the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Common imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Install tensorflow-addons for NMT section\n",
    "# !pip install -q -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Shakespearean Text Using a Character RNN (Char-RNN)\n",
    "\n",
    "We will start by building a model that predicts the next character in a text. This is called a Character RNN, or Char-RNN. Once trained, the model can be used to generate novel text, one character at a time, in the style of the original training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training Dataset\n",
    "\n",
    "> **Theoretical Deep-Dive: Tokenization**\n",
    "> \n",
    "> Before we can feed text to a neural network, we must convert it into numbers. This process is called **tokenization**. We can tokenize at the **word level** (where each unique word gets an ID), the **sub-word level** (where words are broken up, e.g., \"smartest\" -> \"smart\" + \"est\"), or the **character level**.\n",
    "> \n",
    "> For this model, we will use **character-level tokenization**. This is simple and has a very small vocabulary (only 39 unique characters in this case). It's also effective for learning grammar, punctuation, and even generating new \"words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare's work\n",
    "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut URL\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# View the first 100 characters\n",
    "print(shakespeare_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Keras's Tokenizer to encode each character as an integer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])\n",
    "\n",
    "# The tokenizer finds all unique characters and maps them to an ID, starting from 1\n",
    "print(f\"Unique characters: {len(tokenizer.word_index)}\")\n",
    "print(tokenizer.texts_to_sequences([\"First\"]))\n",
    "print(tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode the full text. We subtract 1 to get IDs from 0 to 38 (instead of 1 to 39).\n",
    "# This is useful because 0 is a natural ID for the first token (or for padding).\n",
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "\n",
    "print(f\"max_id: {max_id}, dataset_size: {dataset_size}\")\n",
    "print(f\"Encoded text shape: {encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Split a Sequential Dataset\n",
    "\n",
    "We must split the data into training, validation, and test sets. For sequential data, we **cannot** shuffle the data randomly before splitting, as this would destroy the sequences. The patterns the RNN learns depend on the order of the data.\n",
    "\n",
    "We must split the data along the time axis. We'll use the first 90% for training, the next 5% for validation, and the final 5% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(dataset_size * 0.9)\n",
    "train_set = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chopping the Sequential Dataset into Multiple Windows\n",
    "\n",
    "> **Theoretical Deep-Dive: Truncated Backpropagation Through Time (TBPTT)**\n",
    ">\n",
    "> We can't train the RNN on the full text sequence of 1 million characters. This would be like unrolling a network with 1 million layers, leading to an impossibly slow and unstable training process (due to the vanishing/exploding gradients problem).\n",
    ">\n",
    "> Instead, we use **Truncated Backpropagation Through Time**. We chop the long sequence into many shorter subsequences (or \"windows\"). The RNN is then unrolled *only* over the length of these short windows. This is what the `window()` method helps us do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create windows of 101 characters.\n",
    "# The RNN will be trained to predict the next character, given the previous 100.\n",
    "# So, n_steps = 100.\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "\n",
    "# Use shift=1 to get overlapping windows. drop_remainder=True ensures all windows are the same size.\n",
    "dataset = train_set.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `window()` method creates a *nested dataset* (a dataset of datasets). We must use `flat_map()` to flatten it into a dataset of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts each window (which is a 'Dataset') into a tensor.\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Now we shuffle the windows, batch them, and split them into inputs (X) and targets (Y)\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to encode the inputs. The targets (labels) can remain as character IDs (integers), because we will use `sparse_categorical_crossentropy` as the loss function. This loss function expects integer labels.\n",
    "\n",
    "The inputs, however, must be encoded. We will use **one-hot encoding** because the vocabulary is very small (39 characters). An `Embedding` layer would also work but is overkill here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch_size, n_steps] -> [batch_size, n_steps, max_id]\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# Add prefetching for performance\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Theoretical Deep-Dive: Char-RNN Architecture**\n",
    ">\n",
    "> 1.  **Input:** A batch of one-hot encoded character sequences (shape: `[batch_size, n_steps, max_id]`).\n",
    "> 2.  **Recurrent Layers (`GRU`):** We use `GRU` layers (a more efficient variant of `LSTM`). We stack two layers to learn more complex patterns. `return_sequences=True` is crucial. It tells the GRU layer to output its hidden state at *every time step*, not just the final one. This is required to feed the full sequence to the next layer (and to our final output layer).\n",
    "> 3.  **`TimeDistributed(Dense)` Layer:** The output of the final GRU layer has a shape of `[batch_size, n_steps, n_neurons]`. We need to predict the *next character* at *each time step*. A standard `Dense` layer would flatten this and output a single prediction for the whole sequence. By wrapping the `Dense` layer in `TimeDistributed`, we apply the *same* `Dense` layer to each of the `n_steps` time steps independently. This gives us an output of shape `[batch_size, n_steps, max_id]`.\n",
    "> 4.  **Softmax Activation:** This converts the outputs (logits) at each time step into probability distributions over the 39 possible characters.\n",
    "> 5.  **Loss Function:** We use `sparse_categorical_crossentropy` because our labels (`Y_batch`) are sparse (integer IDs: `[batch_size, n_steps]`), but our model's outputs are probabilities (from softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=10) # The book suggests 20, but 10 is faster for reproduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Theoretical Deep-Dive: Text Generation & Temperature**\n",
    ">\n",
    "> To generate text, we give the model a starting string (a \"seed\"), predict the next character, append that character to the string, and repeat the process.\n",
    ">\n",
    "> If we always pick the character with the *highest* probability (a greedy approach), the text becomes very repetitive and boring. \n",
    ">\n",
    "> To get more interesting text, we sample from the probability distribution predicted by the model. We can control the randomness of this sampling using a **temperature** parameter.\n",
    "> 1.  We divide the output logits by the `temperature`.\n",
    "> 2.  A **low temperature** (e.g., 0.2) makes the distribution \"sharper,\" favoring high-probability characters. This leads to more predictable, conservative text.\n",
    "> 3.  A **high temperature** (e.g., 2.0) makes the distribution \"flatter,\" giving all characters a more equal chance. This leads to more random, creative (and often nonsensical) text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a function to preprocess the seed text\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :] # Get probabilities for the last time step\n",
    "    # Divide logits by temperature\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    # Sample a character ID based on the new distribution\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some text with different temperatures\n",
    "print(complete_text(\"t\", temperature=0.2))\n",
    "print(\"\\n\" + \"-\"*20 + \"\\n\")\n",
    "print(complete_text(\"w\", temperature=1))\n",
    "print(\"\\n\" + \"-\"*20 + \"\\n\")\n",
    "print(complete_text(\"w\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stateful RNN\n",
    "\n",
    "> **Theoretical Deep-Dive: Stateful vs. Stateless RNNs**\n",
    ">\n",
    "> -   **Stateless RNN (default):** At each training iteration, the model's hidden state is reset to zeros. It does not learn any patterns *between* batches. This is why we used overlapping windows (`shift=1`)—to show the model as many different transitions as possible.\n",
    "> -   **Stateful RNN:** The model's final hidden state from one batch is preserved and used as the *initial* state for the next batch. This allows the RNN to learn much longer-term patterns, spanning across batches.\n",
    ">\n",
    "> **Data Requirement:** A stateful RNN *requires* that the batches are sequential and non-overlapping. The N-th sequence in a batch must be the direct continuation of the N-th sequence from the previous batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataset for a Stateful RNN\n",
    "\n",
    "Preparing the data is the hardest part. The simplest way (as done in the book) is to use `batch_size=1`.\n",
    "\n",
    "A more complex (but more efficient) way is to chop the text into `batch_size` equal parts and create a dataset that returns one batch of windows from each part at each step. We will reproduce the simpler `batch_size=1` version first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sequential, non-overlapping dataset (as in the book)\n",
    "dataset_stateful_simple = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "# Use shift=n_steps for non-overlapping windows\n",
    "dataset_stateful_simple = dataset_stateful_simple.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset_stateful_simple = dataset_stateful_simple.flat_map(lambda window: window.batch(window_length))\n",
    "dataset_stateful_simple = dataset_stateful_simple.batch(1) # batch_size=1\n",
    "dataset_stateful_simple = dataset_stateful_simple.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset_stateful_simple = dataset_stateful_simple.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset_stateful_simple = dataset_stateful_simple.prefetch(1)\n",
    "\n",
    "print(list(dataset_stateful_simple.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the more complex, batched implementation. This is what you would typically do in a real project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "\n",
    "# Chop the training text into batch_size equal parts\n",
    "text_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for text_part in text_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(text_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Zip the datasets to create sequential batches\n",
    "dataset_batched = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "\n",
    "# Prepare inputs/targets and one-hot encode\n",
    "dataset_batched = dataset_batched.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset_batched = dataset_batched.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset_batched = dataset_batched.prefetch(1)\n",
    "\n",
    "print(list(dataset_batched.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training the Stateful Model\n",
    "\n",
    "To build a stateful model, we must:\n",
    "1.  Set `stateful=True` in every recurrent layer.\n",
    "2.  Specify the `batch_input_shape` in the first layer (it must know the batch size).\n",
    "3.  Manually reset the model's states at the end of each epoch, as it should not learn to connect the end of the text to the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stateful = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to reset the model's states at the end of each epoch\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stateful.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history_stateful = model_stateful.fit(dataset_batched, epochs=20,\n",
    "                                      callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, a stateful model can only be used to make predictions for batches of the same size. To avoid this, you can create an identical *stateless* model and copy the stateful model's weights to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "Next, we'll build a model to classify IMDb movie reviews as positive (1) or negative (0). This is a **sequence-to-vector** task.\n",
    "\n",
    "Instead of a Char-RNN, we will now process text at the **word level**. This requires several new preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the IMDb Dataset\n",
    "\n",
    "We can load the raw text from `tensorflow_datasets` (TFDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "print(f\"Training set size: {train_size}\")\n",
    "\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    print(X_batch.numpy())\n",
    "    print(y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "We need to convert these text reviews (byte strings) into sequences of word IDs. We'll follow the book's example by creating a preprocessing pipeline using `tf.strings` operations. This is crucial because it means the preprocessing logic can be embedded *inside* the model, simplifying deployment.\n",
    "\n",
    "1.  **Truncate:** Keep only the first 300 characters.\n",
    "2.  **Clean:** Remove `<br />` tags and non-letter characters.\n",
    "3.  **Split:** Split the text by spaces.\n",
    "4.  **Pad:** Convert the resulting `RaggedTensor` to a dense `Tensor` by padding with `\"<pad>\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch,  b\"<br\\\\s*/?>\",  b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch,  b\"[^a-zA-Z']\",  b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "\n",
    "Next, we need to build a vocabulary by counting all word occurrences in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "print(\"Top 3 most common words:\", vocabulary.most_common()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary is huge. We'll truncate it to the 10,000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "\n",
    "# Create a lookup table. We add 1000 'out-of-vocabulary' (oov) buckets.\n",
    "# Any word not in our 10k vocab will be hashed into one of these 1000 buckets.\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Final Dataset Pipeline\n",
    "\n",
    "Now we can create our final training pipeline. It will batch, preprocess, and encode the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)\n",
    "\n",
    "# We also create the test set\n",
    "test_set = datasets[\"test\"].batch(32).map(preprocess)\n",
    "test_set = test_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with an Embedding Layer\n",
    "\n",
    "> **Theoretical Deep-Dive: Word Embeddings**\n",
    ">\n",
    "> We could one-hot encode the word IDs, but with a vocabulary of 11,000, this would create *massive*, *sparse* vectors, which is computationally inefficient.\n",
    ">\n",
    "> A better solution is to use an **Embedding Layer**. This layer is a trainable lookup table. It maps each word ID to a dense vector of a fixed size (e.g., 128 dimensions). \n",
    ">\n",
    "> -   `input_dim`: The size of the vocabulary (10k + 1k oov buckets).\n",
    "> -   `output_dim`: The size of the dense embedding vector (a hyperparameter).\n",
    ">\n",
    "> Initially, these vectors are random. During training, the model learns to place similar words close together in this \"embedding space.\" For example, \"good\" and \"great\" will end up having similar vectors, while \"good\" and \"terrible\" will be far apart. This is a form of **representation learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "total_vocab_size = vocab_size + num_oov_buckets\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(total_vocab_size, embed_size,\n",
    "                           input_shape=[None]), # Input shape is [batch_size, n_steps]\n",
    "    keras.layers.GRU(128, return_sequences=True), # Returns the full sequence of outputs\n",
    "    keras.layers.GRU(128), # Returns only the last output\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\") # Binary classification (positive/negative)\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_set, epochs=5, validation_data=test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "> **Theoretical Deep-Dive: Masking**\n",
    ">\n",
    "> Our input batches are padded with the `\"<pad>\"` token (which maps to ID 0). We don't want the model to learn anything from this padding. We want it to be ignored.\n",
    ">\n",
    "> By setting `mask_zero=True` in the `Embedding` layer, we tell it that the token with ID 0 is a padding token. The layer will generate a **mask** (a boolean tensor) and propagate it to all subsequent layers. \n",
    ">\n",
    "> Recurrent layers like `GRU` and `LSTM` know how to handle this mask: they will simply ignore the time steps where the mask is `False` (i.e., where the token was a pad token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_masked = keras.models.Sequential([\n",
    "    keras.layers.Embedding(total_vocab_size, embed_size,\n",
    "                           mask_zero=True, # <= This is the key change\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model_masked.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model_masked.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the masked model\n",
    "history_masked = model_masked.fit(train_set, epochs=5, validation_data=test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings\n",
    "\n",
    "Instead of learning embeddings from scratch (which requires a lot of data), we can use **pretrained embeddings** trained on a massive text corpus (like all of Wikipedia or Google News). We can do this easily using the **TensorFlow Hub** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow_hub\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# This model from TF Hub is a sentence encoder. \n",
    "# It takes strings as input and outputs a 50-dimensional embedding vector for the whole sentence.\n",
    "# It handles all preprocessing (tokenization, embedding lookup, averaging) internally.\n",
    "model_pretrained = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                     dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_pretrained.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model_pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset that doesn't do any preprocessing\n",
    "train_set_simple = datasets[\"train\"].batch(32).prefetch(1)\n",
    "test_set_simple = datasets[\"test\"].batch(32).prefetch(1)\n",
    "\n",
    "history_pretrained = model_pretrained.fit(train_set_simple, epochs=5, validation_data=test_set_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Machine Translation (NMT)\n",
    "\n",
    "Here we build a model to translate from one language (e.g., English) to another (e.g., French). This is a **sequence-to-sequence (seq2seq)** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Theoretical Deep-Dive: The Encoder-Decoder Architecture**\n",
    ">\n",
    "> A simple seq2seq RNN is not ideal for translation because it tries to translate as it reads. A better approach is the **Encoder-Decoder** model:\n",
    ">\n",
    "> 1.  An **Encoder** RNN reads the source sentence (e.g., \"I drink milk\") and compresses it into a final hidden state (a vector). This vector is often called the **context vector** or **thought vector**.\n",
    "> 2.  A **Decoder** RNN takes this context vector as its *initial hidden state* and generates the target sentence word by word (e.g., \"Je bois du lait\").\n",
    ">\n",
    "> **Training (Teacher Forcing):** As shown in Figure 16-3, the decoder is fed the *actual* target sentence from the training data, shifted one step to the right (i.e., it's fed the `<sos>` (start-of-sequence) token first, then \"Je\", then \"bois\", etc.). It is trained to predict the *next* word at each step.\n",
    ">\n",
    "> **Inference (Making Predictions):** As shown in Figure 16-4, we don't have the target sentence. So, we feed the decoder the `<sos>` token, it predicts the first word (\"Je\"), then we feed *that* word back into it to predict the second word (\"bois\"), and so on, until it predicts an `<eos>` (end-of-sequence) token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book uses the `tensorflow_addons` library to build the NMT model. We will reproduce the architecture snippet from the book.\n",
    "\n",
    "*(Note: This is an architecture snippet and requires a pre-processed dataset (like the one in the book's notebook) to be runnable.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Assume we have vocab_size and embed_size defined\n",
    "vocab_size = 10000\n",
    "embed_size = 128\n",
    "n_units = 512\n",
    "\n",
    "# ENCODER\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size) \n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(n_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "# DECODER\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(n_units)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model_nmt = keras.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])\n",
    "\n",
    "model_nmt.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "print(\"NMT Model Built Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs\n",
    "\n",
    "> **Theoretical Deep-Dive:**\n",
    "> \n",
    "> When processing a word, a normal RNN only has access to the words that came *before* it. For many NLP tasks (like translation), it's incredibly useful to also know the words that come *after* it. \n",
    "> \n",
    "> A **Bidirectional RNN** consists of two RNNs: one reads the sequence from left-to-right, and the other reads it from right-to-left. At each time step, their outputs are concatenated. This gives the model a richer representation of each word *in its full context*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_example = keras.models.Sequential([\n",
    "    keras.layers.Embedding(total_vocab_size, embed_size, input_shape=[None]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "bidirectional_example.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "\n",
    "> **Theoretical Deep-Dive:**\n",
    ">\n",
    "> At inference time, a \"greedy\" decoder just picks the single most probable word at each step. This can lead to errors (e.g., \"How *will* you?\"). \n",
    ">\n",
    "> **Beam Search** is a better approach. It keeps track of the *k* most probable sentences (where *k* is the **beam width**) at each step. It explores multiple hypotheses and is much more likely to find a good translation.\n",
    ">\n",
Example
    "> `tfa.seq2seq.beam_search_decoder.BeamSearchDecoder` can be used to implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Mechanisms\n",
    "\n",
    "> **Theoretical Deep-Dive: The Bottleneck Problem**\n",
    ">\n",
    "> A basic Encoder-Decoder model must compress the *entire* source sentence into a single context vector (the encoder's final state). This is a major bottleneck. Information about the first words can be lost by the time the encoder is done. This makes the model struggle with long sentences.\n",
    ">\n",
    "> **Attention** solves this. The core idea is to allow the decoder to look back at the encoder's outputs (its hidden states from *every* time step) during decoding. At each step, the decoder decides which input words are most relevant for generating the *next* output word.\n",
    ">\n",
    "> This creates a \"shortcut\" between the decoder and the relevant input words, bypassing the bottleneck. We will reproduce the architecture for **Luong Attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet shows how to wrap a decoder cell with LuongAttention\n",
    "# (Assumes encoder_outputs and decoder_cell are defined from the NMT section)\n",
    "\n",
    "attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(\n",
    "    units=n_units, memory=encoder_outputs)\n",
    "\n",
    "attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
    "    decoder_cell, attention_mechanism, attention_layer_size=n_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Transformer Architecture\n",
    "\n",
    "The 2017 paper \"Attention Is All You Need\" introduced the **Transformer**, an architecture that *completely removes* RNNs and relies *only* on attention mechanisms. This is the basis for most state-of-the-art models today (like BERT and GPT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Theoretical Deep-Dive: Transformer Components**\n",
    ">\n",
    "> 1.  **Positional Embeddings:** Since the model has no RNNs, it has no sense of word order. We must inject word position information. We do this by adding a vector (a **Positional Embedding**) to each word embedding. The Transformer uses a fixed (non-trainable) pattern of sines and cosines of different frequencies.\n",
    "> 2.  **Multi-Head Attention:** This is the core of the Transformer. It's composed of multiple **Scaled Dot-Product Attention** layers. \n",
    ">     -   **Scaled Dot-Product Attention:** For each word, the model learns three vectors: a **Query (Q)**, a **Key (K)**, and a **Value (V)**. The Query (a word's question) is compared (via dot product) to every other word's Key. The resulting scores are scaled and softmaxed to get attention weights. These weights are then used to get a weighted sum of all the Values. This is the new representation for that word.\n",
    ">     -   **Multi-Head:** The model does this multiple times in parallel (e.g., 8 \"heads\"). Each head can learn to pay attention to different things. The results are concatenated and fed to a final `Dense` layer.\n",
    "> \n",
    "> The Transformer's encoder and decoder stacks use these layers to build rich, context-aware representations for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This custom layer implements the fixed sine/cosine positional embeddings\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the base building block for the Transformer's Encoder\n",
    "# Note: The book's code uses keras.layers.Attention. \n",
    "# The modern keras.layers.MultiHeadAttention is a more direct implementation.\n",
    "# I will reproduce the conceptual snippet from the book.\n",
    "\n",
    "embed_size = 512; max_steps = 500; vocab_size = 10000; n_units = 512\n",
    "\n",
    "# Create inputs and embeddings (as in the book)\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)\n",
    "\n",
    "# --- Simplified Transformer-like structure --- \n",
    "# (This is a simplified snippet, not the full N-stack model from the paper)\n",
    "# The book uses keras.layers.Attention to show the core idea.\n",
    "K = keras.backend\n",
    "\n",
    "# Encoder Self-Attention\n",
    "Z = encoder_in\n",
    "Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "encoder_outputs = Z\n",
    "\n",
    "# Decoder Masked Self-Attention\n",
    "Z = decoder_in\n",
    "Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "\n",
    "# Decoder Encoder-Decoder Attention (cross-attention)\n",
    "Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "# Final Output Layer\n",
    "outputs = keras.layers.TimeDistributed(\n",
    "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)\n",
    "\n",
    "model_transformer_simple = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[outputs])\n",
    "print(\"Simple Transformer-like model built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recent Innovations in Language Models\n",
    "\n",
    "> **Theoretical Deep-Dive:**\n",
    ">\n",
    "> The end of the chapter highlights the state-of-the-art models that combine the Transformer architecture with massive-scale **unsupervised pretraining**.\n",
    ">\n",
    "> -   **ELMo (Embeddings from Language Models):** Generates *contextualized* word embeddings. The embedding for \"queen\" is different in \"queen bee\" vs. \"Queen of England\".\n",
    "> -   **ULMFiT (Universal Language Model Fine-Tuning):** Showed that pretraining an LSTM on a large corpus (like Wikipedia) and then fine-tuning it on a specific task (like sentiment analysis) could achieve state-of-the-art results, even with very little labeled data.\n",
    "> -   **GPT (Generative Pre-Training):** Used the Transformer's *decoder* stack for pretraining. It was pretrained on a massive dataset to predict the next word. GPT-2, its successor, showed it could perform many tasks *without any fine-tuning* (called **zero-shot learning**).\n",
    "> -   **BERT (Bidirectional Encoder Representations from Transformers):** Used the Transformer's *encoder* stack. Unlike GPT, it's bidirectional (it sees text from both left and right). It was pretrained on two novel tasks:\n",
    ">     1.  **Masked Language Model (MLM):** 15% of words are hidden (`<mask>`), and the model must predict them.\n",
    ">     2.  **Next Sentence Prediction (NSP):** The model reads two sentences and predicts if the second one is the actual next sentence or just a random sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
