{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
    "\n",
    "This notebook contains the code reproductions and theoretical explanations for Chapter 16 of *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "This chapter dives into Natural Language Processing (NLP) using RNNs and newer, more powerful architectures. \n",
    "\n",
    "Key topics covered include:\n",
    "\n",
    "* **Text Generation (Char-RNN):** We start by building a character-level RNN to generate \"Shakespearean\" text. This covers how to prepare sequential text data, create windows, and build both stateless and stateful RNNs.\n",
    "* **Sentiment Analysis:** We build a model to classify movie reviews as positive or negative. This introduces word-level processing, word embeddings, and the crucial concept of **masking** to handle variable-length sequences.\n",
    "* **Encoder-Decoder Models:** We build a model for Neural Machine Translation (NMT). This introduces the **Encoder–Decoder** architecture, where one RNN (the encoder) processes the input sequence into a state vector, and another RNN (the decoder) uses that vector to generate an output sequence. We also cover **beam search** as a technique to improve translation quality.\n",
    "* **Attention Mechanisms:** We improve the Encoder–Decoder model by adding an **attention mechanism**. This allows the decoder to "look back" at the most relevant parts of the *entire* input sequence at each step, solving the bottleneck of using a single fixed-size state vector, which is especially problematic for long sentences.\n",
    "* **The Transformer:** We explore the groundbreaking \"Attention Is All You Need\" architecture. The Transformer eschews RNNs entirely and relies exclusively on attention mechanisms (specifically **self-attention** and **multi-head attention**). It also introduces **positional embeddings** to encode word order.\n",
    "* **Modern Language Models:** The chapter concludes with an overview of state-of-the-art Transformer-based models like BERT and GPT-2, which have revolutionized NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Common setup for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text Using a Character RNN\n",
    "\n",
    "### Theoretical Explanation\n",
    "\n",
    "A **Character-RNN (Char-RNN)** is a model trained to predict the next *character* in a sequence. By learning the statistical patterns of a text (like word spellings, grammar, and punctuation), it can be used to generate new text, one character at a time.\n",
    "\n",
    "We will train an RNN on all of Shakespeare's work. The model will process a sequence of characters (e.g., 100 long) and learn to predict the next character for each position in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Shakespeare dataset\n",
    "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut URL\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text at the character level\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how it works\n",
    "print(tokenizer.texts_to_sequences([\"First\"]))\n",
    "print(tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters\n",
    "\n",
    "print(\"Total distinct characters:\", max_id)\n",
    "print(\"Total characters in the text:\", dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the full text (we subtract 1 to get IDs from 0 to 38, not 1 to 39)\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Split a Sequential Dataset\n",
    "\n",
    "For sequential data, we **cannot** shuffle randomly. We must split it chronologically (or, in this case, textually) to prevent any overlap and data leakage. We'll take the first 90% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chopping the Sequential Dataset into Multiple Windows\n",
    "\n",
    "We can't train the RNN on the full text (over 1 million characters). Instead, we use **truncated backpropagation through time** by creating many smaller \"windows\" from the text.\n",
    "\n",
    "We use `dataset.window()` to create overlapping windows. For example, window 1 is characters 0-100, window 2 is 1-101, etc. The input for each window will be the first 100 chars, and the target will be the last 100 chars (shifted by one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # e.g., 0-100 (input is 0-99, target is 1-100)\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `window()` creates a nested dataset. We must flatten it.\n",
    "# `flat_map()` also lets us transform each window (e.g., batch it into a tensor).\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we shuffle the windows (not the characters *inside* them) and batch them.\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "\n",
    "# Finally, we split each batch into inputs (X) and targets (Y).\n",
    "# X = first 100 chars, Y = last 100 chars (shifted by one)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model needs to predict the *class* of the next character.\n",
    "# We one-hot encode the input characters. The labels (Y) can remain as integers \n",
    "# because we will use \"sparse_categorical_crossentropy\" as the loss.\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# Add prefetching for performance\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    # We use GRU cells, which are similar to LSTMs but a bit simpler.\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2), # Dropout added\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    # TimeDistributed applies a Dense layer to *every* time step.\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                  activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=10) # Reduced epochs for faster run time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Fake Shakespearean Text\n",
    "\n",
    "To generate new text, we feed the model a starting string (seed text), have it predict the next character, append that character to the text, and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a function to preprocess the seed text\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predicting the next char\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "print(tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]) # 'u'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theoretical Explanation: Temperature**\n",
    "\n",
    "Instead of greedily picking the *most likely* next character, we can sample from the probability distribution. This creates more diverse and less repetitive text. \n",
    "\n",
    "The `temperature` hyperparameter controls this: \n",
    "* **Low Temperature (e.g., 0.2):** Boosts high-probability characters, making the text safer and more predictable (but also more repetitive).\n",
    "* **High Temperature (e.g., 2.0):** Flattens the distribution, making all characters more equally likely. This leads to more random, creative, and often nonsensical text.\n",
    "\n",
    "We can do this by dividing the *logits* (the outputs before softmax) by the temperature, before feeding them to a `tf.random.categorical()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=0.2))\n",
    "print(\"-\" * 40)\n",
    "print(complete_text(\"w\", temperature=1))\n",
    "print(\"-\" * 40)\n",
    "print(complete_text(\"w\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateful RNN\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "So far, our RNN has been **stateless**. At each training iteration, it starts with a hidden state of zeros. It has no memory of the text from the previous batch. \n",
    "\n",
    "A **stateful RNN** preserves its hidden state from one batch to the next. This allows it to learn longer-term patterns.\n",
    "\n",
    "To build a stateful RNN:\n",
    "1.  **Set `stateful=True`** in all recurrent layers.\n",
    "2.  **Provide `batch_input_shape`** in the first layer (it needs to know the batch size to preserve states for each sequence in the batch).\n",
    "3.  **Prepare the dataset carefully:** The dataset must be non-overlapping and sequential. Batch *i* must contain the windows that immediately follow the windows in batch *i-1*.\n",
    "4.  **Reset the states** at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the dataset for a stateful model (batch_size=32)\n",
    "# (A simpler way is batch_size=1, but the book shows the batch_size=32 version)\n",
    "\n",
    "# Simple way: batch_size=1\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "# dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "# dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "# dataset = dataset.batch(1)\n",
    "\n",
    "# More complex way: batch_size > 1 (as in the book)\n",
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build the stateful model\n",
    "model_stateful = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                  activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a callback to reset the states at the end of each epoch\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile and fit\n",
    "model_stateful.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history_stateful = model_stateful.fit(dataset, epochs=50,\n",
    "                                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "We now move to a **word-level** model to classify IMDb movie reviews as positive (1) or negative (0). This is a **sequence-to-vector** task. \n",
    "\n",
    "The main challenge is that reviews have **variable lengths**. We can't use fixed-size tensors directly. The solution is **masking**.\n",
    "\n",
    "1.  **Padding:** We pad all sequences in a batch with a special padding token (usually represented by 0) so they all have the same length.\n",
    "2.  **Masking:** We use `mask_zero=True` in the `Embedding` layer. This tells the layer to create a *mask* (a tensor of booleans) that is `False` for all padding tokens (ID 0) and `True` for all other tokens. This mask is then automatically propagated to all subsequent layers (like GRU/LSTM), which will then correctly ignore the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset (pre-tokenized)\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "\n",
    "# Show an example review (list of word IDs)\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the review back to text\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    id_to_word[id_] = token\n",
    "\n",
    "print(\" \".join([id_to_word[id_] for id_ in X_train[0][:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Text with TensorFlow Datasets (TFDS)\n",
    "\n",
    "Instead of the pre-tokenized data, let's load the raw text reviews from TFDS to show a more complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(X_batch, y_batch):\n",
    "    # Truncate to 300 characters\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    # Replace <br /> with spaces\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
    "    # Keep only letters and quotes\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    # Split by spaces\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    # Convert from ragged to dense tensor, padding with \"<pad>\"\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
    "\n",
    "print(preprocess(tf.constant([b\"This movie was faaaaaantastic<br />\"]), tf.constant([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "print(\"Most common words:\", vocabulary.most_common()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the vocabulary\n",
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup table (with 1000 OOV buckets for unknown words)\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final training dataset\n",
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sentiment analysis model with masking\n",
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    # The Embedding layer converts word IDs to dense vectors (embeddings)\n",
    "    # mask_zero=True tells the layer to ignore padding (ID 0)\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                         mask_zero=True,\n",
    "                         input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128), # Only return the last output\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings\n",
    "\n",
    "Instead of learning embeddings from scratch (which requires a lot of data), we can use embeddings pretrained on a huge corpus (like Wikipedia). **TensorFlow Hub** is a repository for pretrained model components, called *modules*. We can load a module as a Keras layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load a pretrained sentence embedding module from TF Hub\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Create a new dataset that doesn't do our manual preprocessing\n",
    "train_set_hub = datasets[\"train\"].batch(32).prefetch(1)\n",
    "history_hub = model.fit(train_set_hub, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Encoder–Decoder Network for Neural Machine Translation\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "An **Encoder–Decoder** model is a common architecture for NMT. \n",
    "\n",
    "1.  **Encoder:** An RNN (e.g., LSTM/GRU) that reads the input sentence (e.g., in English) one word at a time and compresses it into a single state vector (the last hidden state). This vector is often called the *context vector* or *thought vector*.\n",
    "2.  **Decoder:** Another RNN that is initialized with the encoder's final state. Its job is to generate the translated sentence (e.g., in French), one word at a time. \n",
    "\n",
    "**Training:** At each time step, the decoder is fed the *previous* target word. For example, to output \"Je\", it's given the `<sos>` (start-of-sequence) token. To output \"bois\", it's given \"Je\". \n",
    "\n",
    "**Inference:** At inference time, we don't have the target. Instead, we feed the decoder the word it *just* predicted at the previous step. It stops when it outputs an `<eos>` (end-of-sequence) token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tensorflow_addons` package provides tools to build these models. (Note: The book's code for this section is complex, so this is a simplified representation of the concept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a conceptual representation. \n",
    "# The full implementation is very involved.\n",
    "\n",
    "# We'll use a placeholder for the real vocab_size and embed_size\n",
    "vocab_size = 10000\n",
    "embed_size = 128\n",
    "\n",
    "try:\n",
    "    import tensorflow_addons as tfa\n",
    "\n",
    "    encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "    sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "    embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "    encoder_embeddings = embeddings(encoder_inputs)\n",
    "    decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "    encoder = keras.layers.LSTM(512, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "    encoder_state = [state_h, state_c]\n",
    "\n",
    "    sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    decoder_cell = keras.layers.LSTMCell(512)\n",
    "    output_layer = keras.layers.Dense(vocab_size)\n",
    "\n",
    "    decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                  output_layer=output_layer)\n",
    "\n",
    "    final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "        decoder_embeddings, initial_state=encoder_state,\n",
    "        sequence_length=sequence_lengths)\n",
    "    \n",
    "    Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "    model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "                        outputs=[Y_proba])\n",
    "\n",
    "except ImportError:\n",
    "    print(\"TensorFlow Addons is not installed. Skipping this code block.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional RNNs\n",
    "\n",
    "When encoding a word like \"queen\" in \"the queen of hearts\", it's useful to look at *both* the words that came before it (\"the\") and the words that come *after* it (\"of hearts\"). \n",
    "\n",
    "A **bidirectional RNN** (`keras.layers.Bidirectional`) runs two separate RNNs over the input: one from left-to-right and one from right-to-left. It then concatenates their outputs at each time step. This is very common in encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True), \n",
    "                               input_shape=[None, 1])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search\n",
    "\n",
    "Instead of greedily choosing the most likely word at each step, **beam search** keeps track of the *k* most probable sentences so far. At each step, it tries to extend these *k* sentences and again keeps only the *k* most likely results. A larger *k* (beam width) finds better translations but is slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The Encoder-Decoder's fixed-size context vector (`encoder_state`) is a bottleneck. It's impossible to cram all the information of a long sentence into one vector. \n",
    "\n",
    "**Attention** solves this. The main idea is to give the decoder access to *all* of the encoder's outputs (not just its final state). \n",
    "\n",
    "At each step, the decoder uses an **attention mechanism** (a small neural net) to compute a set of *alignment scores*. These scores measure how relevant each input word (encoder output) is to the current word the decoder is about to produce. \n",
    "\n",
    "These scores are converted to weights (via softmax), and the decoder computes a *weighted sum* of the encoder outputs. This *context vector* is dynamic—it \"pays attention\" to different input words at each step.\n",
    "\n",
    "This lets the model handle long sequences and makes the model *explainable*—we can plot the attention weights to see what the decoder was \"looking at\" when it produced each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual implementation with TensorFlow Addons\n",
    "# Note: Bahdanau attention is also available.\n",
    "try:\n",
    "    # Assuming we have an encoder_state and decoder_cell from before\n",
    "    attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(\n",
    "        units=512, memory=encoder_outputs, \n",
    "        memory_sequence_length=sequence_lengths)\n",
    "\n",
    "    attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism, attention_layer_size=512)\n",
    "    \n",
    "except (NameError, AttributeError):\n",
    "    print(\"Skipping attention block because previous components are not fully defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Is All You Need: The Transformer Architecture\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The **Transformer** is an architecture that uses *only* attention mechanisms, completely removing RNNs and CNNs. Because it is not recurrent, it is much faster to train and can be parallelized.\n",
    "\n",
    "Key components:\n",
    "\n",
    "1.  **Positional Embeddings:** Since the model has no RNNs, it has no sense of word order. We \"inject\" order information by adding a **positional embedding** vector to each word embedding. This vector is a function of the word's position in the sentence (using `sin` and `cos` functions of different frequencies).\n",
    "2.  **Multi-Head Attention:** This is the core layer. It's composed of multiple **Scaled Dot-Product Attention** layers (or \"heads\").\n",
    "    * **Self-Attention:** In the encoder, the attention layer compares every word in the sentence to every *other* word in the *same* sentence to build a richer representation (e.g., to understand that \"it\" in \"it is raining\" refers to the weather, not an object).\n",
    "    * **Masked Self-Attention:** In the decoder, this is the same, but each word is *masked* (prevented) from looking at future words (since it can't know what they are).\n",
    "    * **Encoder-Decoder Attention:** This is the same as the attention mechanism in the previous section: the decoder looks at the encoder's outputs.\n",
    "3.  **Feed Forward blocks:** Each attention layer is followed by a simple two-layer feedforward network, which is applied to each word position independently.\n",
    "4.  **Skip Connections & Layer Norm:** Just like in ResNet, skip connections are used around every sub-layer, followed by Layer Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Positional Embeddings\n",
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of inputs for the Transformer\n",
    "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "**Theoretical Explanation:**\n",
    "\n",
    "The core of the Transformer is **Scaled Dot-Product Attention**. It works like a dictionary lookup. For a given word (the **Query**), it compares it to all other words (the **Keys**) using a dot product. These scores are scaled (divided by $\\sqrt{d_{keys}}$) and put through a softmax to get weights. Finally, it computes a weighted sum of the **Values** (the word representations themselves).\n",
    "\n",
    "**Multi-Head Attention** is even more powerful. It runs *h* (e.g., 8) Scaled Dot-Product Attention layers in parallel. Each \"head\" projects the Q, K, and V vectors into a different, smaller subspace. This allows each head to learn *different types* of relationships (e.g., one head might learn subject-verb relationships, another might learn word-order relationships). The outputs of all heads are concatenated and passed through a final `Dense` layer.\n",
    "\n",
    "(The full implementation is provided in the book's notebook, but `keras.layers.MultiHeadAttention` is now the standard.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Transformer (conceptual, simplified)\n",
    "# Keras now provides a keras.layers.MultiHeadAttention layer.\n",
    "\n",
    "Z = encoder_in\n",
    "for N in range(6): # The book uses 6 blocks\n",
    "    # We'll use a placeholder for the real layer for simplicity\n",
    "    # In a real implementation, this would be a full block with LayerNorm, \n",
    "    # Multi-Head Attention, another LayerNorm, and a FeedForward network.\n",
    "    Z = keras.layers.Dense(embed_size, activation=\"relu\")(Z) # Placeholder for a Transformer block\n",
    "    \n",
    "encoder_outputs = Z\n",
    "\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    # This would be a Masked Multi-Head Attention, plus Encoder-Decoder Attention\n",
    "    Z = keras.layers.Dense(embed_size, activation=\"relu\")(Z) # Placeholder\n",
    "\n",
    "outputs = keras.layers.TimeDistributed(\n",
    "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "See Appendix A in the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
